## Chris Nuske has been working really hard to 
## get adaptive sampling working for the lab.
## let's see if we can catch up with Chris a bit,

## we'll work mostly on the lab nanopore computer,
## so readfish installs, etc will be there.

## he has scripts here:

## keeping a github repo here:
https://github.com/danchurch/nanoporeHostDepletionSpruce

## first step is to get ReadFish installed in a conda environment,
## on my account on the lab computer.

## we'll follow their instructions on github:
https://github.com/looselab/readfish

## we'll put a copy of their yaml file on the labcomp here:

cd /media/vol1/daniel/hostDepletion

conda env create -f readfish_env.yml

conda activate readfish

readfish

## seems to work. 

## according to Chris, it's important to add ourselves
## to the minknow user group:

sudo usermod -a -G minknow test

## we can watch minknow on our extra office computer
## do this by clicking on lower left "connection manager"
## and entering the nanoComp computer IP addess:
132.180.112.115



## to test it out, we need a bulk fast5 file
## chris already downloaded a sample bulk fast five,
## it's here

ls /home/chris/PLSP57501_simulation.fast5

ln -s /home/chris/PLSP57501_simulation.fast5 /media/vol1/daniel/hostDepletion/sampleBulk.fast5 


## we need a TOML file. instructions are here:
https://github.com/LooseLab/readfish/blob/dev_staging/TOML.md

## chris has already worked through a lot of the gotchas
## on the TOML file...

## some are here:
ls /home/chris/*.toml

## also one here, but not what we need, I think.
/media/vol2/chris/channels.toml


## this might be a good one:
less /home/chris/pabies_depletion.toml

## let's make a copy of that for us to play with:

cd /media/vol1/daniel/hostDepletion/

cp  /home/chris/pabies_depletion.toml /media/vol1/daniel/hostDepletion/

## what do we need to change for this?

## the tutorial also mentions using existing toml files 
## from  minknow for templates:

cd /opt/ont/minknow/conf/package/sequencing/

## ah, and I see these sequencing toml files are very different
## from the readfish toml files for adaptice samplig

## chris has already modified this one to look for for his 
## bulkfast5 example like we are doing now:

less /opt/ont/minknow/conf/package/sequencing/sequencing_MIN106_DNA.toml

## back it up, then play with it


cd /opt/ont/minknow/conf/package/sequencing/

cp sequencing_MIN106_DNA.toml sequencing_MIN106_DNA.toml.bk
chmod 444 sequencing_MIN106_DNA.toml.bk

## changed the following, under # Sequencing Feature Settings # --> # basic_settings # --> [custom_settings]

"""
simulation = "/media/vol1/daniel/hostDepletion/sampleBulk.fast5"
"""

## changing file to find my bulk fast5 file
## chris already change alignment time limit settings

## we then setup a run using the config test flowcell, 
## and setting the flow cell type to match whatever 
## sequencing toml file we modified (in this case MIN106 DNA, above)

## run started.

## we should be able to eject (reject) all reads currently in 
## pores with readfish:

readfish unblock-all --device MN40608 --experiment-name "Testing ReadFish Unblock All"

## the device id is the same as the "position" reported by minknow

## do I need to match the experiment name? nope

## that should just mean that all reads are rejected

## funny, it looks to me like it is still reported the basecalling
## from these unblocked reads, just as very short sequences.

## anyway, looks like it is working. 

## stop run and clean up the files,,,

## files are usually stored here:
/var/lib/minknow/data

## and now let's test their example for enrichment/depletion
## with human chromosome:

## their toml file for enriching a human chromosome:

wget https://raw.githubusercontent.com/LooseLab/readfish/master/examples/human_chr_selection.toml

## we need a human genome and mmi index of it. I think chris 
## found exactly the same genome as is used in their 
## example...

ln -s /home/chris/hg38.fa /media/vol1/daniel/hostDepletion/hg38.fa

## make an mmi
minimap2 -d hg38.mmi hg38.fa

## put this in our readfish toml file 

## check the validity of our toml file:

readfish validate human_chr_selection.toml

## looks good. 

## started a simulation run on minknow. 
## now with readfish

readfish targets --device MN40608 \
              --experiment-name "RU Test basecall and map" \
              --toml /media/vol1/daniel/hostDepletion/human_chr_selection.toml \
              --log-file ru_test.log


## doesn't work, freezes. possibly due to the guppy server issues 
## chris mentioned. 

## not really sure what this is, a pipe or somethiing,
## but I think we need to change its permissions:

sudo chmod 775 /tmp/.guppy/5555

#sudo chmod 777 /tmp/.guppy/5555

## chris has this line for starting a new basecall server:

guppy_basecall_server --config dna_r9.4.1_450bps_fast.cfg -p 5555 -l /tmp/guppy -x 'cuda:0'


## already in use. Stop run and try again. works, but do we really need two guppy 
## servers running?

## because of the 
## I had to change the following to the readfish toml:

host= "ipc:///tmp/.guppy/"

## seems to be running. I killed the other manually and it just started again. 
## not sure what to do there. Just leave it I guess

## anyway, with the new basecaller, and the new toml file that looks for it,
## does the above command now work? 

conda activate readfish
readfish targets --device MN40608 \
              --experiment-name "RU Test basecall and map" \
              --toml /media/vol1/daniel/hostDepletion/human_chr_selection.toml \
              --log-file ru_test.log

## nope...

## this issue seems pertinent:
https://github.com/LooseLab/readfish/issues/240

## perhaps also pertinent:
https://github.com/LooseLab/readfish/issues/221

## they mention that our guppy server and client software should line up, version-wise:
guppy_basecall_server --version ## gives us version 6.5.7
pip list | grep pyguppy  ## version 6.4.2
## not the same

## does this help?
pip install ont-pyguppy-client-lib==6.5.7

## they do some diagnostics

## is this necessary to do every time we want to do a 
## new experiment?
ls -l /tmp/.guppy/5555

sudo chmod 775 /tmp/.guppy/5555

less /opt/ont/guppy/data/dna_r9.4.1_450bps_fast.cfg

## they test their guppy server/client with this script:
python -c 'from pyguppy_client_lib.pyclient import PyGuppyClient as PGC; \
           c = PGC("ipc:///tmp/.guppy/5555", "dna_r9.4.1_450bps_fast.cfg"); \
           c.connect(); print(c)'

sudo systemctl status guppyd

## okay, works


readfish targets --device MN40608 \
              --experiment-name "RU Test basecall and map" \
              --toml /media/vol1/daniel/hostDepletion/human_chr_selection.toml \
              --log-file ru_test.log

## and works!!! yay!

## to review,  have to reset user permissions every time guppy server is
## restarted on the socket ./tmp/.guppy/5556
## also need to make sure guppy versions (client and server) match

## so what's next?

## where is the data for this? do I have to stop the run?

## the reads are delivered in batches, here:

/var/lib/minknow/data/zoop5/no_sample/20230628_1557_MN40608_zoop5_f31f0515/fast5_pass

TOML="/media/vol1/daniel/hostDepletion/human_chr_selection.toml"
reads="/var/lib/minknow/data/zoop5"

readfish summary $TOML $reads

## looks like this:

contig  number      sum   min    max    std  mean  median    N50
  chr1    5994  3759112   153  45338   1458   627     487    532
 chr10    2959  1969533   154  49075   2072   666     488    537
 chr11    2949  2046589   200  46773   1861   694     501    561
 chr12    4300  2523930   103  49516   1361   587     494    522
 chr13    2176  1333880   184  39207   1442   613     484    520
 chr14    2917  1940354   179  52401   1949   665     501    543
 chr15    3274  1804635    94  46421   1058   551     472    498
 chr16    1554   970043   192  53194   1912   624     473    517
 chr17    2704  1571430   199  53051   1435   581     486    518
 chr18    2847  1554361   140  15657    612   546     482    503
 chr19    1210   834025   159  21556   1307   689     487    583
  chr2    5918  3675999   130  52366   1354   621     489    530
 chr20    1524  1092416   178  49450   2223   717     496    562
 chr21      30   212798   469  38881   8723  7093    4316  14064
 chr22      55   423294   388  58385  11798  7696    3706  15818
  chr3    5608  3597901   144  61175   1558   642     503    541
  chr4    6637  4105216   155  55008   1667   619     492    535
  chr5    4607  3008922   216  43932   1476   653     494    542
  chr6    4001  2628431   166  54594   1754   657     492    538
  chr7    4103  2343278   145  49974   1057   571     498    524
  chr8    3607  2372494   207  48653   1691   658     493    545
  chr9    2988  1966598   193  46570   1829   658     482    532
  chrM     169   174679   273  16400   2092  1034     543   1135
  chrX    4967  3053079   152  52969   1627   615     479    523
  chrY      14    10745   372   2246    499   768     665    993


## not sure what the "number" column indicates
## but the thing to look for is the median/mean/n50
## the low average read length of the other (not 21/22)
## contigs is indicative that they were rejected 
## after alignment, usually around 500-600 bp in our case.

## I guess this means we would need to exclude low-length reads,
## to exclude the non-targeted reads.

## for making readfish TOMLS, there is a good table on this in the nature paper, table 2.

## for the record, the toml for the above experiment looks like:

"""
[caller_settings]
config_name = "dna_r9.4.1_450bps_hac"
host = "ipc:///tmp/.guppy/"
port = 5555

[conditions]
reference = "/media/vol1/daniel/hostDepletion/hg38.mmi"

[conditions.0]
name = "select_chr_21_22"
control = false
min_chunks = 0
max_chunks = inf
targets = ["chr21", "chr22"]
single_on = "stop_receiving"
multi_on = "stop_receiving"
single_off = "unblock"
multi_off = "unblock"
no_seq = "proceed"
no_map = "proceed"
"""

## the above is modified from:
https://github.com/LooseLab/readfish/blob/master/examples/human_chr_selection.toml


## can we play with this a little? For instance, what happens  if:
## 1) we change "stop_receiving" to "unblock" for all non-target reads?
## 2) we invert, and deplete these two target sequences 
## 3) we block everything that aligns to the genome?


## 1) we change "stop_receiving" to "unblock" for all non-target reads?

cp human_chr_selection.toml human_chr_selection_stop2unblock.toml

## so weird. Looking at their readfish toml, the have everything on 
## target as "stop_receiving". Makes me think that this is the
## command for accepting and sequencing the read. 

## if that is correct, then changing these to unblock would 
## reject most things. I think resulting in everything having
## a read length average of ~500 bp. 

## and just got confirmation from the loose lab github folks

"""
Proceed means that one collects more data for an individual read and you assess it again. Stop_receiving tells the sequencer to keep sequencing that read and not evaluate it again.

So -
unblock means a read will be rejected from the pore and a new one sampled.
proceed means the read will continue to sequence and the next batch of singal will be analysed again.
stop_receiving means send no more data about this read and let it sequence to normal completion.

I hope that helps!
"""

## find this here:
https://github.com/LooseLab/readfish/issues/242

## Let's see. Here is the TOML

## human_chr_selection_stop2unblock.toml
[caller_settings]
config_name = "dna_r9.4.1_450bps_hac"
host = "ipc:///tmp/.guppy/"
port = 5555

[conditions]
reference = "/media/vol1/daniel/hostDepletion/hg38.mmi"

[conditions.0]
name = "select_chr_21_22_stop2unblock"
control = false
min_chunks = 0
max_chunks = inf
targets = ["chr21", "chr22"]
single_on = "unblock"
multi_on = "unblock"
single_off = "unblock"
multi_off = "unblock"
no_seq = "proceed"
no_map = "proceed"

## run the readfish command with this new toml

## as always, make sure we have group permission for the socket
ls -l /tmp/.guppy/5555

conda activate readfish

## check that the client and server are talking:
python -c 'from pyguppy_client_lib.pyclient import PyGuppyClient as PGC; \
           c = PGC("ipc:///tmp/.guppy/5555", "dna_r9.4.1_450bps_fast.cfg"); \
           c.connect(); print(c)'

## yup. onto readfish
readfish targets --device MN40608 \
              --experiment-name "stop_receive2unblock" \
              --toml /media/vol1/daniel/hostDepletion/human_chr_selection_stop2unblock.toml \
              --log-file ru_test.log

## running, let it go for a while


TOML="/media/vol1/daniel/hostDepletion/human_chr_selection_stop2unblock.toml"
reads="/var/lib/minknow/data/zoop7"
readfish summary $TOML $reads

## saving this as stop2unblock.txt, but cleaned up version here:

"""
contig  number      sum   min     max   std  mean  median    N50
  chr1    8166  5297756   162   29411  1084   649     500    547
  chr2   10335  6376134   153   50357  1085   617     491    532
  chr3    7447  4992369   203   29286  1261   670     513    557
  chr4    8234  5712847   110   29541  1290   694     505    571
  chr5    5813  4295677   195   28837  1472   739     500    598
  chr6    5877  3905658   186   67939  1415   665     508    552
  chr7    5915  3850456   133   32864  1092   651     509    550
  chr8    4520  3180204   202   29718  1260   704     511    584
  chr9    4843  3443058   189  166227  2660   711     499    560
 chr10    4548  2948968   162   28674  1098   648     502    549
 chr11    4149  2862933   175   34784  1406   690     498    555
 chr12    4310  3044324   197   55612  1394   706     515    578
 chr13    2912  1845003   206   29650  1132   634     499    541
 chr14    3136  2463883   173   38963  1753   786     510    639
 chr15    5912  3623898   128   89365  1750   613     488    524
 chr16    2234  1589472   169   36303  1413   711     496    599
 chr17    2521  1922530   225   95271  2583   763     501    604
 chr18    4134  2647552   190   70140  1510   640     500    542
 chr19    1635  1494578   183  107938  3081   914     545    807
 chr20    1078  1054159   187   40566  2437   978     533   1051
 chr21    1200   970358   217   27835  1655   809     522    686
 chr22     721   554628   211   29682  1726   769     517    647
  chrM     226   208691   271   15379  1701   923     566    753
  chrX    5835  4039849   189   30215  1323   692     505    567
  chrY      23    15382   373    1034   192   669     645    722
"""

## yup. Unlike before, all chromosomes have an average of ~500 bp,
## including 21 and 22. So we basically just blocked everything.

## we'll want to do something like this below, in #3 and with our 
## data. Except that the unmapped reads should all be set to 
## "stop_receiving"


## 2) we invert, and deplete these two target sequences, sequence everything else:

human_chr_deplete21_22.toml
"""
[caller_settings]
config_name = "dna_r9.4.1_450bps_hac"
host = "ipc:///tmp/.guppy/"
port = 5555

[conditions]
reference = "/media/vol1/daniel/hostDepletion/hg38.mmi"

[conditions.0]
name = "deplete_chr_21_22"
control = false
min_chunks = 0
max_chunks = inf
targets = ["chr21", "chr22"]
single_on = "unblock"
multi_on = "unblock"
single_off = "stop_receiving"
multi_off = "stop_receiving"
no_seq = "proceed"
no_map = "proceed"
"""

conda activate readfish

## check that the client and server are talking:
python -c 'from pyguppy_client_lib.pyclient import PyGuppyClient as PGC; \
           c = PGC("ipc:///tmp/.guppy/5555", "dna_r9.4.1_450bps_fast.cfg"); \
           c.connect(); print(c)'

## yup. onto readfish
readfish targets --device MN40608 \
              --experiment-name "deplete21_22" \
              --toml /media/vol1/daniel/hostDepletion/human_chr_deplete21_22.toml \
              --log-file ru_test.log


## check the results:

TOML="/media/vol1/daniel/hostDepletion/human_chr_deplete21_22.toml"
reads="/var/lib/minknow/data/zoop8"
readfish summary $TOML $reads > readfishSummary_human_chr_deplete21_22.txt

## redirect doesn't work. anyway, cleaned up, looks like this:

contig  number      sum    min     max    std   mean  median     N50
  chr1     203  1985656    223  143583  18456   9782    1921   35106
  chr2     194  1940543    216  239229  26135  10003    2192   37848
  chr3     195  1973224    249  321947  29054  10119    1950   41776
  chr4     145  2620134    210  309287  40913  18070    2196   85360
  chr5     191  2117429    259  191101  26404  11086    1865   43932
  chr6     191  1412928    279  151998  16849   7398    1904   32483
  chr7     114  1574735    277  242490  29940  13813    3518   49974
  chr8     148  1255656    304  118447  16594   8484    1776   34932
  chr9     132  1316892    216  179598  23458   9976    1798   43623
 chr10     100  1076103    291  130248  19205  10761    2718   35462
 chr11     135  1087374    215   96666  15629   8055    1805   33647
 chr12     112  1235755    257  208411  24379  11034    1993   35706
 chr13      95   617561    220  206157  24169   6501    1028  103486
 chr14      95   975288    265  149505  25753  10266    1076   46059
 chr15      90  1057315    289  158004  27721  11748    4298   40799
 chr16      55   792690    241  241976  35456  14413    3260   62880
 chr17      71   582331    408  184443  24092   8202    1605   52554
 chr18      47   632308    380  146314  26010  13453    4060   47291
 chr19      87   570863    384  142612  18646   6562    1295   28548
 chr20      55   759285    249  156527  25388  13805    2097   40661
 chr21     903   668203    231  195532   6559    740     428     632
 chr22     366   318060    208   42955   2769    869     444    1195
  chrM      12    87633    555   16467   7057   7303    4484   16362
  chrX     132  2064894    213  321374  41516  15643    2251  134053

## seems to work, chromosome 21 and 22 reads hover around rejection
## size, the others around ~10,000
~

## 3) we block everything that aligns to the genome?

## pertinent is issue 242:
https://github.com/LooseLab/readfish/issues/242

#cp human_chr_selection.toml human_chr_depleteEntireGenome.toml
vim human_chr_depleteEntireGenome.toml

chmod 777 human_chr_depleteEntireGenome.toml

## looks like this:

#human_chr_depleteEntireGenome.toml
[caller_settings]
config_name = "dna_r9.4.1_450bps_hac"
host = "ipc:///tmp/.guppy/"
port = 5555

[conditions]
reference = "/media/vol1/daniel/hostDepletion/hg38.mmi"

[conditions.0]
name = "deplete_genome"
control = false
min_chunks = 0
max_chunks = inf
targets = []
single_on = "unblock"
multi_on = "unblock"
single_off = "unblock"
multi_off = "unblock"
no_seq = "proceed"
no_map = "stop_receiving"

## try it out:

readfish validate human_chr_depleteEntireGenome.toml

readfish targets --device MN40608 \
              --experiment-name "deplete_humanGenome" \
              --toml human_chr_depleteEntireGenome.toml \
              --log-file ru_test.log

TOML="/media/vol1/daniel/hostDepletion/human_chr_depleteEntireGenome.toml"
reads="/var/lib/minknow/data/zoop9"
readfish summary $TOML $reads  

## saving as 
readfishSummary_human_chr_depleteGenome.txt


## okay, so how do recover the reads that are not from the genome?

## seems like the way to do it is to exclude all small reads (<1000)
## align the rest to the reference


####### try on our data #####

## great, so why doesn't it work with our genome?

## first, get our combo haploid + mitochond + chloropl genome:

cd /media/vol1/daniel/hostDepletion/ourData

## can we index this?

genome=/media/vol1/daniel/spruce/Pabies_repeatsCompressed_mt_ch.fa
minimap2 -d Pabies_repeatsCompressed_mt_ch.mmi $genome

## we'll play with chris's bulk fast5 file that he created:

ls -lh /media/vol2/chris/Pabies_tests/bulk_fast5_files/MinION-PC_20230621_1522_FAU29445_MN40608_sequencing_run_bigger_e77dd4cd_5395a225.fast5

## it's precious. let's make a copy of it:

cp /media/vol2/chris/Pabies_tests/bulk_fast5_files/MinION-PC_20230621_1522_FAU29445_MN40608_sequencing_run_bigger_e77dd4cd_5395a225.fast5 \
/media/vol1/daniel/hostDepletion/ourData/picea.fast5

chmod 444 picea.fast5

## we need to change our minknow (not readfish) toml. Since we are not 
## actually sequencing, we can continue to mess with the old 106 chemistry 
## config file

vim /opt/ont/minknow/conf/package/sequencing/sequencing_MIN106_DNA.toml

## this might cause issues, because we are telling guppy that this is a 
## 10.4 cell?

## let's see

## as above, we add/substitute the line:

"""
simulation = "/media/vol1/daniel/hostDepletion/ourData/picea.fast5"
"""

## first let's try enriching for chloropolasts:

>chloroplast

# spruce_chloroplastEnrichment.toml
[caller_settings]
config_name = "dna_r10.4.1_e8.2_400bps_fast.cfg"
host = "ipc:///tmp/.guppy/"
port = 5555

[conditions]
reference = "/media/vol1/daniel/hostDepletion/ourData/Pabies_repeatsCompressed_mt_ch.mmi"

[conditions.0]
name = "enrichChloroplast"
control = false
min_chunks = 0
max_chunks = inf
targets = ["chloroplast"]
single_on = "stop_receiving"
multi_on = "stop_receiving"
single_off = "unblock"
multi_off = "unblock"
no_seq = "proceed"
no_map = "proceed"

## check this file
readfish validate spruce_chloroplastEnrichment.toml

## server/client ok? we have a new flow cell/chemistry

python -c 'from pyguppy_client_lib.pyclient import PyGuppyClient as PGC; \
           c = PGC("ipc:///tmp/.guppy/5555", "dna_r10.4.1_e8.2_400bps_fast.cfg"); \
           c.connect(); print(c)'

## nope. We are not alone:
https://community.nanoporetech.com/posts/guppy-config-files-used-by
## but of course nanopore doesn't really answer their question



## FLO-MIN114

## I think we need to restart the guppy server (and client?)

systemctl status guppyd ## config is r9.4.1...

sudo systemctl status minknow ## config is r9.4.1...

cd /opt/ont/guppy/

guppy_basecall_server --config dna_r10.4.1_e8.2_400bps_fast.cfg -p 5555 -l /tmp/guppy -x 'cuda:0'

## but I don't want to start a new server if possible to avoid. 
## I think we need to alter the minknow toml file:

cd /opt/ont/minknow/conf/package/sequencing

cp sequencing_MIN114_DNA_e8_2_400K.toml sequencing_MIN114_DNA_e8_2_400K.toml.bk

## and add in the simulation setting:
vim /opt/ont/minknow/conf/package/sequencing/sequencing_MIN114_DNA_e8_2_400K.toml

"""
simulation = "/media/vol1/daniel/hostDepletion/ourData/picea.fast5"
"""

## restarted the flowcell

## did that help?

python -c 'from pyguppy_client_lib.pyclient import PyGuppyClient as PGC; \
           c = PGC("ipc:///tmp/.guppy/5555", "dna_r10.4.1_e8.2_400bps_fast.cfg"); \
           c.connect(); print(c)'

## nope
## can't get guppy to take the new chemistry...ugh. 

ls -l /tmp/.guppy/5555
sudo chmod 775 /tmp/.guppy/5555

readfish targets --device MN40608 \
              --experiment-name "enrichChloroplast" \
              --toml spruce_chloroplastEnrichment.toml \
              --log-file chloro_test.log


cp /media/vol1/daniel/hostDepletion/human_chr_selection.toml \
/media/vol1/daniel/hostDepletion/ourData/
mv human_chr_selection.toml bigGenome_depletion.toml 

sudo systemctl restart guppyd

sudo systemctl status guppyd

## as before, the gringer lab has some hints about this:

https://gringer.gitlab.io/presentation-notes/2021/10/08/gpu-calling-in-minknow/#verifying-the-configuration-change-1

## ah, here is the description of the guppy settings
cat /etc/systemd/system/guppyd.service

cd /etc/systemd/system/

## this is a link to:
ls /lib/systemd/system/guppyd.service

## make a backup and link to backup:

sudo cp /lib/systemd/system/guppyd.service /lib/systemd/system/guppyd.service.bk

sudo ln -s /lib/systemd/system/guppyd.service.bk /etc/systemd/system/guppyd.service.bk

## try a manual edit?

sudo vim /etc/systemd/system/guppyd.service
## changed the following line:
ExecStart=/opt/ont/guppy/bin/guppy_basecall_server --log_path /var/log/guppy --config dna_r9.4.1_450bps_fast.cfg --num_callers 1 --port /tmp/.guppy/5555 --ipc_threads 3 --device cuda:all
## to:
ExecStart=/opt/ont/guppy/bin/guppy_basecall_server --log_path /var/log/guppy --config dna_r10.4.1_e8.2_400bps_fast.cfg --num_callers 1 --port /tmp/.guppy/5555 --ipc_threads 3 --device cuda:all

## restart:

sudo systemctl restart guppyd ## didn't work. try:

systemctl daemon-reload

sudo systemctl status guppyd ## reports that it is using the new device, r10.4

## will it work with our new data now?

## always have to check the tcp socket permissions
ls -l /tmp/.guppy/5555
sudo chmod 775 /tmp/.guppy/5555

python -c 'from pyguppy_client_lib.pyclient import PyGuppyClient as PGC; \
           c = PGC("ipc:///tmp/.guppy/5555", "dna_r10.4.1_e8.2_400bps_fast.cfg"); \
           c.connect(); print(c)'

## works now, how about the readfish command itself?

cd /media/vol1/daniel/hostDepletion/ourData

readfish targets --device MN40608 \
              --experiment-name "enrichChloroplast" \
              --toml spruce_chloroplastEnrichment.toml \
              --log-file chloro_test.log

## and its running. Let that go for a while...

## the report is probably really long:

TOML="spruce_chloroplastEnrichment.toml"
reads="/var/lib/minknow/data/spruceZoop4"
readfish summary $TOML $reads  

## output is too big, but luckily chloroplast
## contig is at the bottom, here is a piece of 
## it:

 contig  number      sum    min     max    std   mean  median     N50
 MA_9931298       2    5064  1223   3841   1851   2532    2532   3841
 MA_9932586       5    3231   322   1407    447    646     526    648
 MA_9934715       2     993   465    528     45    496     496    528
 MA_9940839       3     924   282    353     39    308     289    289
 MA_9943563       7   10663   489   4001   1214   1523    1172   2041
 MA_9944826       3    1045   346    351      3    348     348    348
 MA_9944965       2     740   370    370      0    370     370    370
   MA_99477       4    2055   347    728    194    514     490    629
 MA_9949434       3    2133   327   1232    468    711     574   1232
 MA_9952592       2    1717   704   1013    218    858     858   1013
 MA_9952612       2     702   312    390     55    351     351    390
 MA_9954524       3    1747   541    606     36    582     600    600
 MA_9957133       4    1626   364    434     31    406     414    426
 MA_9958369       3    1041   267    488    122    347     286    286
   MA_99587       5    1966   245    755    211    393     323    390
 MA_9960185       6    3948   441   1052    215    658     632    677
 MA_9960987       5    4064   321   2113    740    813     565   2113
  MA_996288       8    2982   311    496     71    373     352    353
 MA_9963269       4    2408   421    738    134    602     624    656
 MA_9965332      15   29081   298   7890   2271   1939     787   4391
 MA_9966395      51   52399   278   4916    736   1027     827   1203
 MA_9967845       2    1147   375    772    281    574     574    772
   MA_99688       2    3461   351   3110   1951   1730    1730   3110
 MA_9970702       2    2682  1073   1609    379   1341    1341   1609
 MA_9970721       2    2739   788   1951    822   1370    1370   1951
  MA_997147       2     716   320    396     54    358     358    396
 MA_9972548       2    1174   377    797    297    587     587    797
 MA_9975192       8    4907   344    858    221    613     703    759
 MA_9979455       2     925   364    561    139    462     462    561
 MA_9980845       2     816   307    509    143    408     408    509
  MA_998108       3    1503   472    523     26    501     508    508
  MA_998135       2    1241   365    876    361    620     620    876
   MA_99829       3    2977   410   1871    774    992     696   1871
  MA_998349       3    9066   240   8145   4442   3022     681   8145
  MA_998504       2    1079   372    707    237    540     540    707
 MA_9994173       2    2351   427   1924   1059   1176    1176   1924
   MA_99942      10   10426   312   2082    602   1043     924   1389
 MA_9997692       2    1028   348    680    235    514     514    680
 MA_9997874      10    3797   223   1027    239    380     289    358
   MA_99981       2    2982  1464   1518     38   1491    1491   1518
 MA_9998159       3    2061   316   1256    500    687     489   1256
 MA_9998442       3    1208   318    479     81    403     411    411
 MA_9999734       3    1197   323    450     67    399     424    424
chloroplast     151  625899   249  14565   2979   4145    3546   5652

## looks pretty good. The chloroplast have a much greater sum of BP,
## longer median and mean. 

#### try entire genome depletion ####

## this time let's limit chunks to 3?

## the base documentation is here
https://github.com/LooseLab/readfish/blob/dev_staging/TOML.md#local-basecalling
## the same issue above is useful here:
https://github.com/LooseLab/readfish/issues/242

## try the 
## "bigGenome_depletion.toml":
[caller_settings]
config_name = "dna_r10.4.1_e8.2_400bps_fast.cfg"
host = "ipc:///tmp/.guppy/"
port = 5555

[conditions]
reference = "/media/vol1/daniel/hostDepletion/ourData/Pabies_repeatsCompressed_mt_ch.mmi"

[conditions.0]
name = "depleteSpruceHost"
control = false
min_chunks = 0
max_chunks = 3
targets = []
single_on = "unblock"
multi_on = "unblock"
single_off = "unblock"
multi_off = "unblock"
no_seq = "proceed"
no_map = "stop_receiving"


## try it out:
readfish validate bigGenome_depletion.toml

readfish targets --device MN40608 \
              --experiment-name "depleteSpruceGenome" \
              --toml bigGenome_depletion.toml \
              --log-file depleteSpruce_test.log


## looks like it is working well, the mapping times 
## are really low, ~0.13s

TOML="spruce_chloroplastEnrichment.toml"
reads="/var/lib/minknow/data/spruceZoop4"
readfish summary $TOML $reads  



######## processing, post readfish #########

## I see several ways forward here. 

## the most important info is that the short reads 
## mostly rejected reads. 

## also they are not that useful for 
## metagenomes. 

## so it makes sense to filter them out, right away,
## and see if enough information remains for 
## metagenomes and MAGs. 

## I guess we could run a metagenome assembler on it 
## right away. 

## and do some gene prediction, send it off to 
## kegg

## to report nice results, we need to do both. Means 
## a full metagenome/MAG pipeline. 
## a quality check, etc. 
 
## first step - filter reads <1000 bp

## we have lots of fasta files, here:

ls /var/lib/minknow/data/spruceZoop5/no_sample/20230630_1147_MN40608_spruceZoop5_f5ee253a/fastq_pass/

ls -lh /var/lib/minknow/data/spruceZoop5/no_sample/20230630_1147_MN40608_spruceZoop5_f5ee253a/fastq_pass/

cd /media/vol1/daniel/hostDepletion/ourData/bigGenome_depletion_data

cp /var/lib/minknow/data/spruceZoop5/no_sample/20230630_1147_MN40608_spruceZoop5_f5ee253a/fastq_pass/*fastq.gz .

gunzip *

cat * > bigGenome.fastq

wc -l bigGenome.fastq ## 128000/4 = 32000 reads

grep -c "^@" bigGenome.fastq ## 32001

## is there a seqtk solution?
seqtk seq -L 1000 bigGenome.fastq > lessThan1000.fastq

## get a fastqc report on this

conda activate

conda install -c bioconda fastqc

rm -r qc/
mkdir qc/
fastqc -t 8 -o qc --extract  lessThan1000.fastq

## get it local

rm -r /home/daniel/Documents/projects/fichtelgebirge_project/spruceGenome/dataFromHostDep/qc
file=/media/vol1/daniel/hostDepletion/ourData/bigGenome_depletion_data/qc/
dest=/home/daniel/Documents/projects/fichtelgebirge_project/spruceGenome/dataFromHostDep/
scp -r -i ~/.ssh/id_ed25519 test@132.180.112.115:$file $dest

cd /home/daniel/Documents/projects/fichtelgebirge_project/spruceGenome/dataFromHostDep/qc

firefox lessThan1000_fastqc.html ## huh, actually looks pretty good, an average quality per read ~18. 


python3 
import math
## convert to probability (http://drive5.com/usearch/manual/quality_score.html)

10**(-18/10) ## something like a 1.5% error rate, or 98.5% correct reads/bp

## not bad.

## it might be good to align these reads against the host genome, and see what is left 
## standing...which genome should we use?

## let's use the full genome, with repeats, plus mitochondria assembly, plus chloroplast:

cat Pabies-haploid.fa Picea_abies_mtDNA_assembly.fa Pabies01-chloroplast.fa > Pabies-haploid_withOrganelles_raw.fa

seqtk seq -l 60 Pabies-haploid_withOrganelles_raw.fa > Pabies-haploid_withOrganelles.fa


minimap2 -a ref.fa query.fq > alignment.sam

minimap2 -ax map-ont ref.fa ont-reads.fq > aln.sam 

minimap2 -ax map-ont ref.fa ont-reads.fq > aln.sam 

genome=/media/vol1/daniel/spruce/Pabies-haploid_withOrganelles.fa
reads=/media/vol1/daniel/hostDepletion/ourData/bigGenome_depletion_data/lessThan1000.fastq

minimap2 -t 4 -I 100g  -ax map-ont $genome $reads > bigReadsAlign2Spruce.sam  ## used up to 47 gig RAM. Wow. need de.NBI instance 

## now we want just those that didn't align to host, as a start.
## later we may not want to do this, may result in lower information 
## about primary metabolism...

conda create -n samtools -c bioconda samtools
## for some reason, had to do a reinstall immediately to a lower version
conda install -c bioconda samtools=1.9 --force-reinstall

conda activate samtools 

samtools view 

samtools fastq -f4 bigReadsAlign2Spruce.sam > unalignedSpruceReads.fastq

wc -l unalignedSpruceReads.fastq ## 2412, so only 600 reads, many of which look like junk...

## if we assume 600 reads out of 32000 original reads are maybe microbial ~1.8%, seems reasonable
## then 

wc -l lessThan1000.fastq ## 73180 / 4 = 18295 reads after exclusion of the unblocked reads. 

## 600 / 73180 = ~3.2% of reads. So this doesn't quite double the microbial reads (maybe) in this set. 

## things are rosier if we judge by basepairs, not reads, because the reads are so long.

## on the other hand, aligned again to host might cause a lot of less-than-great alignments
## pulling away from our microbial reads artificially

## and 1000 bp is too harsh of a cutoff. In this dataset, almost all reads are unblocked 
## by 600 bp. Probably because we  

## interesting, at least some of these are bacterial, 
## like @9b04d8a6-be55-4753-8bae-311b90f96137, blasts to plasmid
## and @c1654718-897f-4269-a511-ae62cdd2b380, also plasmid...hmmm
## and @b5106860-7789-4d7d-a1b7-3af28dc88018, maybe also a plasmid, or just part of ecoli genome...
## did we enrich for plasmids?

## anyway, we need to automate this. Our local computer cannot handle even this little dataset. 
://ds.aai.lifescience-ri.eu/ds/?entityID=https%3A%2F%2Fproxy.aai.lifescience-ri.eu%2Fmetadata%2Fbackend.xml&return=https%3A%2F%2Fproxy.aai.lifescience-ri.eu%2Fsaml2sp%2Fdisco# # reminds me we need to request a de.nbi instance, btw...probably today.
## denbi instance quick.
## done.


