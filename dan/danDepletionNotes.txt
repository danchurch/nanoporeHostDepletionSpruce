

git remote set-url origin git@github.com:danchurch/nanoporeHostDepletionSpruce.git

git push -u origin main


## Chris Nuske has been working really hard to 
## get adaptive sampling working for the lab.
## let's see if we can catch up with Chris a bit,

## we'll work mostly on the lab nanopore computer,
## so readfish installs, etc will be there.

## he has scripts here:

## keeping a github repo here:
https://github.com/danchurch/nanoporeHostDepletionSpruce

## first step is to get ReadFish installed in a conda environment,
## on my account on the lab computer.

## we'll follow their instructions on github:
https://github.com/looselab/readfish

## we'll put a copy of their yaml file on the labcomp here:

cd /media/vol1/daniel/hostDepletion

conda env create -f readfish_env.yml

conda activate readfish

readfish

## seems to work. 

## according to Chris, it's important to add ourselves
## to the minknow user group:

groups test

sudo usermod -a -G minknow test

## we can watch minknow on our extra office computer
## do this by clicking on lower left "connection manager"
## and entering the nanoComp computer IP addess:
132.180.112.115

## to test it out, we need a bulk fast5 file
## chris already downloaded a sample bulk fast five,
## it's here

ls /home/chris/PLSP57501_simulation.fast5


## link for me
ln -s /home/chris/PLSP57501_simulation.fast5 /media/vol1/daniel/hostDepletion/sampleBulk.fast5 

## link for lara
sudo ln -s /home/chris/PLSP57501_simulation.fast5 /media/vol2/lara/sampleBulk.fast5 

## we need a TOML file. instructions are here:
https://github.com/LooseLab/readfish/blob/dev_staging/TOML.md

## chris has already worked through a lot of the gotchas
## on the TOML file...

## some are here:
ls /home/chris/*.toml

## also one here, but not what we need, I think.
/media/vol2/chris/channels.toml


## this might be a good one:
less /home/chris/pabies_depletion.toml

## let's make a copy of that for us to play with:

cd /media/vol1/daniel/hostDepletion/

cp  /home/chris/pabies_depletion.toml /media/vol1/daniel/hostDepletion/

## what do we need to change for this?

## the tutorial also mentions using existing toml files 
## from  minknow for templates:

cd /opt/ont/minknow/conf/package/sequencing/

## ah, and I see these sequencing toml files are very different
## from the readfish toml files for adaptice samplig

## chris has already modified this one to look for for his 
## bulkfast5 example like we are doing now:

less /opt/ont/minknow/conf/package/sequencing/sequencing_MIN106_DNA.toml

## back it up, then play with it

cd /opt/ont/minknow/conf/package/sequencing/

cp sequencing_MIN106_DNA.toml sequencing_MIN106_DNA.toml.bk
chmod 444 sequencing_MIN106_DNA.toml.bk

## to keep track of differences
diff sequencing_MIN106_DNA.toml.bk sequencing_MIN106_DNA.toml

## changed the following, under # Sequencing Feature Settings # --> # basic_settings # --> [custom_settings]

"""
simulation = "/media/vol1/daniel/hostDepletion/sampleBulk.fast5"
"""

## changing file to find my bulk fast5 file
## chris already change alignment time limit settings

## we then setup a run using the config test flowcell, 
## and setting the flow cell type to match whatever 
## sequencing toml file we modified (in this case MIN106 DNA, above)

## run started.

## we should be able to eject (reject) all reads currently in 
## pores with readfish:

conda activate readfish

readfish unblock-all --device MN40608 --experiment-name "Testing ReadFish Unblock All"

## the device id is the same as the "position" reported by minknow

## do I need to match the experiment name? nope

## that should just mean that all reads are rejected

## funny, it looks to me like it is still reported the basecalling
## from these unblocked reads, just as very short sequences.

## anyway, looks like it is working. 

## stop run and clean up the files,,,

## files are usually stored here:
/var/lib/minknow/data

## and now let's test their example for enrichment/depletion
## with human chromosome:

## their toml file for enriching a human chromosome:

wget https://raw.githubusercontent.com/LooseLab/readfish/master/examples/human_chr_selection.toml

## we need a human genome and mmi index of it. I think chris 
## found exactly the same genome as is used in their 
## example...

cd /media/vol1/daniel/hostDepletion/

ln -s /home/chris/hg38.fa /media/vol1/daniel/hostDepletion/hg38.fa

## for lara:
sudo ln -s /home/chris/hg38.fa /media/vol2/lara/hg38.fa
ls /home/chris/hg38.fa 
/media/vol1/daniel/hostDepletion/hg38.fa

## make an mmi
minimap2 -d hg38.mmi hg38.fa

## put this in our readfish toml file 

## check the validity of our toml file:

readfish validate 

readfish validate human_chr_selection.toml

## looks good. 

## started a simulation run on minknow. 
## now with readfish

conda activate readfish

cd /media/vol1/daniel/hostDepletion

readfish targets --device MN40608 \
              --experiment-name "debugging22_8_23_try3" \
              --toml /media/vol1/daniel/hostDepletion/human_chr_selection.toml \
              --log-file ru_test.log


less /media/vol1/daniel/hostDepletion/human_chr_selection.toml 


## doesn't work, freezes. possibly due to the guppy server issues 
## chris mentioned. 

## not really sure what this is, a pipe or somethiing,
## but I think we need to change its permissions:

sudo chmod 775 /tmp/.guppy/5555

ls -l /tmp/.guppy/5555

#sudo chmod 777 /tmp/.guppy/5555

## chris has this line for starting a new basecall server:

guppy_basecall_server --config dna_r9.4.1_450bps_fast.cfg -p 5555 -l /tmp/guppy -x 'cuda:0'

## already in use. Stop run and try again. works, but do we really need two guppy 
## servers running?

## generally, you don't. It is just that the config file for guppy might not be right for 
## the simulation. In our case, we using r10 cells for actual data collection, but the 
## simulation requires the older guppy configuration file.


## checklist for getting minion, readfish, and guppy working together for the 
## tutorial:

## make sure a simulated run actually starts with minion. Might take several tries,
## a lot of times errors pop up, when messages like "script failure",
## "internal error", etc. And all you can do is restart the run and/or 
## restart the computer. 

## make sure guppy is running with the right config file. The fast5 files in the
## readfish tutorial are made with an older r9 flow cell, so guppy will 
## probably perform better with the "dna_r9.4.1_450bps_fast.cfg" setting.
## change to this config file, and restart the guppy service.
## to see how to do this, search "resetting guppy with a new config file" below

## make sure guppy client and guppy server are the same version

## make sure you as a user are a member of the minknow group, because...

## make sure the right file permissions are in place for the minknow group,
## not just the minknow user, can access the port for guppy 
## this is usually the /tmp/.guppy/5555 file, indicated in our toml files

sudo chmod 775 /tmp/.guppy/5555

## I had to change the following to the readfish toml:

host= "ipc:///tmp/.guppy/"

## seems to be running. I killed the other manually and it just started again. 
## not sure what to do there. Just leave it I guess

## anyway, with the new basecaller, and the new toml file that looks for it,
## does the above command now work? 

conda activate readfish
readfish targets --device MN40608 \
              --experiment-name "RU Test basecall and map" \
              --toml /media/vol1/daniel/hostDepletion/human_chr_selection.toml \
              --log-file ru_test.log

## nope...

## this issue seems pertinent:
https://github.com/LooseLab/readfish/issues/240

## perhaps also pertinent:
https://github.com/LooseLab/readfish/issues/221

## they mention that our guppy server and client software should line up, version-wise:
guppy_basecall_server --version ## gives us version 6.5.7

pip list | grep pyguppy  ## version 6.4.2

## not the same

## does this help?
pip install ont-pyguppy-client-lib==6.5.7

## they do some diagnostics

## is this necessary to do every time we want to do a 
## new experiment?
ls -l /tmp/.guppy/5555

sudo chmod 775 /tmp/.guppy/5555

less /opt/ont/guppy/data/dna_r9.4.1_450bps_fast.cfg

## they test their guppy server/client with this script:
python -c 'from pyguppy_client_lib.pyclient import PyGuppyClient as PGC; \
           c = PGC("ipc:///tmp/.guppy/5555", "dna_r9.4.1_450bps_fast.cfg"); \
           c.connect(); print(c)'

sudo systemctl status guppyd

## okay, works


readfish targets --device MN40608 \
              --experiment-name "debug_22_8_23_try3" \
              --toml /media/vol1/daniel/hostDepletion/human_chr_selection.toml \
              --log-file readfishtest.log

## and works!!! yay!

## to review,  have to reset user permissions every time guppy server is
## restarted on the socket ./tmp/.guppy/5556
## also need to make sure guppy versions (client and server) match

## so what's next?

## where is the data for this? do I have to stop the run?

## the reads are delivered in batches, here:

/var/lib/minknow/data/zoop5/no_sample/20230628_1557_MN40608_zoop5_f31f0515/fast5_pass

TOML="/media/vol1/daniel/hostDepletion/human_chr_selection.toml"
reads="/var/lib/minknow/data/zoop5"

readfish summary $TOML $reads

## looks like this:

contig  number      sum   min    max    std  mean  median    N50
  chr1    5994  3759112   153  45338   1458   627     487    532
 chr10    2959  1969533   154  49075   2072   666     488    537
 chr11    2949  2046589   200  46773   1861   694     501    561
 chr12    4300  2523930   103  49516   1361   587     494    522
 chr13    2176  1333880   184  39207   1442   613     484    520
 chr14    2917  1940354   179  52401   1949   665     501    543
 chr15    3274  1804635    94  46421   1058   551     472    498
 chr16    1554   970043   192  53194   1912   624     473    517
 chr17    2704  1571430   199  53051   1435   581     486    518
 chr18    2847  1554361   140  15657    612   546     482    503
 chr19    1210   834025   159  21556   1307   689     487    583
  chr2    5918  3675999   130  52366   1354   621     489    530
 chr20    1524  1092416   178  49450   2223   717     496    562
 chr21      30   212798   469  38881   8723  7093    4316  14064
 chr22      55   423294   388  58385  11798  7696    3706  15818
  chr3    5608  3597901   144  61175   1558   642     503    541
  chr4    6637  4105216   155  55008   1667   619     492    535
  chr5    4607  3008922   216  43932   1476   653     494    542
  chr6    4001  2628431   166  54594   1754   657     492    538
  chr7    4103  2343278   145  49974   1057   571     498    524
  chr8    3607  2372494   207  48653   1691   658     493    545
  chr9    2988  1966598   193  46570   1829   658     482    532
  chrM     169   174679   273  16400   2092  1034     543   1135
  chrX    4967  3053079   152  52969   1627   615     479    523
  chrY      14    10745   372   2246    499   768     665    993


## not sure what the "number" column indicates
## but the thing to look for is the median/mean/n50
## the low average read length of the other (not 21/22)
## contigs is indicative that they were rejected 
## after alignment, usually around 500-600 bp in our case.

## I guess this means we would need to exclude low-length reads,
## to exclude the non-targeted reads.

## for making readfish TOMLS, there is a good table on this in the nature paper, table 2.

## for the record, the toml for the above experiment looks like:

"""
[caller_settings]
config_name = "dna_r9.4.1_450bps_hac"
host = "ipc:///tmp/.guppy/"
port = 5555

[conditions]
reference = "/media/vol1/daniel/hostDepletion/hg38.mmi"

[conditions.0]
name = "select_chr_21_22"
control = false
min_chunks = 0
max_chunks = inf
targets = ["chr21", "chr22"]
single_on = "stop_receiving"
multi_on = "stop_receiving"
single_off = "unblock"
multi_off = "unblock"
no_seq = "proceed"
no_map = "proceed"
"""

## the above is modified from:
https://github.com/LooseLab/readfish/blob/master/examples/human_chr_selection.toml


## can we play with this a little? For instance, what happens  if:
## 1) we change "stop_receiving" to "unblock" for all non-target reads?
## 2) we invert, and deplete these two target sequences 
## 3) we block everything that aligns to the genome?


## 1) we change "stop_receiving" to "unblock" for all non-target reads?

cp human_chr_selection.toml human_chr_selection_stop2unblock.toml

## so weird. Looking at their readfish toml, the have everything on 
## target as "stop_receiving". Makes me think that this is the
## command for accepting and sequencing the read. 

## if that is correct, then changing these to unblock would 
## reject most things. I think resulting in everything having
## a read length average of ~500 bp. 

## and just got confirmation from the loose lab github folks

"""
Proceed means that one collects more data for an individual read and you assess it again. Stop_receiving tells the sequencer to keep sequencing that read and not evaluate it again.

So -
unblock means a read will be rejected from the pore and a new one sampled.
proceed means the read will continue to sequence and the next batch of singal will be analysed again.
stop_receiving means send no more data about this read and let it sequence to normal completion.

I hope that helps!
"""

## find this here:
https://github.com/LooseLab/readfish/issues/242

## Let's see. Here is the TOML

## human_chr_selection_stop2unblock.toml
[caller_settings]
config_name = "dna_r9.4.1_450bps_hac"
host = "ipc:///tmp/.guppy/"
port = 5555

[conditions]
reference = "/media/vol1/daniel/hostDepletion/hg38.mmi"

[conditions.0]
name = "select_chr_21_22_stop2unblock"
control = false
min_chunks = 0
max_chunks = inf
targets = ["chr21", "chr22"]
single_on = "unblock"
multi_on = "unblock"
single_off = "unblock"
multi_off = "unblock"
no_seq = "proceed"
no_map = "proceed"

## run the readfish command with this new toml

## as always, make sure we have group permission for the socket
ls -l /tmp/.guppy/5555

conda activate readfish

## check that the client and server are talking:
python -c 'from pyguppy_client_lib.pyclient import PyGuppyClient as PGC; \
           c = PGC("ipc:///tmp/.guppy/5555", "dna_r9.4.1_450bps_fast.cfg"); \
           c.connect(); print(c)'

## yup. onto readfish
readfish targets --device MN40608 \
              --experiment-name "stop_receive2unblock" \
              --toml /media/vol1/daniel/hostDepletion/human_chr_selection_stop2unblock.toml \
              --log-file ru_test.log

## running, let it go for a while


TOML="/media/vol1/daniel/hostDepletion/human_chr_selection_stop2unblock.toml"
reads="/var/lib/minknow/data/zoop7"
readfish summary $TOML $reads

## saving this as stop2unblock.txt, but cleaned up version here:

"""
contig  number      sum   min     max   std  mean  median    N50
  chr1    8166  5297756   162   29411  1084   649     500    547
  chr2   10335  6376134   153   50357  1085   617     491    532
  chr3    7447  4992369   203   29286  1261   670     513    557
  chr4    8234  5712847   110   29541  1290   694     505    571
  chr5    5813  4295677   195   28837  1472   739     500    598
  chr6    5877  3905658   186   67939  1415   665     508    552
  chr7    5915  3850456   133   32864  1092   651     509    550
  chr8    4520  3180204   202   29718  1260   704     511    584
  chr9    4843  3443058   189  166227  2660   711     499    560
 chr10    4548  2948968   162   28674  1098   648     502    549
 chr11    4149  2862933   175   34784  1406   690     498    555
 chr12    4310  3044324   197   55612  1394   706     515    578
 chr13    2912  1845003   206   29650  1132   634     499    541
 chr14    3136  2463883   173   38963  1753   786     510    639
 chr15    5912  3623898   128   89365  1750   613     488    524
 chr16    2234  1589472   169   36303  1413   711     496    599
 chr17    2521  1922530   225   95271  2583   763     501    604
 chr18    4134  2647552   190   70140  1510   640     500    542
 chr19    1635  1494578   183  107938  3081   914     545    807
 chr20    1078  1054159   187   40566  2437   978     533   1051
 chr21    1200   970358   217   27835  1655   809     522    686
 chr22     721   554628   211   29682  1726   769     517    647
  chrM     226   208691   271   15379  1701   923     566    753
  chrX    5835  4039849   189   30215  1323   692     505    567
  chrY      23    15382   373    1034   192   669     645    722
"""

## yup. Unlike before, all chromosomes have an average of ~500 bp,
## including 21 and 22. So we basically just blocked everything.

## we'll want to do something like this below, in #3 and with our 
## data. Except that the unmapped reads should all be set to 
## "stop_receiving"


## 2) we invert, and deplete these two target sequences, sequence everything else:

human_chr_deplete21_22.toml
"""
[caller_settings]
config_name = "dna_r9.4.1_450bps_hac"
host = "ipc:///tmp/.guppy/"
port = 5555

[conditions]
reference = "/media/vol1/daniel/hostDepletion/hg38.mmi"

[conditions.0]
name = "deplete_chr_21_22"
control = false
min_chunks = 0
max_chunks = inf
targets = ["chr21", "chr22"]
single_on = "unblock"
multi_on = "unblock"
single_off = "stop_receiving"
multi_off = "stop_receiving"
no_seq = "proceed"
no_map = "proceed"
"""

conda activate readfish

## check that the client and server are talking:
python -c 'from pyguppy_client_lib.pyclient import PyGuppyClient as PGC; \
           c = PGC("ipc:///tmp/.guppy/5555", "dna_r9.4.1_450bps_fast.cfg"); \
           c.connect(); print(c)'

## yup. onto readfish
readfish targets --device MN40608 \
              --experiment-name "deplete21_22" \
              --toml /media/vol1/daniel/hostDepletion/human_chr_deplete21_22.toml \
              --log-file ru_test.log


## check the results:

TOML="/media/vol1/daniel/hostDepletion/human_chr_deplete21_22.toml"
reads="/var/lib/minknow/data/zoop8"
readfish summary $TOML $reads > readfishSummary_human_chr_deplete21_22.txt

## redirect doesn't work. anyway, cleaned up, looks like this:

contig  number      sum    min     max    std   mean  median     N50
  chr1     203  1985656    223  143583  18456   9782    1921   35106
  chr2     194  1940543    216  239229  26135  10003    2192   37848
  chr3     195  1973224    249  321947  29054  10119    1950   41776
  chr4     145  2620134    210  309287  40913  18070    2196   85360
  chr5     191  2117429    259  191101  26404  11086    1865   43932
  chr6     191  1412928    279  151998  16849   7398    1904   32483
  chr7     114  1574735    277  242490  29940  13813    3518   49974
  chr8     148  1255656    304  118447  16594   8484    1776   34932
  chr9     132  1316892    216  179598  23458   9976    1798   43623
 chr10     100  1076103    291  130248  19205  10761    2718   35462
 chr11     135  1087374    215   96666  15629   8055    1805   33647
 chr12     112  1235755    257  208411  24379  11034    1993   35706
 chr13      95   617561    220  206157  24169   6501    1028  103486
 chr14      95   975288    265  149505  25753  10266    1076   46059
 chr15      90  1057315    289  158004  27721  11748    4298   40799
 chr16      55   792690    241  241976  35456  14413    3260   62880
 chr17      71   582331    408  184443  24092   8202    1605   52554
 chr18      47   632308    380  146314  26010  13453    4060   47291
 chr19      87   570863    384  142612  18646   6562    1295   28548
 chr20      55   759285    249  156527  25388  13805    2097   40661
 chr21     903   668203    231  195532   6559    740     428     632
 chr22     366   318060    208   42955   2769    869     444    1195
  chrM      12    87633    555   16467   7057   7303    4484   16362
  chrX     132  2064894    213  321374  41516  15643    2251  134053

## seems to work, chromosome 21 and 22 reads hover around rejection
## size, the others around ~10,000
~

## 3) we block everything that aligns to the genome?

## pertinent is issue 242:
https://github.com/LooseLab/readfish/issues/242

#cp human_chr_selection.toml human_chr_depleteEntireGenome.toml
vim human_chr_depleteEntireGenome.toml

chmod 777 human_chr_depleteEntireGenome.toml

## looks like this:

#human_chr_depleteEntireGenome.toml
[caller_settings]
config_name = "dna_r9.4.1_450bps_hac"
host = "ipc:///tmp/.guppy/"
port = 5555

[conditions]
reference = "/media/vol1/daniel/hostDepletion/hg38.mmi"

[conditions.0]
name = "deplete_genome"
control = false
min_chunks = 0
max_chunks = inf
targets = []
single_on = "unblock"
multi_on = "unblock"
single_off = "unblock"
multi_off = "unblock"
no_seq = "proceed"
no_map = "stop_receiving"

## try it out:

readfish validate human_chr_depleteEntireGenome.toml

readfish targets --device MN40608 \
              --experiment-name "deplete_humanGenome" \
              --toml human_chr_depleteEntireGenome.toml \
              --log-file ru_test.log

TOML="/media/vol1/daniel/hostDepletion/human_chr_depleteEntireGenome.toml"
reads="/var/lib/minknow/data/zoop9"
readfish summary $TOML $reads  

## saving as 
readfishSummary_human_chr_depleteGenome.txt


## okay, so how do recover the reads that are not from the genome?

## seems like the way to do it is to exclude all small reads (<1000)
## align the rest to the reference


####### try on our data #####

## great, so why doesn't it work with our genome?

## first, get our combo haploid + mitochond + chloropl genome:

cd /media/vol1/daniel/hostDepletion/ourData

## can we index this?

genome=/media/vol1/daniel/spruce/Pabies_repeatsCompressed_mt_ch.fa
minimap2 -d Pabies_repeatsCompressed_mt_ch.mmi $genome

## copy of genome for lara:

sudo ln -s /media/vol1/daniel/spruce/Pabies_repeatsCompressed_mt_ch.fa \
  /media/vol2/lara/PabiesGenome_repeatsCompressed_mt_ch.fa

## we'll play with chris's bulk fast5 file that he created:

ls -lh /media/vol2/chris/Pabies_tests/bulk_fast5_files/MinION-PC_20230621_1522_FAU29445_MN40608_sequencing_run_bigger_e77dd4cd_5395a225.fast5

## it's precious. let's make a copy of it:

cp /media/vol2/chris/Pabies_tests/bulk_fast5_files/MinION-PC_20230621_1522_FAU29445_MN40608_sequencing_run_bigger_e77dd4cd_5395a225.fast5 \
/media/vol1/daniel/hostDepletion/ourData/picea.fast5

## and make a link for lara:

cd /media/vol2/chris/Pabies_tests/bulk_fast5_files/

sudo ln -s /media/vol2/chris/Pabies_tests/bulk_fast5_files/MinION-PC_20230621_1522_FAU29445_MN40608_sequencing_run_bigger_e77dd4cd_5395a225.fast5 \
  /media/vol2/lara/spruceSimulation.fast5

chmod 444 picea.fast5

## we need to change our minknow (not readfish) toml. Since we are not 
## actually sequencing, we can continue to mess with the old 106 chemistry 
## config file

vim /opt/ont/minknow/conf/package/sequencing/sequencing_MIN106_DNA.toml

## this might cause issues, because we are telling guppy that this is a 
## 10.4 cell?

## let's see

## as above, we add/substitute the line:

"""
simulation = "/media/vol1/daniel/hostDepletion/ourData/picea.fast5"
"""

## first let's try enriching for chloropolasts:

>chloroplast

# spruce_chloroplastEnrichment.toml
[caller_settings]
config_name = "dna_r10.4.1_e8.2_400bps_fast.cfg"
host = "ipc:///tmp/.guppy/"
port = 5555

[conditions]
reference = "/media/vol1/daniel/hostDepletion/ourData/Pabies_repeatsCompressed_mt_ch.mmi"

[conditions.0]
name = "enrichChloroplast"
control = false
min_chunks = 0
max_chunks = inf
targets = ["chloroplast"]
single_on = "stop_receiving"
multi_on = "stop_receiving"
single_off = "unblock"
multi_off = "unblock"
no_seq = "proceed"
no_map = "proceed"

## check this file
readfish validate spruce_chloroplastEnrichment.toml

## server/client ok? we have a new flow cell/chemistry

python -c 'from pyguppy_client_lib.pyclient import PyGuppyClient as PGC; \
           c = PGC("ipc:///tmp/.guppy/5555", "dna_r10.4.1_e8.2_400bps_fast.cfg"); \
           c.connect(); print(c)'

## nope. We are not alone:
https://community.nanoporetech.com/posts/guppy-config-files-used-by
## but of course nanopore doesn't really answer their question



## FLO-MIN114

## I think we need to restart the guppy server (and client?)

systemctl status guppyd ## config is r9.4.1...

sudo systemctl status minknow ## config is r9.4.1...

cd /opt/ont/guppy/

## it seems like, for the current model/chemistry  at time of this writing
## which is an R10.4.1 flow cell and SQK-LSK-114 chemistry (which is what we are using)
## this issue seems to be pertinent:

https://github.com/LooseLab/readfish/issues/250

## we can start a new basecaller with this command

guppy_basecall_server --config dna_r10.4.1_e8.2_400bps_fast.cfg -p 5555 -l /tmp/guppy -x 'cuda:0'

## but I don't want to start a new server if possible to avoid. 
## I think we need to alter the minknow toml file:

cd /opt/ont/minknow/conf/package/sequencing

cp sequencing_MIN114_DNA_e8_2_400K.toml sequencing_MIN114_DNA_e8_2_400K.toml.bk

## and add in the simulation setting:
vim /opt/ont/minknow/conf/package/sequencing/sequencing_MIN114_DNA_e8_2_400K.toml

"""
simulation = "/media/vol1/daniel/hostDepletion/ourData/picea.fast5"
"""

## restarted the flowcell

## did that help?

python -c 'from pyguppy_client_lib.pyclient import PyGuppyClient as PGC; \
           c = PGC("ipc:///tmp/.guppy/5555", "dna_r10.4.1_e8.2_400bps_fast.cfg"); \
           c.connect(); print(c)'

## nope
## can't get guppy to take the new chemistry...ugh. 

ls -l /tmp/.guppy/5555
sudo chmod 775 /tmp/.guppy/5555

readfish targets --device MN40608 \
              --experiment-name "enrichChloroplast" \
              --toml spruce_chloroplastEnrichment.toml \
              --log-file chloro_test.log


cp /media/vol1/daniel/hostDepletion/human_chr_selection.toml \
/media/vol1/daniel/hostDepletion/ourData/
mv human_chr_selection.toml bigGenome_depletion.toml 

sudo systemctl restart guppyd

sudo systemctl status guppyd

systemctl status guppyd


######### resetting guppy with a new config file #####

## as before, the gringer lab has some hints about this:

https://gringer.gitlab.io/presentation-notes/2021/10/08/gpu-calling-in-minknow/#verifying-the-configuration-change-1

## ah, here is the description of the guppy settings
cat /etc/systemd/system/guppyd.service

cd /etc/systemd/system/

## this is a link to:
ls /lib/systemd/system/guppyd.service

## make a backup and link to backup:

sudo cp /lib/systemd/system/guppyd.service /lib/systemd/system/guppyd.service.bk

sudo ln -s /lib/systemd/system/guppyd.service.bk /etc/systemd/system/guppyd.service.bk

less /etc/systemd/system/guppyd.service

## try a manual edit?

## we want to use the r9 setting for our simulations, but an r10 cfg for our 
sudo vim /etc/systemd/system/guppyd.service

## changed the following line:
ExecStart=/opt/ont/guppy/bin/guppy_basecall_server --log_path /var/log/guppy --config dna_r9.4.1_450bps_fast.cfg --num_callers 1 --port /tmp/.guppy/5555 --ipc_threads 3 --device cuda:all
## to:
ExecStart=/opt/ont/guppy/bin/guppy_basecall_server --log_path /var/log/guppy --config dna_r10.4.1_e8.2_400bps_fast.cfg --num_callers 1 --port /tmp/.guppy/5555 --ipc_threads 3 --device cuda:all

## restart:

sudo systemctl restart guppyd ## didn't work. try:

systemctl daemon-reload

sudo systemctl status guppyd ## reports that it is using the new device, r10.4

## will it work with our new data now?

## always have to check the tcp socket permissions
ls -l /tmp/.guppy/5555
sudo chmod 775 /tmp/.guppy/5555

python -c 'from pyguppy_client_lib.pyclient import PyGuppyClient as PGC; \
           c = PGC("ipc:///tmp/.guppy/5555", "dna_r10.4.1_e8.2_400bps_fast.cfg"); \
           c.connect(); print(c)'

## works now, how about the readfish command itself?

cd /media/vol1/daniel/hostDepletion/ourData

readfish targets --device MN40608 \
              --experiment-name "enrichChloroplast" \
              --toml spruce_chloroplastEnrichment.toml \
              --log-file chloro_test.log

less chloro_test.log

## and its running. Let that go for a while...

## the report is probably really long:

TOML="spruce_chloroplastEnrichment.toml"
reads="/var/lib/minknow/data/spruceZoop4"
readfish summary $TOML $reads  

## output is too big, but luckily chloroplast
## contig is at the bottom, here is a piece of 
## it:

 contig  number      sum    min     max    std   mean  median     N50
 MA_9931298       2    5064  1223   3841   1851   2532    2532   3841
 MA_9932586       5    3231   322   1407    447    646     526    648
 MA_9934715       2     993   465    528     45    496     496    528
 MA_9940839       3     924   282    353     39    308     289    289
 MA_9943563       7   10663   489   4001   1214   1523    1172   2041
 MA_9944826       3    1045   346    351      3    348     348    348
 MA_9944965       2     740   370    370      0    370     370    370
   MA_99477       4    2055   347    728    194    514     490    629
 MA_9949434       3    2133   327   1232    468    711     574   1232
 MA_9952592       2    1717   704   1013    218    858     858   1013
 MA_9952612       2     702   312    390     55    351     351    390
 MA_9954524       3    1747   541    606     36    582     600    600
 MA_9957133       4    1626   364    434     31    406     414    426
 MA_9958369       3    1041   267    488    122    347     286    286
   MA_99587       5    1966   245    755    211    393     323    390
 MA_9960185       6    3948   441   1052    215    658     632    677
 MA_9960987       5    4064   321   2113    740    813     565   2113
  MA_996288       8    2982   311    496     71    373     352    353
 MA_9963269       4    2408   421    738    134    602     624    656
 MA_9965332      15   29081   298   7890   2271   1939     787   4391
 MA_9966395      51   52399   278   4916    736   1027     827   1203
 MA_9967845       2    1147   375    772    281    574     574    772
   MA_99688       2    3461   351   3110   1951   1730    1730   3110
 MA_9970702       2    2682  1073   1609    379   1341    1341   1609
 MA_9970721       2    2739   788   1951    822   1370    1370   1951
  MA_997147       2     716   320    396     54    358     358    396
 MA_9972548       2    1174   377    797    297    587     587    797
 MA_9975192       8    4907   344    858    221    613     703    759
 MA_9979455       2     925   364    561    139    462     462    561
 MA_9980845       2     816   307    509    143    408     408    509
  MA_998108       3    1503   472    523     26    501     508    508
  MA_998135       2    1241   365    876    361    620     620    876
   MA_99829       3    2977   410   1871    774    992     696   1871
  MA_998349       3    9066   240   8145   4442   3022     681   8145
  MA_998504       2    1079   372    707    237    540     540    707
 MA_9994173       2    2351   427   1924   1059   1176    1176   1924
   MA_99942      10   10426   312   2082    602   1043     924   1389
 MA_9997692       2    1028   348    680    235    514     514    680
 MA_9997874      10    3797   223   1027    239    380     289    358
   MA_99981       2    2982  1464   1518     38   1491    1491   1518
 MA_9998159       3    2061   316   1256    500    687     489   1256
 MA_9998442       3    1208   318    479     81    403     411    411
 MA_9999734       3    1197   323    450     67    399     424    424
chloroplast     151  625899   249  14565   2979   4145    3546   5652

## looks pretty good. The chloroplast have a much greater sum of BP,
## longer median and mean. 

#### try entire genome depletion ####

## this time let's limit chunks to 3?

## the base documentation is here
https://github.com/LooseLab/readfish/blob/dev_staging/TOML.md#local-basecalling
## the same issue above is useful here:
https://github.com/LooseLab/readfish/issues/242

## try the 
## "bigGenome_depletion.toml":
[caller_settings]
config_name = "dna_r10.4.1_e8.2_400bps_fast.cfg"
host = "ipc:///tmp/.guppy/"
port = 5555

[conditions]
reference = "/media/vol1/daniel/hostDepletion/ourData/Pabies_repeatsCompressed_mt_ch.mmi"

[conditions.0]
name = "depleteSpruceHost"
control = false
min_chunks = 0
max_chunks = 3
targets = []
single_on = "unblock"
multi_on = "unblock"
single_off = "unblock"
multi_off = "unblock"
no_seq = "proceed"
no_map = "stop_receiving"


## try it out:
readfish validate bigGenome_depletion.toml

readfish targets --device MN40608 \
              --experiment-name "depleteSpruceGenome" \
              --toml bigGenome_depletion.toml \
              --log-file depleteSpruce_test.log

## where does this go?
sudo find -name "depleteSpruceGenome" -type d &

## looks like it is working well, the mapping times 
## are really low, ~0.13s

TOML="spruce_chloroplastEnrichment.toml"
reads="/var/lib/minknow/data/spruceZoop4"
readfish summary $TOML $reads  


######## processing, post readfish #########

## I see several ways forward here. 

## the most important info is that the short reads 
## mostly rejected reads. 

## also they are not that useful for 
## metagenomes. 

## so it makes sense to filter them out, right away,
## and see if enough information remains for 
## metagenomes and MAGs. 

## I guess we could run a metagenome assembler on it 
## right away. 

## and do some gene prediction, send it off to 
## kegg

## to report nice results, we need to do both. Means 
## a full metagenome/MAG pipeline. 
## a quality check, etc. 
 
## first step - filter reads <1000 bp

## we have lots of fasta files, here:

ls /var/lib/minknow/data/spruceZoop5/no_sample/20230630_1147_MN40608_spruceZoop5_f5ee253a/fastq_pass/

ls -lh /var/lib/minknow/data/spruceZoop5/no_sample/20230630_1147_MN40608_spruceZoop5_f5ee253a/fastq_pass/

cd /media/vol1/daniel/hostDepletion/ourData/bigGenome_depletion_data

cp /var/lib/minknow/data/spruceZoop5/no_sample/20230630_1147_MN40608_spruceZoop5_f5ee253a/fastq_pass/*fastq.gz .

gunzip *

cat * > bigGenome.fastq

wc -l bigGenome.fastq ## 128000/4 = 32000 reads

grep -c "^@" bigGenome.fastq ## 32001

## is there a seqtk solution?
seqtk seq -L 1000 bigGenome.fastq > lessThan1000.fastq

## get a fastqc report on this

conda activate

conda install -c bioconda fastqc

rm -r qc/
mkdir qc/
fastqc -t 8 -o qc --extract  lessThan1000.fastq

## get it local

rm -r /home/daniel/Documents/projects/fichtelgebirge_project/spruceGenome/dataFromHostDep/qc
file=/media/vol1/daniel/hostDepletion/ourData/bigGenome_depletion_data/qc/
dest=/home/daniel/Documents/projects/fichtelgebirge_project/spruceGenome/dataFromHostDep/
scp -r -i ~/.ssh/id_ed25519 test@132.180.112.115:$file $dest

cd /home/daniel/Documents/projects/fichtelgebirge_project/spruceGenome/dataFromHostDep/qc

firefox lessThan1000_fastqc.html ## huh, actually looks pretty good, an average quality per read ~18. 


python3 
import math
## convert to probability (http://drive5.com/usearch/manual/quality_score.html)

10**(-18/10) ## something like a 1.5% error rate, or 98.5% correct reads/bp

## not bad.

## it might be good to align these reads against the host genome, and see what is left 
## standing...which genome should we use?

## let's use the full genome, with repeats, plus mitochondria assembly, plus chloroplast:

cat Pabies-haploid.fa Picea_abies_mtDNA_assembly.fa Pabies01-chloroplast.fa > Pabies-haploid_withOrganelles_raw.fa

seqtk seq -l 60 Pabies-haploid_withOrganelles_raw.fa > Pabies-haploid_withOrganelles.fa


minimap2 -a ref.fa query.fq > alignment.sam

minimap2 -ax map-ont ref.fa ont-reads.fq > aln.sam 

minimap2 -ax map-ont ref.fa ont-reads.fq > aln.sam 

genome=/media/vol1/daniel/spruce/Pabies-haploid_withOrganelles.fa
reads=/media/vol1/daniel/hostDepletion/ourData/bigGenome_depletion_data/lessThan1000.fastq

minimap2 -t 4 -I 100g  -ax map-ont $genome $reads > bigReadsAlign2Spruce.sam  ## used up to 47 gig RAM. Wow. need de.NBI instance 

## now we want just those that didn't align to host, as a start.
## later we may not want to do this, may result in lower information 
## about primary metabolism...

conda create -n samtools -c bioconda samtools
## for some reason, had to do a reinstall immediately to a lower version
conda install -c bioconda samtools=1.9 --force-reinstall

conda activate samtools 

samtools view 

samtools fastq -f4 bigReadsAlign2Spruce.sam > unalignedSpruceReads.fastq

wc -l unalignedSpruceReads.fastq ## 2412, so only 600 reads, many of which look like junk...

## if we assume 600 reads out of 32000 original reads are maybe microbial ~1.8%, seems reasonable
## then 

wc -l lessThan1000.fastq ## 73180 / 4 = 18295 reads after exclusion of the unblocked reads. 

## 600 / 73180 = ~3.2% of reads. So this doesn't quite double the microbial reads (maybe) in this set. 

## things are rosier if we judge by basepairs, not reads, because the reads are so long.

## on the other hand, aligned again to host might cause a lot of less-than-great alignments
## pulling away from our microbial reads artificially

## so another way to gauge the potential amount of microbial information in this data set is to 
## align to known microbial sequences rather than to eukaryotic host and subtracting

## and 1000 bp is too harsh of a cutoff. In this dataset, almost all reads are unblocked 
## by 600 bp. Probably because we  

## interesting, at least some of these are bacterial, 
## like @9b04d8a6-be55-4753-8bae-311b90f96137, blasts to plasmid
## and @c1654718-897f-4269-a511-ae62cdd2b380, also plasmid...hmmm
## and @b5106860-7789-4d7d-a1b7-3af28dc88018, maybe also a plasmid, or just part of ecoli genome...
## did we enrich for plasmids?

## anyway, we need to automate this. Our local computer cannot handle even this little dataset. 
://ds.aai.lifescience-ri.eu/ds/?entityID=https%3A%2F%2Fproxy.aai.lifescience-ri.eu%2Fmetadata%2Fbackend.xml&return=https%3A%2F%2Fproxy.aai.lifescience-ri.eu%2Fsaml2sp%2Fdisco# # reminds me we need to request a de.nbi instance, btw...probably today.
## denbi instance quick.
## done.

## the simulation is still running - is there some way to know how many reads are in the 
## 

ls -lh /media/vol2/chris/Pabies_tests/bulk_fast5_files/MinION-PC_20230621_1522_FAU29445_MN40608_sequencing_run_bigger_e77dd4cd_5395a225.fast5

## I think this is from this run:
cd /var/lib/minknow/data/Pabies_bulk_fast5/bigger/20230621_1517_MN40608_FAU29445_e77dd4cd
du -sh ## 2.1 G

## he also has this run:
cd /var/lib/minknow/data/Pabies_bulk_fast5/Version_1/20230621_1347_MN40608_FAU29445_a01d2181
du -sh ## 650 M

cd /var/lib/minknow/data/Pabies_bulk_fast5/

## not sure 
## this is only 112M of passed fastq sequence data. That can't possibly be what I'm simulating with...
## I'll have to ask Chris

## anyway, stop the simulated run for now. We are already at the edge of computational resources for alignments, etc. 

## generally good news, looks like our setup can handle the large genome, at least in the simulations. 

## the plan:

## run the above pipeline for "full" dataset, then go further by 
## downloading some kind of prokaryote database from NCBI

## data is here:

cd /var/lib/minknow/data/spruceZoop5/no_sample/20230630_1147_MN40608_spruceZoop5_f5ee253a/fastq_pass/

cp /var/lib/minknow/data/spruceZoop5/no_sample/20230630_1147_MN40608_spruceZoop5_f5ee253a/fastq_pass/*fastq.gz ./

cd /media/vol1/daniel/hostDepletion/ourData/bigGenome_depletion_data/bigDataRedo

gunzip *fastq.gz

cat *.fastq > comboSpruceZoop5.fastq 

## let's go down to 500bp, the read distribution indicates this is a pretty 
## good cutoff for the readfish rejection threshold

seqtk seq -L 500 comboSpruceZoop5.fastq > comboSpruceZoop5_onlyBigReads.fastq

grep -c "^@" comboSpruceZoop5.fastq  ## 108,743 reads
grep -c "^@" comboSpruceZoop5_onlyBigReads.fastq  ## 68,902 reads

## fastqc

conda activate 

mkdir qcBig/
fastqc -t 8 -o qcBig --extract comboSpruceZoop5_onlyBigReads.fastq

mkdir qcSmall/

fastqc -t 8 -o qcSmall --extract comboSpruceZoop5.fastq

#file=/media/vol1/daniel/hostDepletion/ourData/bigGenome_depletion_data/bigDataRedo/qcSmall/

file=/media/vol1/daniel/hostDepletion/ourData/bigGenome_depletion_data/bigDataRedo/qcBig/

dest=/home/daniel/Documents/projects/fichtelgebirge_project/spruceGenome/dataFromHostDep/qc/

scp -r -i ~/.ssh/id_ed25519 test@132.180.112.115:$file $dest

## run these through same pipeline as above  

genome=/media/vol1/daniel/spruce/Pabies-haploid_withOrganelles.fa
reads=/media/vol1/daniel/hostDepletion/ourData/bigGenome_depletion_data/bigDataRedo/comboSpruceZoop5_onlyBigReads.fastq
minimap2 -t 4 -I 100g  -ax map-ont $genome $reads > bigReadsAlign2Spruce.sam  ## too memory hungry, process killed

## can we put guppy on pause for a minute?
cd /lib/systemd/system/

#sudo cp /lib/systemd/system/guppyd.service /lib/systemd/system/guppyd.service.bk

sudo vim /lib/systemd/system/guppyd.service 

## leaving for a while. Clean up the minknow 114 flowcell sequencing toml for Chris, he may need it

vim /opt/ont/minknow/conf/package/sequencing/sequencing_MIN114_DNA_e8_2_400K.toml
## commented out line 179 in that file

##### 22.7.23 #####

## we have a new experiment running, with host depletion running

## let's check on it, and see if it seems like we should keep it
## running 

## data is here, on lab comp

ls /var/lib/minknow/data/repeatingDansStuff1/21_7_23/20230721_1214_MN40608_FAS81869_d4801a32

cd /var/lib/minknow/data/repeatingDansStuff1/21_7_23/20230721_1214_MN40608_FAS81869_d4801a32

wc -l unblocked_read_ids.txt ## 367603 reads blocked

## manually check a few. still on nanoComp

FAS81869_pass_d4801a32_55579c61_47.fastq.gz

mkdir -p /media/vol1/daniel/spruce/hostDepletion_7.22.23

cp FAS81869_pass_d4801a32_55579c61_47.fastq.gz /media/vol1/daniel/spruce/hostDepletion_7.22.23/

cd /media/vol1/daniel/spruce/hostDepletion_7.22.23/

gunzip FAS81869_pass_d4801a32_55579c61_47.fastq.gz

## keep only the long reads:

seqtk seq -L 500 FAS81869_pass_d4801a32_55579c61_47.fastq > FAS81869_pass_d4801a32_55579c61_47_onlyBigReads.fastq

grep -c "^@" FAS81869_pass_d4801a32_55579c61_47.fastq ## 4000
grep -c "^@" FAS81869_pass_d4801a32_55579c61_47_onlyBigReads.fastq ## 2819

less FAS81869_pass_d4801a32_55579c61_47_onlyBigReads.fastq 

## make some backups...

## the good stuff (passed reads) are already compressed, here:

cd /var/lib/minknow/data/repeatingDansStuff1/21_7_23/20230721_1214_MN40608_FAS81869_d4801a32

tar cvf hostDepletion_7.22.23.tar ./fastq_pass

## now put this on home computer and de.nbi computer.
## would put this on officeDesktop but it's off. 

path2get="/var/lib/minknow/data/repeatingDansStuff1/21_7_23/20230721_1214_MN40608_FAS81869_d4801a32/hostDepletion_7.22.23.tar"
path2put="/home/daniel/Documents/projects/fichtelgebirge_project/spruceGenome/depletion2big4github/spruceDepletion_7.22.23/"
scp -i /home/daniel/.ssh/./id_ed25519 test@132.180.112.115:$path2get $path2put

## get it on denbi:
path2get="/home/daniel/Documents/projects/fichtelgebirge_project/spruceGenome/depletion2big4github/spruceDepletion_7.22.23/hostDepletion_7.22.23.tar"
path2put="/vol/piceaNanopore/dan/"
scp -P 30419 -i /home/daniel/.ssh/./id_ed25519 $path2get ubuntu@129.70.51.6:$path2put

## get it on the officeComp that lara is using:
path2get="/home/daniel/Documents/projects/fichtelgebirge_project/spruceGenome/depletion2big4github/spruceDepletion_7.22.23/hostDepletion_7.22.23.tar"
path2put="/home/daniel/Documents/backUpSpruceHostDepletion"
scp -i /home/daniel/.ssh/./id_ed25519 $path2get daniel@132.180.112.24:$path2put
## that broke...fix later

## ok, backed up. Now, working on de.NBI, we're going to 
## need to get files between the denbi and lab computers

ssh-keygen -t rsa -f lab2denbi

cd /vol/piceaNanopore/dan

tar xvkf hostDepletion_7.22.23.tar

## we'll do two pipelines. In one, we'll try to "subtract" host reads 
## with an alignment. In the second, we'll just go straight to assembly, 
## hoping that the host will come out as just one more MAG. 

## is there an easy way to use the list of the unblocked reads 
## to decide which reads that were rejected. 

## on the lab nanopore computer, these unblocked (rejected) reads:
ls /var/lib/minknow/data/repeatingDansStuff1/21_7_23/20230721_1214_MN40608_FAS81869_d4801a32/unblocked_read_ids.txt

## let's put this on the denbi comp, from lab comp:
path2get=/var/lib/minknow/data/repeatingDansStuff1/21_7_23/20230721_1214_MN40608_FAS81869_d4801a32/unblocked_read_ids.txt
path2put="/vol/piceaNanopore/dan/chrisHostDepletionTrial/nanoporeRunData/"
scp -P 30419 -i /home/test/.ssh/lab2denbi $path2get ubuntu@129.70.51.6:$path2put

## can we use this somehow to select the reads we need?

## on denbi, maybe use seqkit for this:

cd /vol/piceaNanopore/dan/chrisHostDepletionTrial/nanoporeRunData/fastq_pass

gunzip -k *

## actually, do that with pigz 
pigz -p10 -dk * 

## make one big file:

cat *fastq > allChrisHostDepletionTrial_pass.fastq

wc -l allChrisHostDepletionTrial_pass.fastq ## 6657908, or 1,664,477 reads

conda create -n seqkit -c bioconda seqkit

conda activate seqkit

## try a toy file`

head unblocked_read_ids.txt > unblockIds30.txt

allPassed="/vol/piceaNanopore/dan/chrisHostDepletionTrial/nanoporeRunData/fastq_pass/allChrisHostDepletionTrial_pass.fastq"

head -n 1 $allPassed 

echo "22374732-69a0" > id.txt

seqkit grep $allPassed -f id.txt -o testing.fa

seqkit grep $allPassed -p "22374732" -o testing.fa

seqkit grep $allPassed -irp "22374732" 

seqkit grep $allPassed -irnp "22374732" 

seqkit grep $allPassed -rn -f unblockIds30.txt #-o testing.fa

## extend this to the full unblocked read list. 
## all unblocked reads are rejected 

seqkit grep $allPassed -vrnj 10 -f unblocked_read_ids.txt -o testing.fa

## takes forever. try nohup, up the cores

nohup seqkit grep $allPassed -vrnj 20 -f unblocked_read_ids.txt -o allChrisHostDepletionTrial_noShorts.fastq

grep -c "^@" allChrisHostDepletionTrial_noShorts.fastq

/vol/piceaNanopore/dan/chrisHostDepletionTrial/nanoporeRunData

ssh -p 30419 -i /home/daniel/.ssh/./id_ed25519 ubuntu@129.70.51.6 \
  "grep -c "^@" /vol/piceaNanopore/dan/chrisHostDepletionTrial/nanoporeRunData/allChrisHostDepletionTrial_noShorts.fastq"

grep -c "^@" /vol/piceaNanopore/dan/chrisHostDepletionTrial/nanoporeRunData/allChrisHostDepletionTrial_noShorts.fastq

## there are 1,664,548 reads in the passed reads. 
## there are 617,842 unblocked reads. 
## so there should be ~1 million reads after removing the short reads
## that is taking a long time. let that run overnight,  



path2get=/media/vol1/daniel/spruce/Pabies-haploid_withOrganelles.fa
path2put="/vol/piceaNanopore/dan/chrisHostDepletionTrial/spruceRefGenome/"
scp -P 30419 -i /home/test/.ssh/lab2denbi $path2get ubuntu@129.70.51.6:$path2put

## next day: that is taking forever! one overnight gives us ~100000 reads.
##  we may need to shut that down  just curious, if we simply remove all 
## short reads, does this approach the right numbers? knowing that we 
## should have ~1 million reads after removal of unblocked reads?
  
allPassed="/vol/piceaNanopore/dan/chrisHostDepletionTrial/nanoporeRunData/fastq_pass/allChrisHostDepletionTrial_pass.fastq"
seqtk seq -L 500 $allPassed > allChrisHostDepletionTrial_biggerThan500.fastq

grep -c "^@" allChrisHostDepletionTrial_biggerThan500.fastq ## 1211902
## about 200,000 too many, try a stricter cutoff:

seqtk seq -L 600 $allPassed > allChrisHostDepletionTrial_biggerThan600.fastq
grep -c "^@" allChrisHostDepletionTrial_biggerThan600.fastq ## 1180127, closer

seqtk seq -L 700 $allPassed > allChrisHostDepletionTrial_biggerThan700.fastq
grep -c "^@" allChrisHostDepletionTrial_biggerThan700.fastq ## 1162100, pretty much the same. weird.
## let's keep the 600 bp read length minimum, keep going. 

## two days later, seqkit is still going. 
## can seqtk do it better?

## we need the names of all the reads in the raw nanopore data:

sed -n '1~4p' fastq_pass/allChrisHostDepletionTrial_pass.fastq | \
cut -f 1 -d " " | \
sed "s/^@//" > allChrisHostDepletionTrial_pass_names.txt

wc -l allChrisHostDepletionTrial_pass_names.txt               ## 1664477
wc -l fastq_pass/allChrisHostDepletionTrial_pass.fastq        ## 1664477 (x4)
wc -l unblocked_read_ids.txt                                  ##  617842

## 1664477 - 617842 = 1046635, should be 1,046,635 reads after we remove unblocked reads

## take out the names of the unblocked reads from the list of all read names:

sort <(cat allChrisHostDepletionTrial_pass_names.txt unblocked_read_ids.txt) > comboNames.txt
uniq -u comboNames.txt > allChrisHostDepletionTrial_noShorts_names.txt

wc -l allChrisHostDepletionTrial_noShorts_names.txt ## 1319301, that didn't work.

## I guess some of these unblocked read ids weren't included in the final
## passed reads by minknow. Will this offend seqtk if there are unmatched 
## names in the list?:

seqtk subseq fastq_pass/allChrisHostDepletionTrial_pass.fastq \
 allChrisHostDepletionTrial_noShorts_names.txt > allChrisHostDepletionTrial_noShorts.fastq

wc -l allChrisHostDepletionTrial_noShorts.fastq ## 5005332 / 4 = 1251333
## 1,251,333 reads after we get rid of unblocked reads. Not to different from:
## 1,180,127 reads when we remove reads < 600 bp long, see below

## files sizes:
## <600 bp file size = 11348659574
## reads-manually-removed file size = 11420259495

11348659574 - 11420259495 = -71599921

## ~70 megabytes difference in file size. Is it worth redoing 
## the pipeline below with this file?
## soon but not yet.

## make a more sensible script of this process for Lara:

## step 1 - concatenate all your fastq files that passed. You know how to do this.

## step 2 - get a list of all the names of reads in the fastq.
## I use the program "sed" for this. The following command 
## prints every fourth line, and cleans extra information with "cut"
## and another sed command to get rid of "@" fromt the read names. 
## This should result in a file that is a list of names of 
## all your passed reads from the second run

sed -n '1~4p' yourCombinedPassedReads.fastq | 
cut -f 1 -d " " | 
sed "s/^@//" > yourCombinedPassedReads.txt

## check it! does the text in this file match
## the names of your passed reads?

## step 3 - remove the names of reads that were unblocked (rejected)
## by readfish. We do this by concatenating, sorting and removing 
## duplicates:

## our file for unblocked reads for the hostdepletion (phase two) should be here:
/var/lib/minknow/data/experiment_spruce_threephase/phase2hostDepletion/20230913_2238_MN40608_FAX46654_d4c9fea9/unblocked_read_ids.txt
## make a copy of this (be careful, don't overwrite it)

cat yourCombinedPassedReads.txt unblocked_read_ids.txt > passedAndUnblocked_notSorted.txt
sort passedAndUnblocked_notSorted.txt > comboNames.txt
uniq -u comboNames.txt > passedReads_noUnblocked.txt

## check it! does it look like reads that were unblocked by readfish are no 
## no longer in there? try using grep to check a few reads from "unblocked_read_ids.txt"

grep "nameOfUnblockedReadHere!!" passedReads_noUnblocked.txt 
## if it finds the unblocked read, something is probably wrong...

## once you are sure this worked, use this list to pull out the reads
## that we want from the original fastq:

seqtk subseq yourCombinedPassedReads.fastq passedReads_noUnblocked.txt > passedReads_noUnblocked.fastq


###########

## for now build the pipeline with crude, <600 bp reads removed file:

cd /vol/piceaNanopore/dan/chrisHostDepletionTrial/alignment

genome="/vol/piceaNanopore/dan/chrisHostDepletionTrial/spruceRefGenome/Pabies-haploid_withOrganelles.fa"
reads="/vol/piceaNanopore/dan/chrisHostDepletionTrial/nanoporeRunData/allChrisHostDepletionTrial_biggerThan600.fastq"
#reads="/vol/piceaNanopore/dan/chrisHostDepletionTrial/nanoporeRunData/allChrisHostDepletionTrial_noShorts.fastq"
#minimap2 -t 20 -ax map-ont $genome $reads > bigReadsAlign2Spruce.sam  
## trying to debug with better settings, see below:
minimap2 -R "@RG\\tID:spruceDep\\tSM:spruceDep" -I12g -t 20 -ax map-ont $genome $reads > bigReadsAlign2Spruce.sam  

## what is our range of mapQ scores...  

## I think we need picard/gatk:

conda update -n base -c defaults conda

conda create -n gatk -c bioconda gatk4 picard 

conda create -n samtools -c bioconda "samtools>=1.15"

conda activate gatk

conda install -c bioconda samtools ## for some reason, the version I can 
## get with bioconda is old. use ubuntu rep instead (Apt)

conda remove -n gatk --all

## had to add an old ssl library for this??
## needed an old repo added to apt sources.list:
deb http://security.ubuntu.com/ubuntu xenial-security main
## then install was okay
sudo apt install libssl1.0.0

## https://gatk.broadinstitute.org/hc/en-us/articles/13832754678171-QualityScoreDistribution-Picard-

picard QualityScoreDistribution \
  I=bigReadsAlign2Spruce.sam \
  O=mapQdist_christDep.txt \
  CHART=mapQdist_christDep.pdf


## and of course something is off with our sam file

## following: https://gatk.broadinstitute.org/hc/en-us/articles/360035891231-Errors-in-SAM-or-BAM-files-can-be-diagnosed-with-ValidateSamFile

## using new picard syntax:
picard ValidateSamFile -I bigReadsAlign2Spruce.sam -MODE SUMMARY

## doesn't work yet. try old way
picard ValidateSamFile \
      I=bigReadsAlign2Spruce.sam \
      MODE=SUMMARY

########################################################################
#### HISTOGRAM    java.lang.String
##Error Type      Count
##ERROR:MISSING_READ_GROUP        1
##ERROR:MISSING_SEQUENCE_DICTIONARY       37027303
##WARNING:QUALITY_NOT_STORED      52
##WARNING:RECORD_MISSING_READ_GROUP       37229909
########################################################################

picard ValidateSamFile \
      I=bigReadsAlign2Spruce.sam \
      IGNORE_WARNINGS=true \
      MODE=VERBOSE

## this tells us that basically all alignments are missing a sequence dictionary...

##############################################################################################
# ERROR: Record 71, Read name cebbe9b3-e56e-4479-8c15-5b86cc621719, Empty sequence dictionary.
# ERROR: Record 72, Read name cebbe9b3-e56e-4479-8c15-5b86cc621719, Empty sequence dictionary.
##############################################################################################

## so following: 
## https://gatk.broadinstitute.org/hc/en-us/articles/360035531652-FASTA-Reference-genome-format
## we need both a gatk fasta dictionary file, and a fasta index

## gatk fasta dictionary

genome="/vol/piceaNanopore/dan/chrisHostDepletionTrial/spruceRefGenome/Pabies-haploid_withOrganelles.fa"

gatk CreateSequenceDictionary -R $genome -O Pabies-haploid_withOrganelles.dict

less bigReadsAlign2Spruce.sam

## fasta index 
samtools faidx $genome -o Pabies-haploid_withOrganelles.fai

## both of these put the file right next to the ref genome in:

cd /vol/piceaNanopore/dan/chrisHostDepletionTrial/spruceRefGenome

conda activate gatk

picard ValidateSamFile \
      I=bigReadsAlign2Spruce.sam \
      MODE=SUMMARY

picard ValidateSamFile \
      I=bigReadsAlign2Spruce.sam \
      IGNORE_WARNINGS=true \
      MODE=VERBOSE

picard AddOrReplaceReadGroups

picard AddOrReplaceReadGroups \
    I=$genome \
    O="Pabies-haploid_withOrganelles.bam" \
    RGID=1 \
    RGLB=chrisDepletion \
    RGPU=55579c61d43f72e259ebddaedd7434c488f47e6a \
    RGPL=nanopore \
    RGSM=spruce \
    CREATE_INDEX=True

## it's not finding dictionaries...do we have to be in same directory

cd /vol/piceaNanopore/dan/chrisHostDepletionTrial/nanoporeRunData

/vol/piceaNanopore/dan/chrisHostDepletionTrial/spruceRefGenome

## ugh, screw gatk/picard

## first step, get totally unmapped reads - how many are there?

## samtools needs proper headers. Minimap2 doesn't make them, 
## unless you give it enough memory to swallow the whole 
## reference genome at once, with --I12g
## see: 
## https://github.com/lh3/minimap2/blob/master/FAQ.md#3-the-output-sam-doesnt-have-a-header

samtools view bigReadsAlign2Spruce.sam \
  --threads 20 \
  -q "5" \
  -U "poorHostMatches.sam" \
  -h \
  -O SAM \
  -o "goodHostMatches.sam"

## convert these to poor matches back to fastq:
samtools fastq poorHostMatches.sam > poorHostMatches.fastq
## to get the totally unalign reads:
samtools fastq -f 4 bigReadsAlign2Spruce.sam > hostNonMatches.fastq 

grep -c "^@" poorHostMatches.fastq ## 257,671 reads...
grep -c  "^@" hostNonMatches.fastq ## 55,466 reads...

## let's go with just the poorHostMatches, and the raw reads

## goal for tonight - get metaflye going on at least one of these

## look at the raw reads with fastqc 

conda create -n fastqc -c bioconda fastqc

conda activate fastqc

cd /vol/piceaNanopore/dan/chrisHostDepletionTrial

fastqc -o /vol/piceaNanopore/dan/chrisHostDepletionTrial/fastqcOut/poorHostMatches/ \
       --memory 1024 \
       -t 20 \
    /vol/piceaNanopore/dan/chrisHostDepletionTrial/alignment/poorHostMatches.sam

fastqc -o /vol/piceaNanopore/dan/chrisHostDepletionTrial/fastqcOut/allReads \
       --memory 1024 \
       -t 20 \
  /vol/piceaNanopore/dan/chrisHostDepletionTrial/nanoporeRunData/fastq_pass/allChrisHostDepletionTrial_pass.fastq &> allChrisFastqc.log

## let's get this local and look at it

path2get=/vol/piceaNanopore/dan/chrisHostDepletionTrial/fastqcOut/
scp -rP 30419 ubuntu@129.70.51.6:$path2get .

## there are fragments much smaller than our reads
## The alignment is giving only fragments back out of the 
## sam file. We don't want the actual alignments, 
## we just want every fragment that had some alignment to
## host.  

## back on denbi


## for instance let's find this read in it's various forms:

cd /vol/piceaNanopore/dan/chrisHostDepletionTrial

#seqName="a102de22-3e5a-406a-835c-bd58d58052bd"

seqName="01c5b5f5-d3e9-4c3f-8aa3-308e008f5328"

echo "#####################################################################" > randomReadCheckAlignment.txt && \
echo "########### allChrisHostDepletionTrial_biggerThan600.fastq ##########" >> randomReadCheckAlignment.txt && \
grep -A 1 $seqName nanoporeRunData/allChrisHostDepletionTrial_biggerThan600.fastq >> randomReadCheckAlignment.txt && \
echo "" >> randomReadCheckAlignment.txt && \
echo "#####################################################################" >> randomReadCheckAlignment.txt && \
echo "#################### bigReadsAlign2Spruce.sam #######################" >> randomReadCheckAlignment.txt && \
grep $seqName alignment/bigReadsAlign2Spruce.sam   >> randomReadCheckAlignment.txt
echo "" >> randomReadCheckAlignment.txt && \
echo "#####################################################################" >> randomReadCheckAlignment.txt && \
echo "######################  goodHostMatches.sam  ########################" >> randomReadCheckAlignment.txt && \
grep $seqName alignment/goodHostMatches.sam   >> randomReadCheckAlignment.txt
echo "" >> randomReadCheckAlignment.txt && \
echo "#####################################################################" >> randomReadCheckAlignment.txt && \
echo "#####################   poorHostMatches.sam   #######################" >> randomReadCheckAlignment.txt && \
grep $seqName alignment/poorHostMatches.sam   >> randomReadCheckAlignment.txt

less randomReadCheckAlignment.txt

## ugh. losing track here. We want a way to remove reads that align well to 
## to the spruce genome. Perhaps a better way to do this is to set a really
## high threshold for matches to spruce genome, and collect the names of 
## these sequences

## the first part is easy:

samtools view bigReadsAlign2Spruce.sam \
  --threads 20 \
  -q "30" \
  -U "poorHostMatches.sam" \
  -h \
  -O SAM \
  -o "goodHostMatches.sam"

## the second part is not. How do we collect the names of the sequences
## with good matches? 

## /^@RG gets us to the alignments, past the really long header

samtools view -@ 20 goodHostMatches.sam | head

samtools view -@ 20 goodHostMatches.sam | cut -f1 | sort | uniq > goodHostMatches_readNames.txt

wc -l goodHostMatches_readNames.txt ## 1,026,367, out of maybe 1,180,000. Hmm...that's most. 

## oh well. Try to get rid of them, see if seqtk works faster than our
## grep solution with seqkit

cd /vol/piceaNanopore/dan/chrisHostDepletionTrial/nanoporeRunData

echo "22374732-69a0-477f-b5de-1d7fbfba33fa" > name.lst
echo "01c5b5f5-d3e9-4c3f-8aa3-308e008f5328" >> name.lst
seqtk subseq fastq_pass/allChrisHostDepletionTrial_pass.fastq name.lst > out.fq
## seems to work

## to do this we need to subtract the goodhostmatches from the larger
## pool of unblocked reads. 


cd /vol/piceaNanopore/dan/chrisHostDepletionTrial

allChrisHostDepletionTrial_biggerThan600.fastq

alignment/goodHostMatches_readNames.txt

## the names of our fastq file should be:
sed -n '1~4p' allChrisHostDepletionTrial_biggerThan600.fastq | \
cut -f 1 -d " " | \
sed "s/^@//" > allChrisHostDepletionTrial_biggerThan600_names.txt

allChrisHostDepletionTrial_biggerThan600_names.txt
goodHostMatches_readNames.txt 

sort <(cat goodHostMatches_readNames.txt allChrisHostDepletionTrial_biggerThan600_names.txt) > comboNames.txt
uniq -u comboNames.txt > nonHostReadNames.txt

## use this to subset to just reads of interest:
seqtk subseq allChrisHostDepletionTrial_biggerThan600.fastq nonHostReadNames.txt > nonHostReads.fastq
## whoah that was fast. 

wc -l goodHostMatches_readNames.txt                      ## 1026367
wc -l allChrisHostDepletionTrial_biggerThan600_names.txt ## 1180067
wc -l comboNames.txt                                     ## 2206434 
wc -l <(uniq comboNames.txt)                             ## 1180067
wc -l <(uniq -u comboNames.txt)                          ## 153700
wc -l nonHostReadNames.txt                               ## 153700
wc -l nonHostReads.fastq                                 ## 614800 / 4 = 153700

## so we theoretically end up with ~153,700 reads that are not well aligned to host. 

## run fastqc on them and try an assembly

mkdir /vol/piceaNanopore/dan/chrisHostDepletionTrial/fastqcOut/nonHostReads

fastqc -o /vol/piceaNanopore/dan/chrisHostDepletionTrial/fastqcOut/nonHostReads/ \
       --memory 1024 \
       -t 20 \
    /vol/piceaNanopore/dan/chrisHostDepletionTrial/nanoporeRunData/nonHostReads.fastq &> fastqc.log &

path2get=/vol/piceaNanopore/dan/chrisHostDepletionTrial/fastqcOut/nonHostReads/ 
scp -rP 30419 ubuntu@129.70.51.6:$path2get 

## really need to update my nanopore qc.
## try nanopack tools:

conda install -c bioconda nanoqc

## they look ok. try an assembly:

cd /vol/piceaNanopore/dan/chrisHostDepletionTrial

conda activate flye

reads=/vol/piceaNanopore/dan/chrisHostDepletionTrial/nanoporeRunData/nonHostReads.fastq
outdir=/vol/piceaNanopore/dan/chrisHostDepletionTrial/assemblies/nonHostReadAssembly

nohup flye --meta \
    --nano-raw $reads \
    --threads 25 \
    --out-dir $outdir \
    &> noHostAssembly.log &

## quick, 10 min or so, maybe less
## that looks weird. some of the host definitely slipped through. Do gene predictions on it tomorrow. 

## will flye accept the full fastq, with a lot of host reads?


reads=/vol/piceaNanopore/dan/chrisHostDepletionTrial/nanoporeRunData/allChrisHostDepletionTrial_noShorts.fastq
outdir=/vol/piceaNanopore/dan/chrisHostDepletionTrial/assemblies/hostReduced/

nohup flye --meta \
    --nano-raw $reads \
    --threads 25 \
    --out-dir $outdir \
    &> reducedHostAssembly.log &

## looks like that took ~2 hours.
## results are interesting, can't immediately dismiss them at least,
## a few circular genomes reported, some weird initial blasts, etc.

## so what now? 

## gene prediction, send off to KEGG. And try binning, you never know. 

conda activate prodigal


cd /vol/piceaNanopore/dan/chrisHostDepletionTrial/assemblies/hostReduced/

hostReducedAssembly=/vol/piceaNanopore/dan/chrisHostDepletionTrial/assemblies/hostReduced/assembly.fasta
prodigal \
  -a spruceReducedMicrobiomeMetagenome.genes.faa \
  -d spruceReducedMicrobiomeMetagenome.genes.fna \
  -f gff \
  -o spruceReducedMicrobiomeMetagenome.genes.gff \
  -i $hostReducedAssembly

grep -c "^>" spruceReducedMicrobiomeMetagenome.genes.faa ## 97122 genes

cd /vol/piceaNanopore/dan/chrisHostDepletionTrial/assemblies/nonHostReadAssembly/
nonHostAssembly=/vol/piceaNanopore/dan/chrisHostDepletionTrial/assemblies/nonHostReadAssembly/assembly.fasta

prodigal \
  -a spruceRemovedMicrobiomeMetagenome.genes.faa \
  -d spruceRemovedMicrobiomeMetagenome.genes.fna \
  -f gff \
  -o spruceRemovedMicrobiomeMetagenome.genes.gff \
  -i $nonHostAssembly

grep -c "^>" spruceRemovedMicrobiomeMetagenome.genes.faa ## 4687. Huh. Trying to remove host may be a bad idea.

## get local and send off to kegg for fun:

clear
ls -l /vol/piceaNanopore/dan/chrisHostDepletionTrial/assemblies/hostReduced/spruceReducedMicrobiomeMetagenome.genes.faa
ls -l /vol/piceaNanopore/dan/chrisHostDepletionTrial/assemblies/nonHostReadAssembly/spruceRemovedMicrobiomeMetagenome.genes.faa

#path2get=/vol/piceaNanopore/dan/chrisHostDepletionTrial/assemblies/hostReduced/spruceReducedMicrobiomeMetagenome.genes.faa
path2get=/vol/piceaNanopore/dan/chrisHostDepletionTrial/assemblies/nonHostReadAssembly/spruceRemovedMicrobiomeMetagenome.genes.faa
scp -rP 30419 -i ~/.ssh/id_ed25519 ubuntu@129.70.51.6:$path2get .

ls /home/test/.ssh/lab2denbi

## back to binning. Try using the same 3 softwares as the class. 

## metabat2 install
conda create -n metabat2 -c bioconda metabat2

## concoct install

#conda config --add channels defaults
#conda config --add channels bioconda
#conda config --add channels conda-forge
conda create -n concoct -c bioconda python=3 concoct

## nope. still issues with scitkit learn as before.

## we can try what we did with the class - use a yaml file:
conda env create -f concoct.yml

## this is the yaml file I used:

##############################
name: concoct
channels:
  - conda-forge
  - bioconda
dependencies:
  - concoct=1.1.0
  - libopenblas=*=openmp*
  - mkl
  - python>=3
  - samtools>=1.9
  - scikit-learn=1.1.*
variables:
  USE_OPENMP: 1
##############################

## seems to work ok...

## vamb install
conda create -n vamb -c bioconda vamb minimap2
## seems okay, but really out of date. Let's 
## try with pip:

conda remove -n vamb --all

conda deactivate

## let's try without conda, as recommended by the developers:

pip install vamb ## and fails. jeezus. try in a conda environment

conda create -n vamb 

## ugh, not working. Try github:

conda create -n vamb python=>=3.9.0

conda activate vamb 

cd /vol/piceaNanopore

git clone https://github.com/RasmussenLab/vamb -b master

cd vamb

pip install -e .

## maxbin install (??)
## just curious, did maxbin2 ever fix it's issues?

conda install -c bioconda maxbin2

## failed 

## taking forever. anyway, we got three. 

## das tool install

conda create -n das_tool -c bioconda das_tool

DAS_Tool
## seems okay

## metabat2 binning

conda activate metabat2


## hostRemoved

cd /vol/piceaNanopore/dan/chrisHostDepletionTrial/binning/hostRemoved
readsHostRemoved=/vol/piceaNanopore/dan/chrisHostDepletionTrial/nanoporeRunData/nonHostReads.fastq
assemblyHostRemoved=/vol/piceaNanopore/dan/chrisHostDepletionTrial/assemblies/nonHostReadAssembly/assembly.fasta 

## for our nanopore alignments we use minimap
minimap2 -d assemblyHostRemoved.mmi $assemblyHostRemoved # make index

## make the alignments,
## following: https://github.com/RasmussenLab/vamb/blob/master/README.md

minimap2 -t 25 -ax map-ont assemblyHostRemoved.mmi --split-prefix mmsplit $readsHostRemoved -R @RG\\tID:thisIsAreadGroup | samtools view -F 3584 -b --threads 25 > hostRemovedAligned2Contigs.bam


minimap2 -t 25 -ax map-ont assemblyHostRemoved.mmi --split-prefix mmsplit $readsHostRemoved | samtools view -F 3584 -b --threads 25 > hostRemovedAligned2Contigs.bam

## this fails. formatting issues with on-the-fly-SAM file, it seems. Don't want to spend time 
## debugging, I think such a small metagenome can't be useful for mags, anyway. 

## not sure if the other, larger file will be any better, but gotta try:

## hostReduced
cd /vol/piceaNanopore/dan/chrisHostDepletionTrial/binning/hostReduced
readsHostReduced=/vol/piceaNanopore/dan/chrisHostDepletionTrial/nanoporeRunData/allChrisHostDepletionTrial_noShorts.fastq
assemblyHostReduced=/vol/piceaNanopore/dan/chrisHostDepletionTrial/assemblies/hostReduced/assembly.fasta 

## for our nanopore alignments we use minimap
minimap2 -d assemblyHostReduced.mmi $assemblyHostReduced # make index

## make the alignments, takes 

minimap2 -t 25 -ax map-ont assemblyHostReduced.mmi -I30g --split-prefix mmsplit $readsHostReduced | samtools view -F 3584 -b --threads 25 > hostReducedAligned2Contigs.bam

## not working...why not?
minimap2 -t 25 -ax map-ont assemblyHostReduced.mmi \
  -I30g \
  -o test.sam \
  --split-prefix mmsplit \
  $readsHostReduced 

## first read is 
22374732-69a0-477f-b5de-1d7fbfba33fa

## that seems fine, so I guess it is the second part of the pipe
## that is breaking?

## checking out that flag combination 
## (using superhandy site here: http://broadinstitute.github.io/picard/explain-flags.html)

## read fails platform/vendor quality checks (0x200) 512
## read is PCR or optical duplicate (0x400) 1024
## supplementary alignment (0x800) 2048
512+1024+2048  = 3584

## anyway, that's not the issue. samtools just fails generally with it:

samtools view test.sam

samtools view -H test.sam

## seems like the headers for all of our alignments 
## are duplicated? 

grep scaffold_3628 test.sam

grep scaffold_3628 $assemblyHostReduced

## are these duplicated with our assembly?

grep ">" $assemblyHostReduced | sort > dupsInAss.txt
grep ">" $assemblyHostReduced | sort | uniq -d ## none?
grep ">" $assemblyHostReduced | sort | uniq -c > dupsInAss.txt ## really can't find these...

## why is minimap creating a double header entry?

## will this go away if we switch to fasta for the queries?
seqtk seq -A $readsHostReduced > allChrisHostDepletionTrial_noShorts.fasta

minimap2 -t 25 -ax map-ont assemblyHostReduced.mmi \
  -I30g \
  -o test.sam \
  --split-prefix mmsplit \
  allChrisHostDepletionTrial_noShorts.fasta

samtools view test.sam ## still same problem

grep contig_999 test.sam

grep -n "SN:contig_1002" test.sam ## 1, 4777

grep -n 22374732-69a0-477f-b5de-1d7fbfba33fa test.sam  ## 9552

## starting on line 4777, all headers are repeated. Information starts
## again on 9552. So can we delete lines 4777-9551? 

## ok, last resort, let's delete them and see what happens

samtools view test.sam ## works

samtools view -hF 3584 --threads 25 test.sam > hostReducedAligned2Contigs.sam

## seems to work. weird. Hope this doesn't cause any downstream issues.

less $assemblyHostReduced 

## sort and index the alignment. nanopore takes much more time than the illumina reads
samtools sort -l 1 \
    -n \
    -@25 \
    -o hostReducedAligned2ContigsSorted.bam \
    -O BAM \
    hostReducedAligned2Contigs.sam

## vamb tools requires sorting by name, so can't can't index it:
samtools index -@ 25 ./hostReducedAligned2ContigsSorted.bam ## throws errors

## concoct needs sorting by coordinates, not name ("-n" above)

## redo for concoct:
samtools sort -l 1 \
    -@25 \
    -o hostReducedAligned2ContigsSortedforConcoct.bam \
    -O BAM \
    hostReducedAligned2Contigs.sam
samtools index -@ 25 ./hostReducedAligned2ContigsSortedforConcoct.bam 

## go forward with metabat
conda activate metabat2

mkdir metabat

cd /vol/piceaNanopore/dan/chrisHostDepletionTrial/binning/hostReduced/metabat


## define our variables

assembly="/vol/piceaNanopore/dan/chrisHostDepletionTrial/assemblies/hostReduced/assembly.fasta"
bam="/vol/piceaNanopore/dan/chrisHostDepletionTrial/binning/hostReduced/hostReducedAligned2ContigsSorted.bam"
otherBam="/vol/piceaNanopore/dan/chrisHostDepletionTrial/binning/hostReduced/hostReducedAligned2ContigsSortedforConcoct.bam"

## run the program
runMetaBat.sh $assembly $bam

## that produced nothing. Try with a differently formatted bam: 
runMetaBat.sh $assembly $otherBam

## nope, no bins. 

## try the others...

conda deactivate

conda activate vamb

cd /vol/piceaNanopore/dan/chrisHostDepletionTrial/binning/hostReduced

## let's redo the above alignments...

assemblyHostReduced=/vol/piceaNanopore/dan/chrisHostDepletionTrial/assemblies/hostReduced/assembly.fasta 
reads=/vol/piceaNanopore/dan/chrisHostDepletionTrial/nanoporeRunData/allChrisHostDepletionTrial_noShorts.fastq 
alignment=
readsAligned2Assembly4vamb.bam

cd /vol/piceaNanopore/dan/chrisHostDepletionTrial/binning/hostReduced/vambOut

wget https://raw.githubusercontent.com/RasmussenLab/vamb/37277e23fa1b28718eca4c6a5daa958f42758b13/src/concatenate.py

python3 concatenate.py vambHostReducedHostAssembly.fna.gz $assemblyHostReduced 
minimap2 -d catalogue.mmi vambHostReducedHostAssembly.fna.gz # make index

minimap2 -t 25 -ax map-ont catalogue.mmi \
  -I30g \
  -o intermediateVambRead2AssemblyAlignments.sam \
  --split-prefix mmsplit \
  $reads

samtools view -F 3584 -b --threads 20 intermediateVambRead2AssemblyAlignments.sam > readsAligned2Assembly4vamb.bam

## has the same problems with duplicated headers as before

grep -n "SN:S1Ccontig_1002" intermediateVambRead2AssemblyAlignments.sam ## 1, 4111

## our first read is here:
grep -nB 2 22374732-69a0-477f-b5de-1d7fbfba33fa intermediateVambRead2AssemblyAlignments.sam  ## line 8220 reads start

## looks like our last header should be:
grep -n -A1 -B2 "SN:S1Cscaffold_3628" intermediateVambRead2AssemblyAlignments.sam ## 4109, 8219

## line 4110 has the PG info, keep it

## we should be able to remove these duplicate headers:
sed '4111, 8219d' intermediateVambRead2AssemblyAlignments.sam > intermediateVambRead2AssemblyAlignments_deDup.sam

## try again 
samtools view -F 3584 -b --threads 20 intermediateVambRead2AssemblyAlignments_deDup.sam \
> readsAligned2Assembly4vamb.bam

## vamb is complaining about sorting...

samtools sort -l 1 \
    -@25 \
    -o readsAligned2Assembly4vambSorted.bam \
    -O BAM \
    readsAligned2Assembly4vamb.bam

## seems to work. Weird, don't know why mininmap is creating duplicate headers. 
## anyway, try vamb again:

rm -r hostReducedVambOut/

conda deactivate 

conda activate vamb
cd /vol/piceaNanopore/dan/chrisHostDepletionTrial/binning/hostReduced/vambOut/
assemblyHostReduced=/vol/piceaNanopore/dan/chrisHostDepletionTrial/binning/hostReduced/vambOut/vambHostReducedHostAssembly.fna.gz
alignment=/vol/piceaNanopore/dan/chrisHostDepletionTrial/binning/hostReduced/vambOut/readsAligned2Assembly4vambSorted.bam
vamb --outdir hostReducedVambOut \
  --fasta $assemblyHostReduced \
  --bamfiles $alignment \
  --minfasta 250000 \
  -t 8  &


cd /vol/piceaNanopore/dan/chrisHostDepletionTrial/binning/hostReduced/vambOut/hostReducedVambOut/bins

## concoct

cd /vol/piceaNanopore/dan/chrisHostDepletionTrial/binning/hostReduced/

conda activate concoct 

assembly="/vol/piceaNanopore/dan/chrisHostDepletionTrial/assemblies/hostReduced/assembly.fasta"
readAlignments="/vol/piceaNanopore/dan/chrisHostDepletionTrial/binning/hostReduced/hostReducedAligned2ContigsSortedforConcoct.bam"
outdir="/vol/piceaNanopore/dan/chrisHostDepletionTrial/binning/hostReduced/concoct/"

cut_up_fasta.py $assembly -c 10000 -o 0 --merge_last -b concoctContigs_10K.bed > concoctContigs_10K.fa
concoct_coverage_table.py concoctContigs_10K.bed $readAlignments > coverage_table.tsv

concoct \
  --composition_file concoctContigs_10K.fa \
  --coverage_file coverage_table.tsv \
  -t 25 \
  -b $outdir

cd $outdir
merge_cutup_clustering.py clustering_gt1000.csv > clustering_merged.csv

mkdir concoct_bins

extract_fasta_bins.py $assembly clustering_merged.csv --output_path concoct_bins

## change names:
cd /vol/piceaNanopore/dan/chrisHostDepletionTrial/binning/hostReduced/concoct/concoct_bins

for i in *; do
  mv $i "concoct_$i"
done

## example: concoct_15.fa

## well, concoct found a lot of bins. most of them are probably trash, but...
## vamb also found some. what happened to metabat?

## checked again, and metabat is a no-go

## let's try das_tool on just the two sets of bins that we have.

conda deactivate 

conda activate das_tool

cd /vol/piceaNanopore/dan/chrisHostDepletionTrial/binning/refine

concoctBins=/vol/piceaNanopore/dan/chrisHostDepletionTrial/binning/hostReduced/concoct/concoct_bins
vambBins=/vol/piceaNanopore/dan/chrisHostDepletionTrial/binning/hostReduced/vambOut/hostReducedVambOut/bins
assembly=/vol/piceaNanopore/dan/chrisHostDepletionTrial/assemblies/hostReduced/assembly.fasta

ls $concoctBins
ls $vambBins
ls $assembly


find . -name Fasta_to_Contig2Bin.sh 

## concoct:
Fasta_to_Contig2Bin.sh \
    -e fa \
    -i $concoctBins \
    > concoct.contigs2bin.tsv

head concoct.contigs2bin.tsv ## that looks okay

## vamb
Fasta_to_Contig2Bin.sh \
    -e fna \
    -i $vambBins \
    > vamb.contigs2bin.tsv

head vamb.contigs2bin.tsv ## "S1C" is appended, like before, need to get rid of these

## we need to cut the first three letters out of or so...
cut --complement -c 1-3 vamb.contigs2bin.tsv > vamb.contigs2bin_edited.tsv

less vamb.contigs2bin_edited.tsv ## better

DAS_Tool  -i concoct.contigs2bin.tsv,vamb.contigs2bin_edited.tsv \
    -l concoct,vamb \
    -c $assembly \
    -t 25 \
    --write_bins \
    --score_threshold=0.1 \
    -o hostReducedBinsRefined

## hah, even with that low threshold, one genome 
## survives:

/vol/piceaNanopore/dan/chrisHostDepletionTrial/binning/refine/hostReducedBinsRefined_DASTool_bins/concoct_76.fa

## surprised it was a concoct genome that survived...does order matter?

cd /vol/piceaNanopore/dan/chrisHostDepletionTrial/binning/refine/dasOut
DAS_Tool  -i ../vamb.contigs2bin_edited.tsv,../concoct.contigs2bin.tsv \
    -l vamb,concoct \
    -c $assembly \
    -t 25 \
    --write_bins \
    --score_threshold=0.1 \
    -o hostReducedBinsRefined

## I'm assuming this either a organelle or the remnants of the host...

less /vol/piceaNanopore/dan/chrisHostDepletionTrial/binning/refine/dasOut/hostReducedBinsRefined_DASTool_bins/concoct_76.fa
## that is a chloroplast genome...of course

## briefly checking the other VAMB bins...

## vae_17.fna has matches to a walking stick (Timema) genome? wtf
## plus a rrna 5s subunit match to a cedar tree? double wtf. 
## anyway, some weird stuff

## hope dies last maybe best to try phyllophlan and checkM on these things.

## checkM

#conda config --set channel_priority flexible
conda create -n checkm -c bioconda checkm-genome
#conda config --set channel_priority strict

conda deactivate
conda activate checkm

## variables

## we can put our checkm outputs here

checkMout="/vol/piceaNanopore/dan/chrisHostDepletionTrial/qualityCheck/"

cd $checkMout

concoctBins=/vol/piceaNanopore/dan/chrisHostDepletionTrial/binning/hostReduced/concoct/concoct_bins

vambBins=/vol/piceaNanopore/dan/chrisHostDepletionTrial/binning/hostReduced/vambOut/hostReducedVambOut/bins

nohup checkm lineage_wf -t 15 $vambBins qcVamb &> vambCheckm.log &

nohup checkm lineage_wf -t 15 $concoctBins -x fa  qcConcoctBins  &> concoctCheckm.log &

ls $concoctBins 

## looks pretty dismal. MAGs are not possible here. 

## vae_3175 may be mitochondrial

## phylophlan

conda config --set channel_priority flexible
conda create --name phylophlan -c bioconda phylophlan
conda config --set channel_priority strict


## find Chris's run which did not use adaptive sequencing 
## (the one used for simulations). 
## try metagenome assembly and prediction of bacterial genes?

## in both, include an alignment of all genes to mitochondrial 
## and chloroplast genomes of spruce.

## see if there is a significant in prokaryotic genes recovered.  

## I guess the best yardstick here is the number of prokaryotic
## and fungal genes, 

## the easieast is the bacterial genes. 

## where is chris's data, that we used for simulations of the spruce?


## i think it is this?:


ls -lh /media/vol2/chris/Pabies_tests/bulk_fast5_files/MinION-PC_20230621_1522_FAU29445_MN40608_sequencing_run_bigger_e77dd4cd_5395a225.fast5

cd /media/vol2/chris/Pabies_tests/bulk_fast5_files/

## but where is the fastq for this?
cd /var/lib/minknow/data/Pabies_bulk_fast5/bigger/20230621_1517_MN40608_FAU29445_e77dd4cd

## make a combined pass fastq for this, put it on denbi:

cd /var/lib/minknow/data/Pabies_bulk_fast5/bigger/20230621_1517_MN40608_FAU29445_e77dd4cd/fastq_pass/


## put this on denbi:
path2get=/var/lib/minknow/data/Pabies_bulk_fast5/bigger/20230621_1517_MN40608_FAU29445_e77dd4cd/fastq_pass/allpabies_noAdaptSeq.fastq
path2put="/vol/piceaNanopore/dan/chrisHostDepletionTrial/compareAdapt/"
scp -rP 30419 -i ~/.ssh/lab2denbi $path2get ubuntu@129.70.51.6:$path2put 

## looks like it only ran for 4 hours:
less /var/lib/minknow/data/Pabies_bulk_fast5/bigger/20230621_1517_MN40608_FAU29445_e77dd4cd/final_summary_FAU29445_e77dd4cd_5395a225.txt

## huh, there are only 24 584 reads in our pabies simulation fastq file...seems small.

## not sure if this will be comparable. where is the equivalent fastq_pass for our actual spruce depletion experiment

## here, on the lab comp

/var/lib/minknow/data/repeatingDansStuff1/21_7_23/20230721_1214_MN40608_FAS81869_d4801a32/fastq_pass

## our sequence experiment for exclusion, where adaptive seqencincg "worked":
ls /var/lib/minknow/data/repeatingDansStuff1/21_7_23/20230721_1214_MN40608_FAS81869_d4801a32


## to convince myself that this worked, I want to compare the number of prokaryotic genes that come out 
## an assembly from the non-adptive-seq sequencing run, and the adaptive seq run:


cd /vol/piceaNanopore/dan/chrisHostDepletionTrial/assemblies/hostReduced/

## remind me, how many prok genes does prodigal think we have?:
grep -c "^>" spruceReducedMicrobiomeMetagenome.genes.faa ## 97122, lots but how many are not from mitochondria and chloroplasts?


## while we're here, grab these faa and send to ghost koala:
path2get=/vol/piceaNanopore/dan/chrisHostDepletionTrial/assemblies/hostReduced/spruceReducedMicrobiomeMetagenome.genes.faa
scp -rP 30419  ubuntu@129.70.51.6:$path2get . 

## let's try an assembly with the  

cd /vol/piceaNanopore/dan/chrisHostDepletionTrial/compareAdapt/

conda activate flye

reads=/vol/piceaNanopore/dan/chrisHostDepletionTrial/compareAdapt/allpabies_noAdaptSeq.fastq
outdir=/vol/piceaNanopore/dan/chrisHostDepletionTrial/compareAdapt/allpabies_noAdaptSeq_assembly/
#mkdir -p $outdir

nohup flye --meta \
    --nano-raw $reads \
    --threads 25 \
    --out-dir $outdir \
    &> allpabies_noAdaptSeq_assembly.log &

## and of course it fails.

## remind myself why I don't filter reads through the picea genome again? 
## one thing (maybe the first?) to report is simply the number of reads 
## that don't map to host+organelles well

spruceGenome=/vol/piceaNanopore/dan/chrisHostDepletionTrial/spruceRefGenome/Pabies-haploid_withOrganelles.fa
noAdaptReads=/vol/piceaNanopore/dan/chrisHostDepletionTrial/compareAdapt/allpabies_noAdaptSeq.fastq
cd /vol/piceaNanopore/dan/chrisHostDepletionTrial/compareAdapt/
nohup minimap2  -I100g -t 20 -ax map-ont $spruceGenome $noAdaptReads 1> noAdaptSeq_Align2Spruce.sam 2> noAdaptSeq_Align2Spruce.log &

## the same process, using our adaptive seq reads:
spruceGenome=/vol/piceaNanopore/dan/chrisHostDepletionTrial/spruceRefGenome/Pabies-haploid_withOrganelles.fa
yesAdaptReads=/vol/piceaNanopore/dan/chrisHostDepletionTrial/nanoporeRunData/allChrisHostDepletionTrial_noShorts.fastq
cd /vol/piceaNanopore/dan/chrisHostDepletionTrial/compareAdapt/
nohup minimap2  -I100g -t 25 -ax map-ont $spruceGenome $yesAdaptReads 1> adaptSeq_Align2Spruce.sam 2> adaptSeq_Align2Spruce.log &

## these will result in both unmapped reads and poorly mapped reads.
## I guess we take only the best reads and try assembling these:

samtools view -q 50 --threads 20 noAdaptSeq_Align2Spruce.sam > noAdapt_hostReads.sam

cut -f1 noAdapt_hostReads.sam | wc -l ## 55817
cut -f1 noAdapt_hostReads.sam | uniq | wc -l ## 19391 
cut -f1 noAdapt_hostReads.sam | uniq > noAdapt_hostReadNames.txt 

wc -l noAdapt_hostReadNames.txt ## 19391

wc -l allpabies_noAdaptSeq.fastq ## 98336 / 4 = 24584 reads

## 24584 - 19391 = 5193 reads that don't align well to host. 
5193 / 24584 ## .211, or 21% 
## get a fasta for these:

grep "^@" allpabies_noAdaptSeq.fastq | cut --complement -c 1 | cut -f1 -d" " > allpabies_noAdaptSeq_names.txt

sort <(cat allpabies_noAdaptSeq_names.txt noAdapt_hostReadNames.txt) | uniq -u > noAdapt_nonHost_readNames.txt 

wc -l noAdapt_nonHost_readNames.txt ## 5193, as expected. 

seqtk subseq allpabies_noAdaptSeq.fastq noAdapt_nonHost_readNames.txt > pAbies_noAdapt_nonHost.fastq 

wc -l pAbies_noAdapt_nonHost.fastq  ## makes sense

## does it make sense to try to assemble these?
conda activate flye

reads=pAbies_noAdapt_nonHost.fastq
outdir=pAbies_noAdapt_nonHost/
#mkdir -p $outdir
nohup flye --meta \
    --nano-raw $reads \
    --threads 25 \
    --out-dir $outdir \
    &> allpabies_noAdaptSeq_assembly.log &
## fails

## repeat with host reduced dataset
samtools view -q 50 --threads 20 adaptSeq_Align2Spruce.sam > adapt_hostReads.sam

cut -f1 adapt_hostReads.sam | uniq > adapt_hostReadNames.txt 

wc -l adapt_hostReadNames.txt


yesAdaptReads=/vol/piceaNanopore/dan/chrisHostDepletionTrial/nanoporeRunData/allChrisHostDepletionTrial_noShorts.fastq

head $yesAdaptReads 

grep -c "^@.* runid" $yesAdaptReads  ## 1251333

wc -l $yesAdaptReads  ## 5005332 / 4 = 1251333

grep "^@.* runid" $yesAdaptReads | cut --complement -c 1 | cut -f1 -d" " > allpabies_adaptSeq_names.txt

wc -l allpabies_adaptSeq_names.txt ## 1251333 looks good

sort <(cat allpabies_adaptSeq_names.txt adapt_hostReadNames.txt) | uniq -u > adapt_nonHost_readNames.txt 

wc -l adapt_nonHost_readNames.txt ## 247594 reads

seqtk subseq $yesAdaptReads adapt_nonHost_readNames.txt > pAbies_adapt_nonHost.fastq 

grep -c "^@.* runid" pAbies_adapt_nonHost.fastq  ## 247594
grep -c "^@.* runid" $yesAdaptReads  ## 1251333

1251333 - 247594 ## 1003739

247594/1251333 ## 19% 


## does it make sense to try to assemble these?
conda activate flye

reads=pAbies_adapt_nonHost.fastq
outdir=pAbies_adapt_nonHost/
#mkdir -p $outdir

nohup flye --meta \
    --nano-raw $reads \
    --threads 25 \
    --out-dir $outdir \
    &> allpabies_adaptSeq_assembly.log &

## does it make any sense to run gene predictions on the raw reads?

## can't hurt, I guess:

mkdir -p genePreds/noAdapt_nonHost_genes


cd genePreds/noAdapt_nonHost_genes

## make a fasta
seqtk seq -A /vol/piceaNanopore/dan/chrisHostDepletionTrial/compareAdapt/pAbies_noAdapt_nonHost.fastq > pAbies_noAdapt_nonHost.fasta

conda activate prodigal

reads=/vol/piceaNanopore/dan/chrisHostDepletionTrial/compareAdapt/genePreds/noAdapt_nonHost_genes/pAbies_noAdapt_nonHost.fasta


prodigal \
  -a pAbies_noAdapt_nonHost.genes.faa \
  -d pAbies_noAdapt_nonHost.genes.fna \
  -f gff \
  -o pAbies_noAdapt_nonHost.genes.gff \
  -i $reads

grep -c ">" pAbies_noAdapt_nonHost.genes.fna ## 11356 genes, from a 13M file

## file size is actually 12985120 bytes
11356/13 = 873.5 #gene/M
11356/12985120 = 0.0008745394728735661 ## gene/byte
12985120/11356 = 1143 ## bytes/gene

## repeat with adaptive seq

cd /vol/piceaNanopore/dan/chrisHostDepletionTrial/compareAdapt/

reads=/vol/piceaNanopore/dan/chrisHostDepletionTrial/compareAdapt/pAbies_adapt_nonHost.fastq
seqtk seq -A $reads > pAbies_noAdapt_nonHost.fasta


cd genePreds/adapt_nonHost_genes

seqtk seq -A /vol/piceaNanopore/dan/chrisHostDepletionTrial/compareAdapt/pAbies_adapt_nonHost.fastq > pAbies_adapt_nonHost.fasta

reads=/vol/piceaNanopore/dan/chrisHostDepletionTrial/compareAdapt/genePreds/adapt_nonHost_genes/pAbies_adapt_nonHost.fasta

prodigal \
  -a pAbies_adapt_nonHost.genes.faa \
  -d pAbies_adapt_nonHost.genes.fna \
  -f gff \
  -o pAbies_adapt_nonHost.genes.gff \
  -i $reads

grep -c ">" pAbies_adapt_nonHost.genes.fna ## 610658 prokaryotic genes predicted from these adaptive seq non-host reads

## 672778641 bytes or ~ 642M

610658/642 = 951.18

610658/672778641 = 0.000907665557117471

672778641/610658 = 1101.7 ## bytes/gene


cd /vol/piceaNanopore/dan/chrisHostDepletionTrial/nanoporeRunData/


grep -c "^@.* runid=" fastq_pass/allChrisHostDepletionTrial_pass.fastq ## 1,664,477 reads, 12gig

## I don't really understand. The unblocked read file has 617842 sequences,
## the unblocked read file has 617,842 reads, but not all passed read quality..

grep -c "^@.* runid=" allChrisHostDepletionTrial_noShorts.fastq ## 1,251,333 reads, 11gig

## somehow, rejecting 600000 reads didn't improve our ratio of non-host reads?

## probably several things at play here, not the least that we are comparing a 
## tiny run to a big run. 


ls -lh /vol/piceaNanopore/dan/chrisHostDepletionTrial/assemblies/hostReduced/assembly.fasta

ls -lh /vol/piceaNanopore/dan/chrisHostDepletionTrial/compareAdapt/pAbies_adapt_nonHost/assembly.fasta

grep -c "^>" /vol/piceaNanopore/dan/chrisHostDepletionTrial/compareAdapt/pAbies_adapt_nonHost/assembly.fasta
## metagenome with 618 reads? 5.2M?  ugh

## may


cd /vol/piceaNanopore/dan/bactGenomes/genbank

wget ftp://ftp.ncbi.nih.gov/genomes/genbank/bacteria/assembly_summary.txt

wc -l /vol/piceaNanopore/dan/bactGenomes/genbank/assembly_summary.txt ## 1696420

wget ftp://ftp.ncbi.nlm.nih.gov/genomes/README_assembly_summary.txt

cd /vol/piceaNanopore/dan/bactGenomes/refSeq
wget ftp://ftp.ncbi.nih.gov/genomes/refseq/bacteria/assembly_summary.txt

wc -l /vol/piceaNanopore/dan/bactGenomes/refSeq/assembly_summary.txt ## 310753

## seems like the genbank one is much bigger, though probably of much lower quality...
## let's start with the refseq one.

cd /vol/piceaNanopore/dan/bactGenomes/refSeq

## let's assume they're all good:

sed '1,2d' assembly_summary.txt | cut  -f 20  > ncbiRefGenomes.txt

## but this doesn't give us the actual file, gives us the ftp folder, and wget 
## isn't being allowed to recursively download


head ncbiRefGenomes.txt

cut -d"/" -f 10  ncbiRefGenomes.txt > genNames.txt
## add the file ending
sed -i 's/.*/&_genomic.fna.gz/' genNames.txt
paste ncbiRefGenomes.txt genNames.txt -d"/" > files2download.txt

## these seem to work when I try them manually. Can we do a big download

nohup wget  -i files2download.txt &> downloadRefSeq.log &

wc -l files2download.txt

## revisiting this, it looks like we got a lot of pseudomonas aerigenosus, helicobacter, and campylobacter

grep ">" bactRefGenome.fa | cut -f2 -d" " 

grep ">" bactRefGenome.fa | cut -f2 -d" " | sort | uniq | wc -l

## can we balance this out a bit?

cd /vol/piceaNanopore/dan/bactGenomes

cut -f8 assembly_summary.txt | sort | uniq > bactGenomeNames_uniq.txt

cut -f8 assembly_summary.txt | cut -f1,2 -d" " | sort | uniq | wc -l ## ~20,000 species

cut -f8 assembly_summary.txt | cut -f1,2 -d" " | sort | uniq > allBactSpeciesList.txt 
sed -i '1d' allBactSpeciesList.txt 

sed '1d' assembly_summary.txt > assembly_summary.tsv  

## how can we use this to get ~fair representation? this is probably a job for python...

python3
import pandas as pd
import os
speciesList=pd.read_csv('allBactSpeciesList.txt')
assembly_summary=pd.read_csv('assembly_summary.tsv', sep='\t')

assembly_summary.columns

assembly_summary.head()

       
assembly_summary[['taxid','isolate', 'species_taxid', 'organism_name']].iloc[1:40,:]


## looks like the best bet is to use the species_taxid column:
      
assembly_summary['organism_name'].drop_duplicates().shape

## so this should give us a list of the first instance of 
## each unique organism name:

firstInstances = ~assembly_summary['organism_name'].duplicated(keep='first')


assembly_summary[ firstInstances ]['organism_name']

## for the moment, let's say we want one genome from each 
## so the new download list should be:

assembly_summary[ firstInstances ]['ftp_path'].to_csv('allSppDownloadlist.csv', index=False, header=False )

## out of python, let's reuse our code from above to edit the ftp paths:

cd /vol/piceaNanopore/dan/bactGenomes

cut -d"/" -f 10 allSppDownloadlist.csv > genNames.txt
sed -i 's/.*/&_genomic.fna.gz/' genNames.txt
paste allSppDownloadlist.csv genNames.txt -d"/" > files2download.txt


## try a new download, which will surely run all weekend:
cd /vol/piceaNanopore/dan/bactGenomes/
getFiles=/vol/piceaNanopore/dan/bactGenomes/files2download.txt
putFiles=/vol/piceaNanopore/dan/bactGenomes/refSeq/
nohup wget -i $getFiles -P $putFiles &> downloadRefSeq.log &


## repeat all the above with fungi:

cd /vol/piceaNanopore/dan/fungalGenomes

wget ftp://ftp.ncbi.nlm.nih.gov/genomes/README_assembly_summary.txt

wget ftp://ftp.ncbi.nih.gov/genomes/genbank/fungi/assembly_summary.txt

cut -f8 assembly_summary.txt | cut -f1,2 -d" " | sort | uniq | wc -l ## ~4300 fungal species

sed '1d' assembly_summary.txt > assembly_summary.tsv  

sed '1d' assembly_summary.txt > assembly_summary.tsv  

## back into python

python3
import pandas as pd
import os

#assembly_summary=pd.read_csv('assembly_summary.tsv', sep='\t')
## error parsing line 10465. Back out to shell
sed -n '10465p' assembly_summary.tsv ## ah, this record has errors
## we don't really need any information after the ftp address, anyway:
cut -f1-20 assembly_summary.tsv > assembly_summary_cleaned.tsv 

## back in python:
assembly_summary=pd.read_csv('assembly_summary_cleaned.tsv', sep='\t')

assembly_summary['organism_name'].drop_duplicates().shape ## almost 600 genomes...

firstInstances = ~assembly_summary['organism_name'].duplicated(keep='first')
assembly_summary[ firstInstances ]['ftp_path'].to_csv('allSppDownloadlist.csv', index=False, header=False )

## out of python
cd /vol/piceaNanopore/dan/fungalGenomes
cut -d"/" -f 10 allSppDownloadlist.csv > genNames.txt
sed -i 's/.*/&_genomic.fna.gz/' genNames.txt
paste allSppDownloadlist.csv genNames.txt -d"/" > files2download.txt

## try a new download, which will surely run all weekend:
cd /vol/piceaNanopore/dan/fungalGenomes/

getFiles=/vol/piceaNanopore/dan/fungalGenomes/files2download.txt
putFiles=/vol/piceaNanopore/dan/fungalGenomes/refSeq/
nohup wget -i $getFiles -P $putFiles &> downloadRefSeq.log &

## now, we need to make a blast database 


### let that run for a while...

## let's work in the large ephemeral drive that denbi gave us:

## let's a database of our fungal genomes and bacterial genomes

#mv /vol/piceaNanopore/dan/bactGenomes /mnt/ephem/
#mv /vol/piceaNanopore/dan/fungalGenomes /mnt/ephem/

#mv /mnt/ephem/fungalGenomes /vol/piceaNanopore/dan/ 
#mv /vol/piceaNanopore/dan/fungalGenomes/refSeq /mnt/ephem/fungalGenomes

du /vol/piceaNanopore/dan/fungalGenomes/refSeq  -h


cd /mnt/ephem/bactGenomes/refSeq

gunzip -r .

cd /mnt/ephem/fungalGenomes/refSeq

gunzip -r . &


conda create -n blast -c bioconda blast

conda activate blast

cd /mnt/ephem/bactGenomes/refSeq 

nohup cat * > bactRefGenome.fa & ## too many files

## try this:

ls /mnt/ephem/bactGenomes/refSeq 

## make bact combined file
cd /vol/piceaNanopore/dan/bactGenomes

nohup find /mnt/ephem/bactGenomes/refSeq -type f -exec cat {} + > allbactGenomes.fa &
grep -c "^>" allbactGenomes.fa > howmanybact.txt &

cat /vol/piceaNanopore/dan/bactGenomes/howmanybact.txt

## make fungal combined file
cd /vol/piceaNanopore/dan/fungalGenomes
nohup find /mnt/ephem/fungalGenomes/refSeq -type f -exec cat {} + > allFungalGenomes.fa &

## should be 5977 genomes in this file:
grep -c "^>" allFungalGenomes.fa > howmanyfungi.txt &

cat /vol/piceaNanopore/dan/fungalGenomes/howmanyfungi.txt

## make databases

## bacterial blastdb
conda activate blast

cd /vol/piceaNanopore/dan/bactGenomes

nohup makeblastdb -in allbactGenomes.fa -parse_seqids -dbtype nucl &

## fungal blastdb

cd /vol/piceaNanopore/dan/fungalGenomes
nohup makeblastdb -in allFungalGenomes.fa -parse_seqids -dbtype nucl &

## raw reads blasted against bacteria:
## we are interested in blasting the raw reads, and the assemblies we have.

cd /vol/piceaNanopore/dan/bactGenomes

rawReadsAdapt=/vol/piceaNanopore/dan/chrisHostDepletionTrial/nanoporeRunData/allChrisHostDepletionTrial_noShorts.fasta

nohup blastn -db allbactGenomes.fa \
  -query $rawReadsAdapt \
  -num_threads 10 \
  -num_alignments 1 \
  -out depletionTrial_bacterial_blastn.csv \
  -outfmt 10 &


sed -i '1i qseqid,sseqid,pident,length,mismatch,gapopen,qstart,qend,sstart,send,evalue,bitscore' $genome"_"$query"_Blastn.csv"

## not sure how to get multiple output formats without running multiple alignments? 
## seems really inefficient but rerun with 

nohup blastn -db allbactGenomes.fa \
  -query $rawReadsAdapt \
  -num_threads 10 \
  -num_alignments 3 \
  -out depletionTrial_bacterial_blastn.txt &


head /vol/piceaNanopore/dan/bactGenomes/depletionTrial_bacterial_blastn.csv

ls -lh /vol/piceaNanopore/dan/bactGenomes/depletionTrial_bacterial_blastn.csv

less /vol/piceaNanopore/dan/bactGenomes/depletionTrial_bacterial_blastn.csv

wc -l /vol/piceaNanopore/dan/bactGenomes/depletionTrial_bacterial_blastn.csv

cd /vol/piceaNanopore/dan/bactGenomes

less allbactGenomes.fa

bactTxt=/vol/piceaNanopore/dan/bactGenomes/depletionTrial_bacterial_blastn.txt

ls -lh  $bactTxt

grep -c "Query=" $bactTxt

grep -c "No hits found" $bactTxt

grep -c "Sequences producing significant alignments:" $bactTxt && grep -c "Query=" $bactTxt 

grep -c "Sequences producing significant alignments:" $bactTxt

less $bactTxt

bactCSV=/vol/piceaNanopore/dan/bactGenomes/depletionTrial_bacterial_blastn.csv

wc -l $bactCSV

ls -lh  $bactCSV

rawReadsAdapt=/vol/piceaNanopore/dan/chrisHostDepletionTrial/nanoporeRunData/allChrisHostDepletionTrial_noShorts.fasta

grep -c "^>" $rawReadsAdapt & #1251333 total reads in the adapt set

477951/1249155 ## .382


## seems to hover around 38-39% 

## looks like it is working...

## fungal genomes

## blast them too!

## we only have enough cores to start one:

cd /vol/piceaNanopore/dan/fungalGenomes

rawReadsAdapt=/vol/piceaNanopore/dan/chrisHostDepletionTrial/nanoporeRunData/allChrisHostDepletionTrial_noShorts.fasta

nohup blastn -db allFungalGenomes.fa \
  -query $rawReadsAdapt \
  -num_threads 5 \
  -num_alignments 1 \
  -out depletionTrial_fungal_blastn.csv \
  -outfmt 10 &

sed -i '1i qseqid,sseqid,pident,length,mismatch,gapopen,qstart,qend,sstart,send,evalue,bitscore' $genome"_"$query"_Blastn.csv"

## not run
#nohup blastn -db fungRefGenome.fa \
#  -query $rawReadsAdaptFa \
#  -num_threads 10 \
#  -num_alignments 3 \
#  -out depletionTrial_fungal_blastn.txt &

## check these:

fungalCSV=/vol/piceaNanopore/dan/fungalGenomes/depletionTrial_fungal_blastn.csv
wc -l $fungalCSV


## there are some repeats. For instance

rawReadsAdapt=/vol/piceaNanopore/dan/chrisHostDepletionTrial/nanoporeRunData/allChrisHostDepletionTrial_noShorts.fasta

grep -A 1 "5d61c9f3-3592-4123-80aa-b87bad800aac" $rawReadsAdapt

grep "5d61c9f3-3592-4123-80aa-b87bad800aac" /vol/piceaNanopore/dan/bactGenomes/depletionTrial_bacterial_blastn.csv

grep -A 40 "5d61c9f3-3592-4123-80aa-b87bad800aac" /vol/piceaNanopore/dan/bactGenomes/depletionTrial_bacterial_blastn.txt

grep "5d61c9f3-3592-4123-80aa-b87bad800aac" /vol/piceaNanopore/dan/fungalGenomes/depletionTrial_fungal_blastn.csv

## so I think we need to combine the reads that blasted to either fungal or bacterial genomes 
## and host genomes. So we need a third blast search to host, which should go much more easily.
## comparisons among the three should follow, and every read that blasts most confidently to 
## a microbe should be retained. For the moment, let's not do much curation on this, just get 
## an estimate of reads that are likely microbial in origin

##### adapt/Spruce check ####

## also need to get a catalogue of which reads aligned well to the host
## where is our spruce genome?...

## make database
cd /vol/piceaNanopore/dan/chrisHostDepletionTrial/spruceRefGenome/
nohup makeblastdb -in Pabies-haploid_withOrganelles.fa -parse_seqids -dbtype nucl &

## use it:

## put data in compare directory:

 
cd /vol/piceaNanopore/dan/chrisHostDepletionTrial/compareAdapt/hostBlastMatches

fullSpruceGenome=/vol/piceaNanopore/dan/chrisHostDepletionTrial/spruceRefGenome/Pabies-haploid_withOrganelles.fa
rawReadsAdapt=/vol/piceaNanopore/dan/chrisHostDepletionTrial/nanoporeRunData/allChrisHostDepletionTrial_noShorts.fasta

## csv of adapt/spruce search
nohup blastn -db $fullSpruceGenome \
  -query $rawReadsAdapt \
  -num_threads 10 \
  -num_alignments 1 \
  -out depletionTrial_Spruce_blastn.csv \
  -outfmt 10 &

## not run:  
## txt file of match details
nohup blastn -db $fullSpruceGenome \
  -query $rawReadsAdapt \
  -num_threads 10 \
  -num_alignments 2 \
  -out depletionTrial_bacterial_blastn.txt &


## let's compare to our small control run, which Chris intended for simulations

## this should be simple, just repeat with the blastn process with 
## our small run that chris created for simulations.

## where is this? 

## on nanocomp
cd /var/lib/minknow/data/Pabies_bulk_fast5/bigger/20230621_1517_MN40608_FAU29445_e77dd4cd/fastq_pass/

## on denbiComp

cd /vol/piceaNanopore/dan/chrisHostDepletionTrial/compareAdapt
seqtk seq -A allpabies_noAdaptSeq.fastq > allpabies_noAdaptSeq.fasta

noAdaptSeq="/vol/piceaNanopore/dan/chrisHostDepletionTrial/compareAdapt/allpabies_noAdaptSeq.fasta"

ls -lh $noAdaptSeq

grep -c "^>" $noAdaptSeq ## 24584 reads

## should be short:

## non-adaptive bacterial blast:

conda activate blast

cd /vol/piceaNanopore/dan/bactGenomes

noAdaptSeq="/vol/piceaNanopore/dan/chrisHostDepletionTrial/nanoporeRunData/allpabies_noAdaptSeq.fasta"

## csv:
nohup blastn -db allbactGenomes.fa \
  -query $noAdaptSeq \
  -num_threads 10 \
  -num_alignments 1 \
  -out noAdaptiveSamp_bacterial_blastn.csv \
  -outfmt 10 &

sed -i '1i qseqid,sseqid,pident,length,mismatch,gapopen,qstart,qend,sstart,send,evalue,bitscore' $genome"_"$query"_Blastn.csv"

## long text:
nohup blastn -db allbactGenomes.fa \
  -query $noAdaptSeq \
  -num_threads 10 \
  -num_alignments 3 \
  -out noAdaptiveSamp_bacterial_blastn.txt &

grep -c "Sequences producing significant alignments:" noAdaptiveSamp_bacterial_blastn.txt && grep -c "Query=" noAdaptiveSamp_bacterial_blastn.txt 

7863/24584 ## .32 possible bacterial matches

## not run

## non-adaptive fungal blast, csv:

cd /vol/piceaNanopore/dan/fungalGenomes
noAdaptSeq="/vol/piceaNanopore/dan/chrisHostDepletionTrial/nanoporeRunData/allpabies_noAdaptSeq.fasta"
nohup blastn -db allFungalGenomes.fa \
  -query $noAdaptSeq \
  -num_threads 10 \
  -num_alignments 1 \
  -out noAdaptiveSamp_fungal_blastn.csv \
  -outfmt 10 &


## non-adaptive/host-genome blast, csv:
cd /vol/piceaNanopore/dan/chrisHostDepletionTrial/compareAdapt/hostBlastMatches

noAdaptSeq="/vol/piceaNanopore/dan/chrisHostDepletionTrial/nanoporeRunData/allpabies_noAdaptSeq.fasta"
fullSpruceGenome="/vol/piceaNanopore/dan/chrisHostDepletionTrial/spruceRefGenome/Pabies-haploid_withOrganelles.fa"


nohup blastn -db $fullSpruceGenome \
  -query $noAdaptSeq \
  -num_threads 10 \
  -num_alignments 1 \
  -out noAdaptiveSamp_Spruce_blastn.csv \
  -outfmt 10 &


adapt2spruce=/vol/piceaNanopore/dan/chrisHostDepletionTrial/compareAdapt/hostBlastMatches/depletionTrial_Spruce_blastn.csv

noAdapt2spruce=/vol/piceaNanopore/dan/chrisHostDepletionTrial/compareAdapt/hostBlastMatches/noAdaptiveSamp_Spruce_blastn.csv


wc -l $noAdapt2spruce ## 29972 or  29,972, out of? 

grep -c "^>" $noAdaptSeq ## 24584. Why does this keep returning more matches than queries? We only requested 1 alignment?

29972/24584

cut $noAdapt2spruce -d"," -f1 | sort | uniq | wc -l ## 23581

23581/24584 ## 96% of noAdapt reads find some sort of match to spruce

less $adapt2spruce

wc -l $adapt2spruce ## 1615413, this is more than raw reads?

cut $adapt2spruce -d"," -f1 | sort | uniq | wc -l ## 1188134
## 1,188,134. 

1188134/1251333 ## 0.949 of adapt reads have some sort of match to host. Pretty much the same. WTF


## fungi:
noAdapt2fungi=/vol/piceaNanopore/dan/fungalGenomes/noAdaptiveSamp_fungal_blastn.csv
adapt2fungi=/vol/piceaNanopore/dan/fungalGenomes/depletionTrial_fungal_blastn.csv

wc -l $noAdapt2fungi  ## 8492

cut $noAdapt2fungi -d"," -f1 | sort | uniq | wc -l ## 7501

7501/24584 ## 30% of noAdapt reads have a fungal match

wc -l $adapt2fungi ## 505789 or 505,789 reads

cut $adapt2fungi -d"," -f1 | sort | uniq | wc -l ## 386103

386103/1251333 ## 30% reads, same as noAdapt.

## ugh, really starting to look like using readfish didn't help us at all.
## but we need to examing the quality of the matches. 

## general strategy
## we need go through each read, find its best match to host/bacteria/fungi, 
## then tally the totals

## don't think this will make much of a difference, but let's see. 

## organize a bit, we have six dataframes of interest. 

## fungal, bacterial, host x adapt/noAdapt

cd /vol/piceaNanopore/dan/chrisHostDepletionTrial/compareAdapt

## panda time

python3 

import pandas as pd
import os, random
from Bio import SeqIO

os.chdir('/vol/piceaNanopore/dan/chrisHostDepletionTrial/compareAdapt')

head=[ 'qseqid','sseqid','pident','length','mismatch',
  'gapopen','qstart','qend','sstart','send','evalue','bitscore' ]


noAdapt2spruce = pd.read_csv("noAdaptiveSamp_Spruce_blastn.csv", names=head)
adapt2spruce = pd.read_csv("depletionTrial_Spruce_blastn.csv", names=head)
noAdapt2bact = pd.read_csv("noAdaptiveSamp_bacterial_blastn.csv", names=head)
adapt2bact = pd.read_csv("depletionTrial_bacterial_blastn.csv", names=head)
noAdapt2fung = pd.read_csv("noAdaptiveSamp_fungal_blastn.csv", names=head)
adapt2fung = pd.read_csv("depletionTrial_fungal_blastn.csv", names=head)

## one question we could ask is, 
## do average bit scores vary between the adaptive and 
## non-adaptive runs, in alignment to host?
## probably not, but to check:

noAdapt2spruce['bitscore'].mean() ## 1200
adapt2spruce['bitscore'].mean() ## 1385

noAdapt2bact['bitscore'].mean() ## 759
adapt2bact['bitscore'].mean() ## 881

noAdapt2fung['bitscore'].mean() ## 624
adapt2fung['bitscore'].mean() ## 694

## in all three, the average bitscore is slightly 
## higher in the adaptive run. No information there.

## lots of questions...
## but to start, I think we need the names of the reads:

adaptFasta = "/vol/piceaNanopore/dan/chrisHostDepletionTrial/nanoporeRunData/allChrisHostDepletionTrial_noShorts.fasta"
allAdaptNamesl = []

for i in SeqIO.parse(adaptFasta, "fasta"):
    allAdaptNamesl.append(i.id)

allAdaptNamesS = pd.Series(allAdaptNamesl)

## make tallies of uniq reads to each category

## try one read with multiple mappings. get decision procedure.

## apply to all reads.

## eat dinner first.

## maybe the best approach is to "normalize" bit score by 
## length of match, then compare?
## but if "normalize" by length, we lose information about the 
## length of the match, and long matches are better...maybe 
## just take the largest bit map score?

## let's find one read with multiple mappings to examine:
 
adapt2spruce
adapt2bact
adapt2fung

adapt2spruce.head()
adapt2bact.head()
adapt2fung.head()


adapt2bact['qseqid'].head().str.contains('a20b6212-b57d-45ec-8287-c2211f219182')

adapt2spruce.set_index('qseqid', inplace=True)
adapt2bact.set_index('qseqid', inplace=True)
adapt2fung.set_index('qseqid', inplace=True)

adapt2spruce.reset_index(inplace=True)
adapt2bact.reset_index(inplace=True)
adapt2fung.reset_index(inplace=True)

adapt2bact.loc['a20b6212-b57d-45ec-8287-c2211f219182']
adapt2fung.loc['a20b6212-b57d-45ec-8287-c2211f219182']

## how do we find shared sequences among these?

adapt2spruce.values

adaptSpruce2bactSet = set(adapt2spruce['qseqid']).intersection(set(adapt2bact['qseqid']))
adaptSpruce2fungSet = set(adapt2spruce['qseqid']).intersection(set(adapt2fung['qseqid']))
adaptBact2fungSet = set(adapt2bact['qseqid']).intersection(set(adapt2fung['qseqid']))

len(adaptBact2fungSet.difference(adaptSpruce2bactSet))

help(set(adapt2bact['qseqid']).intersection)

adaptAllset = (
set(adapt2bact['qseqid'])
  .intersection(set(adapt2fung['qseqid']))
  .intersection(set(adapt2spruce['qseqid']))
)


len(set(adapt2spruce['qseqid'])) ## 1188134 adapt reads that align to bacteria somewhere
len(set(adapt2fung['qseqid'])) ## 439708 adapt reads that align to fungi somewhere
len(set(adapt2bact['qseqid'])) ## 478762 adapt reads that align to bacteria somewhere

len(adaptSpruce2bactSet) ## 420454 reads align to both bacteria and host
len(adaptSpruce2fungSet) ## 384684 reads align to both fungi and host
len(adaptBact2fungSet) ## 284242 reads align to both bacteria and fungi
len(adaptAllset) ## 229675 reads align to all three, spruce/bact/fung

## are there reads that are unique to each?

onlyFung = set(adapt2fung['qseqid']).difference(adapt2bact['qseqid']).difference(adapt2spruce['qseqid'])
onlyBact = set(adapt2bact['qseqid']).difference(adapt2fung['qseqid']).difference(adapt2spruce['qseqid']) 
onlySpruce = set(adapt2spruce['qseqid']).difference(adapt2fung['qseqid']).difference(adapt2bact['qseqid']) 
allMicro = set(adapt2bact['qseqid']).union(adapt2fung['qseqid'])

onlyMicro = (set(adapt2bact['qseqid'])
                .union(adapt2fung['qseqid'])
                .difference(adapt2spruce['qseqid']))

## total unique reads that mapped to something:

allalignedReads = (
set(adapt2spruce['qseqid'])
.union(adapt2bact['qseqid'])
.union(adapt2fung['qseqid']) ) 

len(onlyFung) ## 457
len(onlyBact) ## 3741
len(onlySpruce) ## 612671
len(set(adapt2spruce['qseqid'])) ## 1188134 or 1,188,134

len(set(adapt2bact['qseqid'])) ## 478 762 reads

len(allMicro) ## 634228
len(onlyMicro) ## 58765
len(allalignedReads) ## 1246899 or 1,246,899 reads

## 420 454 reads align to both host and bact. 
len(adaptSpruce2bactSet) 

## this meands 87% of our microbial reads also align to host
## casting doubt on their actual origin
len(adaptSpruce2bactSet) / len(set(adapt2bact['qseqid'])) 

## might be good to venn diagram a bit, this is getting confusing

## the takeaways, we have 58,765 definite microbial reads, out of 1.2 million...
54567 / 1246899 ## 0.0438 or 4.3% in noAdapt

## but a total of 634228 reads that have aligned to the microbes, most of which 
## also aligned to spruce genome. So another simple metric would be total microbial
## reads, don't worry about host:

len(allMicro) / len(allalignedReads) ## 50.9%

## to make the  comparisons, make the important sets also from noAdapt data:

noAdapt2spruce
noAdapt2bact
noAdapt2fung 

noAdaptOnlyMicro = (set(noAdapt2bact['qseqid'])
                .union(noAdapt2fung['qseqid'])
                .difference(noAdapt2spruce['qseqid']))

noAdaptallalignedReads = (set(noAdapt2spruce['qseqid'])
                            .union(noAdapt2bact['qseqid'])
                            .union(noAdapt2fung['qseqid']) ) 

noAdaptallMicro = set(noAdapt2bact['qseqid']).union(noAdapt2fung['qseqid'])

len(noAdaptOnlyMicro) ## 797
len(noAdaptallalignedReads) ## 24378
len(noAdaptallMicro) ## 10832

len(noAdaptOnlyMicro)/len(noAdaptallalignedReads) ## 0.0326 or 3.3%, not much less.
len(noAdaptallMicro)/len(noAdaptallalignedReads) ## 0.444 or 44.4%, so adaptive seq is ~6% higher

len(noAdaptallMicro)/len(noAdaptallalignedReads) ## 0.444 or 44.4%, so adaptive seq is ~6% higher

## so we still have a whole lot of ambiguity. We have to attack this problem somehow...

## the idea here is that if we look at these reads more closely, it is possible that 
## nearly all the noAdapt reads that blast to both host and microbe are actually only host,
## and some of the adapt reads that blast to both host and microbe are actually only microbial

## if either of those are true, it strengthens the case for the adaptive run actually working...

## two ways to go with this, quality cutoffs vs classification

## probably best to do a classification. look at a few examples

## reads that blast to both host and bacteria in noAdapt:

noAdaptSpruce2bactSet = set(noAdapt2spruce['qseqid']).intersection(set(noAdapt2bact['qseqid']))

len(noAdaptSpruce2bactSet) ## 7068 reads align to both host and bacteria in noAdapt data

noAdaptSpruce2bactSet

## subset our df to this:

noAdapt2spruce.set_index('qseqid', inplace=True)
noAdapt2bact.set_index('qseqid', inplace=True)

aa = noAdapt2spruce.loc[list(noAdaptSpruce2bactSet)] ## reads 
bb = noAdapt2bact.loc[list(noAdaptSpruce2bactSet)]
cc = pd.concat([aa,bb])

seq="ca93338d-31db-4f97-8295-80b856663911" ## example where we categorize to bacteria
seq="b9647056-50ae-40db-b2d6-cc3237b68226" ## example where we categorize to host
dd = cc.loc[seq]; dd

## or just:
seq=random.sample(list(noAdaptSpruce2bactSet),1)[0]
dd = cc.loc[seq]; dd

## problem is, this is assymetrical. Reads that come from the host should have a 
## really high identity. And reads that come from bacteria will not, because
## chances are we don't have an exact match to their actual genome, even with 
## 20,000 genomes or whatever we got...

## maybe it makes most sense to take strict alignments out (> 95% sequence ID?)
## and assume the rest are microbial?

## noAdapt, take all reads that aligned to a microbe, then remove any that 
## have >95% sequence identity to a host read

## mean %identity for reads that blast to host: 

noAdapt2spruce['pident'].mean() ## 83%

adapt2spruce['pident'].mean() ## 85%. Actually higher, maybe. 

## jeezus, is there any difference? We excluded 600,000 reads 
## and it doesn't matter? I am so confused. 

aa = noAdapt2spruce.loc[list(noAdaptSpruce2bactSet)] ## reads 

aa[aa['pident'] > 94.999].shape

noAdapt2spruce.shape #29972
noAdapt2spruce[noAdapt2spruce['pident'] > 94.999].shape ## 484, miniscule amount

pd.set_option('display.max_rows', None)

noAdapt2spruce.shape
noAdapt2spruce[noAdapt2spruce['pident'] > 94.999].shape
noAdapt2bact[noAdapt2bact['pident'] > 94.999].shape

pd.reset_option('all')

## nope. Strict matches to host don't seem to be anymore common in our 
## nonAdaptive run, in the reads that do actually align to host.

## okay, let's iterate through, make matches, and take those with the highest
## bit score. I have a feeling this will also show no difference between
## the two runs, but we have to try:

noAdapt2spruce.set_index('qseqid', inplace=True)
noAdapt2bact.set_index('qseqid', inplace=True)

## they both need an identity column before we concat:

len(noAdaptSpruce2bactSet)

aa = noAdapt2spruce.loc[list(noAdaptSpruce2bactSet)]
aa['aligned2'] = 'spruce'
bb = noAdapt2bact.loc[list(noAdaptSpruce2bactSet)]
bb['aligned2'] = 'bact'
cc = pd.concat([aa,bb])

## the process for one read would go like this:
ff = pd.DataFrame(columns=['qseqid','aligned2'])

seq=random.sample(list(noAdaptSpruce2bactSet),1)[0]
dd = cc.loc[seq]
dd.reset_index(inplace=True)
ee = dd.iloc[dd['bitscore'].idxmax(), ][['qseqid','aligned2']]
ff = pd.concat([ff,ee.to_frame().T], ignore_index=True)


## do this as a loop

noAdaptSpruce2bactSet = set(noAdapt2spruce['qseqid']).intersection(set(noAdapt2bact['qseqid']))
aa = noAdapt2spruce.loc[list(noAdaptSpruce2bactSet)]
aa['aligned2'] = 'spruce'
bb = noAdapt2bact.loc[list(noAdaptSpruce2bactSet)]
bb['aligned2'] = 'bact'
cc = pd.concat([aa,bb])
ff = pd.DataFrame(columns=['qseqid','aligned2'])
for seq in list(noAdaptSpruce2bactSet):
    dd = cc.loc[seq]
    dd.reset_index(inplace=True)
    ee = dd.iloc[dd['bitscore'].idxmax(), ][['qseqid','aligned2']]
    ff = pd.concat([ff,ee.to_frame().T], ignore_index=True)

print(ff.groupby('aligned2').count())

## inputs to this are: 
## df of reads aligned to spruce
## df of reads aligned to bact

## so we can make this a function/script:

## do this outside of conda...
pip3 install Biopython
pip3 install pandas

###################################################################
#!/usr/bin/env/python3

import pandas as pd
import os, random
from Bio import SeqIO

os.chdir('/vol/piceaNanopore/dan/chrisHostDepletionTrial/compareAdapt')

def bestGuessBactSpruce(aligned2spruceDF, aligned2bactDF):
  Spruce2bactSet = set(aligned2spruceDF['qseqid']).intersection(set(aligned2bactDF['qseqid']))
  aligned2spruceDF.set_index(['qseqid'], inplace=True)
  aligned2bactDF.set_index(['qseqid'], inplace=True)
  aa = aligned2spruceDF.loc[list(Spruce2bactSet)]
  aa['aligned2'] = 'spruce'
  bb = aligned2bactDF.loc[list(Spruce2bactSet)]
  bb['aligned2'] = 'bact'
  cc = pd.concat([aa,bb])
  ff = pd.DataFrame(columns=['qseqid','aligned2'])
  for seq in list(Spruce2bactSet):
      dd = cc.loc[seq]
      dd.reset_index(inplace=True)
      ee = dd.iloc[dd['bitscore'].idxmax(), ][['qseqid','aligned2']]
      ff = pd.concat([ff,ee.to_frame().T], ignore_index=True)
  return(ff)

head=[ 'qseqid','sseqid','pident','length','mismatch',
  'gapopen','qstart','qend','sstart','send','evalue','bitscore' ]

noAdapt2spruce = pd.read_csv("noAdaptiveSamp_Spruce_blastn.csv", names=head)
adapt2spruce = pd.read_csv("depletionTrial_Spruce_blastn.csv", names=head)
noAdapt2bact = pd.read_csv("noAdaptiveSamp_bacterial_blastn.csv", names=head)
adapt2bact = pd.read_csv("depletionTrial_bacterial_blastn.csv", names=head)

noAdaptSeqCategorized = bestGuessBactSpruce(noAdapt2spruce, noAdapt2bact)
adaptSeqCategorized = bestGuessBactSpruce(adapt2spruce, adapt2bact)


noAdaptSeqCategorized.to_csv('noAdaptSeqCategorized.csv')
adaptSeqCategorized.to_csv('adaptSeqCategorized.csv')
#############################################################################3

print(noAdaptSeqCategorized.groupby('aligned2').count())
print(adaptSeqCategorized.groupby('aligned2').count())

cd /vol/piceaNanopore/dan/chrisHostDepletionTrial/compareAdapt

chmod 777 categorizeSeqsSpruceOrBact.py

nohup python3 ./categorizeSeqsSpruceOrBact.py &

python3
import pandas as pd
import os, random
from Bio import SeqIO

os.chdir('/vol/piceaNanopore/dan/chrisHostDepletionTrial/compareAdapt')

noAdaptSeqCategorized = pd.read_csv('noAdaptSeqCategorized.csv')
adaptSeqCategorized = pd.read_csv('adaptSeqCategorized.csv')

noAdaptSeqCategorized.head()

noAdaptSeqCategorized.groupby('aligned2').count()

adaptSeqCategorized.groupby('aligned2').count()
49874/(370580 + 49874) ## 11 percent

## so we don't see a dramatic increase in bacterial reads, I think. 

######################################################

## and so we try again, with a different experimental approach

## we'll break up the flow cell into three experiments

## 1) control (25% of bp), to give us a benchmark for how much enrichment actually occurred
## 2) host depletion (25% of bp), 
## 3) bacterial genome enrichment. We will choose bacterial genomes to enrich based on the bacterial 
##    reads that we recover from the above. Presumably these will correspond to the most abundant
##    bacteria in the sample

## there are three ways to measure flow cell outputs: 

## 1 - file sizes. How many gig does a flow cell output?
## 2 - number of bp. should correlate very strongly with #1
## 3 - number of reads, changes radically with the distribution of read lengths.

## how did the last flow cell perform?
## how does this compare to published results


## 

cd /var/lib/minknow/data/
du -h --max-depth=1

cd /var/lib/minknow/data/repeatingDansStuff1

## maybe here?
cd /var/lib/minknow/data/repeatingDansStuff1/21_7_23/20230721_1214_MN40608_FAS81869_d4801a32

less final_summary_FAS81869_d4801a32_55579c61.txt

## sequencing went on for three days, sounds right. 
## 2023-07-21T12:17:33.526256+02:00
## 2023-07-24T16:39:06.924865+02:00


## I think the good report is here, get it local:
path2get=/var/lib/minknow/data/repeatingDansStuff1/21_7_23/20230721_1214_MN40608_FAS81869_d4801a32/throughput_FAS81869_d4801a32_55579c61.csv
scp -r -i ~/.ssh/id_ed25519 test@132.180.112.115:$path2get .

## look at this in python:

python3
import pandas as pd
import matplotlib.pyplot as plt; plt.ion()


aa = pd.read_csv("throughput_FAS81869_d4801a32_55579c61.csv")

aa.head()

aa.tail()

## we reached a total of 8574837804 bases sequenced 
## = 8,574,837,804 , or 8 billion bases

## this was 2,427,820 reads.

## so our control should have ~
(8574837804 / 4) ## 2143709451 or 2,143,709,451 or ~2000000000 bp

(8574837804 / 5) ## 1714967560.8 or 1,714,967,560 or ~1.7 billion bp

## our host depletion should have ~
(8574837804 / 4) ## 2143709451 or 2,143,709,451 or ~2000000000 bp
(8574837804 / 5) ## 1714967560.8 or 1,714,967,560 or ~1.7 billion bp

## if we go with 1.7 bp, how long did this take last time

## and our final, microbially enriched effort should use the rest, hopefully 4+ billion bp

## how long will this take:

plt.plot(aa['Experiment Time (minutes)']/60,aa['Basecalled Bases'])

##### try host depletion again #####

## now we will repeat the initial experiment of depleting host
## let's try refreshing the mmi file, don't know if the 
## one we have really works

cd /media/vol1/daniel/spruce

cd /media/vol1/daniel/hostDepletion/ourData/

\time minimap2 -d Pabies_repeatsCompressed_mt_ch.mmi Pabies_repeatsCompressed_mt_ch.fa 
## that used ~24gig memory. not horrible

## so our index is here:
index=/media/vol1/daniel/spruce/Pabies_repeatsCompressed_mt_ch.mmi

cd /media/vol1/daniel/hostDepletion/hostDepletion3phase

## our toml file should look something like this:

##### hostDepletion3phase_phase2.toml #########
[caller_settings]
config_name = "dna_r10.4.1_e8.2_400bps_fast.cfg"
host = "ipc:///tmp/.guppy/"
port = 5555

[conditions]
reference = "/media/vol1/daniel/spruce/Pabies_repeatsCompressed_mt_ch.mmi"

[conditions.0]
name = "hostDepletion3phase_phase2"
control = false
min_chunks = 0
max_chunks = 3
targets = []
single_on = "unblock"
multi_on = "unblock"
single_off = "unblock"
multi_off = "unblock"
no_seq = "proceed"
no_map = "stop_receiving"
############################

readfish validate hostDepletion3phase_phase2.toml
## returns the following which I do not understand:

"""
Region 'hostDepletion3phase_phase2' (control=False) has 0 contigs of
which 0 are in the reference.
"""

## sounds ominous. Other than that, seems ok. 

## let's try it and see if it rejects reads.

## readfish command should look like this:

## should probably run this on the machine itself, and not remotely.

cd /media/vol1/daniel/hostDepletion/hostDepletion3phase

readfish targets --device MN40608 \
              --experiment-name "hostDepletion3phase" \
              --toml hostDepletion3phase_phase2.toml \
              --log-file hostDepletion3phase_phase2.log


## alternatively, maybe I can install minknow on my local machine? meh. 

## if I start this now, it will have to run for ~4 hours...

## as soon as possible, we'll need to start the alignments from these
## runs to get bacterial genomes for enrichment. 


##### get bacterial reads out of controls #####

## so we've run through about half the flow cell
## on a non-adaptive sample and a host-depletion 
## sample. Now it's time to try to enrich for 
## specific bacteria. We only have one DNA sample
## prepped, and its been on the sequencer for 
## 20 hours. I thought we would have multiple
## preps...we don't, so we have to be ~quick here.

## step one would be a minimap alignment to 
## to host, to remove most of the uncertain 
## reads. 

## step two would be a blast or minimap alignment
## to the concatenated reference genomes that 
## we have from NCBI

## we probably need denbi for this. We know that 
## the adaptive sampling with host depletion probably
## didn't do much. As in, they are probably pretty
## similar in composition, maybe slightly higher
## microbial reads in the host-depletion. 

## so get them onto denbi, index the host 
## genome, then take all the bad matches:

## get both files onto denbi, have to send them 
## from the lab computer. For the moment, 
## let's put them in the ephemeral storage:

/mnt/ephem

## transfer the phase1 passed reads from labcomp to denbi: ##
cd /var/lib/minknow/data/experiment_spruce_threephase/controll_130923/20230913_1355_MN40608_FAX46654_83018e78
## add in some of the extra info
cp throughput_FAX46654_83018e78_fa004816.csv fastq_pass/
tar cvf phase1control.tar fastq_pass/
## ship it
path2get="/var/lib/minknow/data/experiment_spruce_threephase/controll_130923/20230913_1355_MN40608_FAX46654_83018e78/phase1control.tar"
path2put="/mnt/ephem/"
scp -rP 30419 -i ~/.ssh/lab2denbi $path2get ubuntu@129.70.51.6:$path2put

## transfer the phase2 passed reads from labcomp to denbi: ##
cd /var/lib/minknow/data/experiment_spruce_threephase/phase2hostDepletion/20230913_2238_MN40608_FAX46654_d4c9fea9
## add in some of the extra info
cp throughput_FAX46654_d4c9fea9_2bd0e514.csv fastq_pass/
cp unblocked_read_ids.txt fastq_pass/
tar cvf phase2hostDepletion.tar fastq_pass/
## ship it
path2get=/var/lib/minknow/data/experiment_spruce_threephase/phase2hostDepletion/20230913_2238_MN40608_FAX46654_d4c9fea9/phase2hostDepletion.tar
path2put="/mnt/ephem/"
scp -rP 30419 -i ~/.ssh/lab2denbi $path2get ubuntu@129.70.51.6:$path2put

## back on denbi
## now extract, decompress, concat these

cd /mnt/ephem/look4bact3phase

mkdir phase1control

cd /mnt/ephem/look4bact3phase/phase1control/

tar xvf phase1control.tar

mv fastq_pass/ phase1controlReads/


cd /mnt/ephem/look4bact3phase/phase1control/phase1controlReads
gunzip *

cat *fastq > phase1controlReads.fastq
mv phase1controlReads.fastq ../
gzip *

## our concatenated reads are therefore in a single file at:
ls /mnt/ephem/look4bact3phase/phase1control/phase1controlReads.fastq

mkdir phase2hostDepletion/

tar xvf phase2hostDepletion.tar

mv fastq_pass/ phase2hostDepletionReads/

cd /mnt/ephem/look4bact3phase/phase2hostDepletion/phase2hostDepletionReads
gunzip *
cat *fastq > phase2hostDepletionReads.fastq
mv phase2hostDepletionReads.fastq ../
gzip *


#### align 3phase reads to host:

## we have an mmi of host here, the one we 
## used for readfish, on nanocomp...is it on denbi?

## something similar here:
cd /vol/piceaNanopore/dan/chrisHostDepletionTrial/spruceRefGenome

\time minimap2 -d Pabies-haploid_withOrganelles.mmi Pabies-haploid_withOrganelles.fa
## used ~24 gig, for a 9.5 gig genome. 
## so don't think it will be possible to do the really large fastas of 
## concatenated bacterial ref genomes, probably have to stick with blast for that...

ls $sprucemmi

grep -c "^@.*runid=" phase1controlReads.fastq

wc -l phase1controlReads.fastq ## 310832 reads. seems small

## now align our control reads against this host genome:

cd /mnt/ephem/look4bact3phase/phase1control



grep -c "^@.*runid=" phase1controlReads.fastq ## 310832 reads. seems small

sprucemmi=/vol/piceaNanopore/dan/chrisHostDepletionTrial/spruceRefGenome/Pabies-haploid_withOrganelles.mmi
reads=/mnt/ephem/look4bact3phase/phase1control/phase1controlReads.fastq
cd /mnt/ephem/look4bact3phase/phase1control
nohup minimap2  -I100g -t 20 -ax map-ont $sprucemmi $reads 1> phase1control_Align2Spruce.sam 2> phase1control_Align2Spruce.log &
## outputs are funny, no header, samtools won't play with it

## try with paf file format
cd /mnt/ephem/look4bact3phase/phase1control
sprucemmi=/vol/piceaNanopore/dan/chrisHostDepletionTrial/spruceRefGenome/Pabies-haploid_withOrganelles.mmi
reads=/mnt/ephem/look4bact3phase/phase1control/phase1controlReads.fastq
nohup minimap2 \
    -I100g \
    --secondary=no \
    -t 20 \
    -x map-ont \
    $sprucemmi $reads 1> phase1control_Align2Spruce.paf 2> phase1control_Align2Spruce.paf.log &

## removing secondary alignments takes us from 13,642,947 to 9,868,731 alignments
## this should create a blastn-like output. take into pandas, get all the really bad or missing matches:

cut -f1 phase1control_Align2Spruce.paf | sort | uniq | wc -l ## 297780 uniq sequences, did we lose some? 

less phase1control_Align2Spruce.paf ## needs some trimming:


## test it out:
head -n 3 phase1control_Align2Spruce.paf | cut -f13- --complement  

cut -f13- --complement phase1control_Align2Spruce.paf > phase1control_Align2Spruce.tsv


## get this into python, get all the shitty or non-alignments:

python3
import pandas as pd
from Bio import SeqIO
import os

pafHeaders=[ "QueryName","QueryLength","QueryStart","QueryEnd","RelativeStrand",
"TargetName","TargeLength","TargetStart","TargetEnd",
"ResidueMatches","AlignmentBlockLngth","MappingQuality"]
aa=pd.read_csv("phase1control_Align2Spruce.tsv", sep="\t", names=pafHeaders)

aa.head()

aa['MappingQuality'].max() ##60, makes sense
aa['MappingQuality'].min() ##0, also makes sense
aa['MappingQuality'].mean() ## 24, huh, not great. but who cares right now? 

## these are phred scores of the probability of a read being misaligned
## phred scores a negative log, higher means lower prob of being wrong, 
## or higher prob of being right. 

## I think I had this mixed up earlier.

## anyway, maybe we take all the <10 matches?

## first, we need to get the best-scoring alignment
## for every read...

## I think we can get that this way:
bb = aa.groupby('QueryName')['MappingQuality'].max()

bb.shape ## 297780

aa['QueryName'].unique().shape ## 297780, same, good
## but there are 310832 reads in the run...what happened
## to the others? that's ~13,000 reads, could be useful here... 

310832 - 297780 ## 13,052 reads not included  

#filter = bb == 0 
#bb[filter].shape ## 520 reads if we use mq=0 

filter = bb < 5 
cc = bb[filter].index.to_list()

len(cc) ## 1760 reads if we use mq=5, keep it there

aa.set_index('QueryName', inplace=True)

pd.set_option('display.max_rows', None)

aa.loc[cc] ## we still have lots of multimatches, just of poor quality

pd.reset_option('all')

aa.reset_index(inplace=True)

## so these are the reads we should check for bacterial alignments.

## but there are still 12000 reads missing out there - why didn't minimap 
## include these?

## which are they?
readsInAlignment = bb.index.to_list()

len(readsInAlignment)

## we should be able to read in the fastq, and keep just the poor matches 
## above, plus the reads that seem to have been discarded by minimap

poorMatches = []
for seqRec in SeqIO.parse("phase1controlReads.fastq", "fastq"):
    print(seqRec.id)
    if seqRec.id in cc: 
      poorMatches.append(seqRec)

notInAlignment = []
for seqRec in SeqIO.parse("phase1controlReads.fastq", "fastq"):
    print(seqRec.id)
    if seqRec.id not in readsInAlignment: 
      notInAlignment.append(seqRec)
      print("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")

len(poorMatches) ##1760
len(notInAlignment) ##13052

## qualities are still in there:
print(poorMatches[0].format("fastq"))

## add the reads that were left out by minimap

notInAlignment[0]

poorOrExluded = poorMatches + notInAlignment

len(poorOrExluded)

## write these out as a fasta:
poorOrExluded

SeqIO.write(poorOrExluded, "phase1controlReads_notAligned.fasta", "fasta")

## then run blast alignment against the bact ref genome

## our blastn database for the bacterial reference genomes are here:

conda activate blast

cd /mnt/ephem/look4bact3phase/phase1control/
bactRefGenome=/vol/piceaNanopore/dan/bactGenomes/allbactGenomes.fa
phase1unaligned=phase1controlReads_notAligned.fasta
nohup blastn -db $bactRefGenome \
  -query $phase1unaligned \
  -num_threads 25 \
  -num_alignments 3 \
  -out phase1control_bacterial_alignments.csv \
  -outfmt 10 &

## started at 16:00 on thursday
## finished in ~1 hour



###### repeat with phase2 ##########

cd /mnt/ephem/look4bact3phase/phase2hostDepletion

grep -c "^@.*runid=" phase2hostDepletionReads.fastq ## 587477 Comparable.

cd /mnt/ephem/look4bact3phase/phase2hostDepletion

#sprucemmi=/vol/piceaNanopore/dan/chrisHostDepletionTrial/spruceRefGenome/Pabies-haploid_withOrganelles.mmi
#reads=/mnt/ephem/look4bact3phase/phase2hostDepletion/phase2hostDepletionReads.fastq
#nohup minimap2  -I100g -t 20 -ax map-ont $sprucemmi $reads 1> phase2hostDepletion_Align2Spruce.sam 2> phase2hostDepletion_Align2Spruce.log &
#samtools view phase2hostDepletion_Align2Spruce.sam 

## yeah, also fucked

## do paf instead

#cd /mnt/ephem/look4bact3phase/phase2hostDepletion
#sprucemmi=/vol/piceaNanopore/dan/chrisHostDepletionTrial/spruceRefGenome/Pabies-haploid_withOrganelles.mmi
#reads=/mnt/ephem/look4bact3phase/phase2hostDepletion/phase2hostDepletionReads.fastq
#nohup minimap2  -I100g -t 20 -x map-ont $sprucemmi $reads 1> phase2hostDepletion_Align2Spruce.paf 2> phase2hostDepletion_Align2Spruce.paf.log &

## try this instead, no secondary:
cd /mnt/ephem/look4bact3phase/phase2hostDepletion
sprucemmi=/vol/piceaNanopore/dan/chrisHostDepletionTrial/spruceRefGenome/Pabies-haploid_withOrganelles.mmi
reads=/mnt/ephem/look4bact3phase/phase2hostDepletion/phase2hostDepletionReads.fastq
nohup minimap2 \
  -I100g \
  -t 20 \
  -x map-ont \
  --secondary=no \
  $sprucemmi $reads \
  1> phase2hostDepletion_Align2Spruce.paf \
  2> phase2hostDepletion_Align2Spruce.paf.log &


cut -f13- --complement phase2hostDepletion_Align2Spruce.paf > phase2hostDepletion_Align2Spruce.tsv

head phase2hostDepletion_Align2Spruce.tsv

wc -l phase2hostDepletion_Align2Spruce.tsv ## 10604949

cut -f1 phase2hostDepletion_Align2Spruce.tsv | sort | uniq | wc -l ## 562733

grep -c "@.*runid=" phase2hostDepletionReads.fastq ## 587,477 
wc -l phase2hostDepletionReads.fastq ## 587,477 sequences. As before, so are not included in the alignment

## python to parse results

python3
import pandas as pd
from Bio import SeqIO
import os

pafHeaders=[ "QueryName","QueryLength","QueryStart","QueryEnd","RelativeStrand",
"TargetName","TargeLength","TargetStart","TargetEnd",
"ResidueMatches","AlignmentBlockLngth","MappingQuality"]
aa=pd.read_csv("phase2hostDepletion_Align2Spruce.tsv", sep="\t", names=pafHeaders)


aa.head()

aa['MappingQuality'].max() ##60
aa['MappingQuality'].min() ##0
aa['MappingQuality'].mean() ## 22
aa['QueryName'].unique().shape ## 562733

bb = aa.groupby('QueryName')['MappingQuality'].max()
filter = bb < 5 
cc = bb[filter].index.to_list()

len(cc) ## 4752 reads if we use mq=5, keep it there

pd.set_option('display.max_rows', None)

aa.set_index('QueryName', inplace=True)

aa.loc[cc] ## we still have lots of multimatches, just of poor quality

pd.reset_option('all')

aa.reset_index(inplace=True)

readsInAlignment = bb.index.to_list()

poorMatches = []
for seqRec in SeqIO.parse("phase2hostDepletionReads.fastq", "fastq"):
    print(seqRec.id)
    if seqRec.id in cc: 
      poorMatches.append(seqRec)

len(poorMatches) ## 4752, seems right

587477 - 562733 ## 24,744 reads not included in the alignment!

readsInAlignment = bb.index.to_list()
notInAlignment = []; a=0; b=587477; c=0; d=24744
for seqRec in SeqIO.parse("phase2hostDepletionReads.fastq", "fastq"):
    a+=1; print(f'{a} records checked out of {b} records in fastq file')
    if seqRec.id not in readsInAlignment: 
      c+=1; print(f'found another read not in the alignment: {c} reads written out of total {d} reads excluded from alignment to host')
      notInAlignment.append(seqRec)


len(poorMatches) ##4752
len(notInAlignment) ##24744

poorOrExluded = poorMatches + notInAlignment

len(poorOrExluded)

## write these out as a fasta:

poorOrExluded

SeqIO.write(poorOrExluded, "phase2hostDepletion_notAligned.fasta", "fasta")

conda activate blast

cd /mnt/ephem/look4bact3phase/phase2hostDepletion

bactRefGenome=/vol/piceaNanopore/dan/bactGenomes/allbactGenomes.fa
phase2unaligned=phase2hostDepletion_notAligned.fasta

nohup blastn -db $bactRefGenome \
  -query $phase2unaligned \
  -num_threads 25 \
  -num_alignments 3 \
  -out phase2hostDepletion_bacterial_alignments.csv \
  -outfmt 10 &

#### connect the blast matches to genomes  ####

## okay, now we need to connect the accession numbers in this table to their 
## genome:

head /mnt/ephem/look4bact3phase/phase1control/phase1control_bacterial_alignments.csv

bactRefGenome=/vol/piceaNanopore/dan/bactGenomes/allbactGenomes.fa

## for example, where is NZ_JANRML010000035.1?

grep "NZ_JANRML010000035.1" $bactRefGenome

## that is going to take forever...
## we need to get a catalogue of where these things came from. 

less $bactRefGenome

head -n 30 $bactRefGenome | grep "^>" 

cd /mnt/ephem/look4bact3phase

grep "^>" $bactRefGenome > allRefBactHeaders.txt

grep -c "^>" $bactRefGenome ## 5941882
wc -l allRefBactHeaders.txt ## 5941882

head allRefBactHeaders.txt 

tail allRefBactHeaders.txt | cut -f1-3 -d" " | sed "s/ /,/" |
sed "s/>//"

cut -f1-3 -d" " allRefBactHeaders.txt | sed "s/ /,/" |
sed "s/>//" > allRefBactHeaders.csv


## ah, there is an extra comma here:
sed -n '35522p' allRefBactHeaders.csv 
## and here
sed -n '37404p' allRefBactHeaders.csv 

## actually, there are like 100 of those things:
grep -c ",$" allRefBactHeaders.csv ##111

sed -i "s/,$//" allRefBactHeaders.csv ##111

head allRefBactHeaders.csv  

less /mnt/ephem/bactGenomes/genNames.txt

grep "Chroococcidiopsis" /mnt/ephem/bactGenomes/genNames.txt

grep "Chroococcidiopsis" /mnt/ephem/bactGenomes/assembly_summary.txt

grep "NZ_MVDI01000074" /mnt/ephem/bactGenomes/assembly_summary.txt

## okay, done. How can we turn this into something useful?
## one thing we have here is RAM. 
## so try a pandas solution

conda deactivate

python3
import pandas as pd
from Bio import SeqIO
import os

aa = pd.read_csv('allRefBactHeaders.csv', names=['accession', 'sp'])
aa.shape ## 5941882

## that works, but doesn't actually help us find the genomes we 
## need, give a list of of accession numbers that are contigs.

## we are going to have to match up species names with genome file 
## names.

## this means looking in the first line of each genome file and 
## and getting the species name out of it. 
## ugh. 

cd /mnt/ephem/look4bact3phase

for i in /mnt/ephem/bactGenomes/refSeq/*; do
  echo -n $i >> bactRefGenFirstLines.txt
  sed -n "1p" $i >> bactRefGenFirstLines.txt
done &

wc -l bactRefGenFirstLines.txt ## 67014 fits the number of genomes

head bactRefGenFirstLines.txt

## this seems to work:
head bactRefGenFirstLines.txt | cut --complement -f1-5 -d"/" | sed 's/ /\t/'| sed 's/>/\t/'
tail -n 4 bactRefGenFirstLines.txt | cut --complement -f1-5 -d"/" | sed 's/ /\t/'| sed 's/>/\t/'

cut --complement -f1-5 -d"/" bactRefGenFirstLines.txt | sed 's/ /\t/'| sed 's/>/\t/' > bactRefGenFirstLines.tsv


## now use this third column to look for genome files to grab
## this will be difficult, probably best done in python...

## so for example, we need these organisms from our control:
/mnt/ephem/look4bact3phase/phase1control/phase1control_bacterial_alignments.csv

## for each line of this, we need to take the contig name,
## look for a string match somewhere in third column of
/mnt/ephem/look4bact3phase/bactRefGenFirstLines.tsv
## and take the genome name (first column)

## that will tell us which genomes to grab.
## but once we do that, we have to make 
## targets out of these, somehow...

## think about that in a minute. first, which genomes
## do we need?

cd /mnt/ephem/look4bact3phase

python3
import pandas as pd
from Bio import SeqIO
import os

bactRefGenomes = pd.read_csv('/mnt/ephem/look4bact3phase/bactRefGenFirstLines.tsv', names=['filename','accession', 'sp'], sep='\t')

## for phase1control:

head=[ 'qseqid','sseqid','pident','length','mismatch',
  'gapopen','qstart','qend','sstart','send','evalue','bitscore' ]
phase1aligns=pd.read_csv("/mnt/ephem/look4bact3phase/phase1control/phase1control_bacterial_alignments.csv", names=head)

phase1aligns.head()

pd.set_option('display.max_rows', None)
phase1aligns ## there is a diversity of microbes here
pd.reset_option('all')

phase1aligns['sseqid'].unique().shape ## there are 4000 different contigs aligned too

phase1bactContigs=pd.Series(phase1aligns['sseqid'].unique())

pd.set_option('display.max_rows', None)
#phase1bactContigs
bactRefGenomes['accession']
pd.reset_option('all')

bactRefGenomes['accession'].shape ## 67014
bactRefGenomes['accession'].unique().shape ## 67014 all unique, that's good

bactRefGenomes['accession'].str[0:10].unique() ## 61439 some repeats

bactRefGenomes['accession'].str[0:10].unique().shape ## 61439 some repeats?? how can there be more than:
bactRefGenomes['accession'].str[0:11].unique().shape ## 67014 ??? we recover original, lower number.
## don't have time to figure this out. Assume we need 0:11

## we should be able to shorten our accession numbers to this,

phase1aligns['sseqid'].str[0:11].unique().shape ## this brings us to 3045 uniq values

phase1bactAccessions = pd.Series(phase1aligns['sseqid'].str[0:11].unique())

phase1bactAccessions.iloc[0]

## and can we find these in our genomes

aa=phase1bactAccessions.iloc[0]
bactRefGenomes['accession'].str.contains(aa).sum()

## let's check, with these acession names, are all represented by a genome?
bb = phase1bactAccessions.apply(lambda x: bactRefGenomes['accession'].str.contains(x).any()) 

bactRefGenomes['accession']

bb.sum() ## 2863, out of 3045 unique contig names, so nope 

## ugh, why didn't I put genome names in the contig names!!???

## are there contigs that are represented multiple times?
phase1aligns.head()

phase1aligns.groupby('sseqid').count()['length']

(phase1aligns.groupby('sseqid').count()['length'] > 1).sum() ## 854 yes, lots
(phase1aligns.groupby('sseqid').count()['length'] > 2).sum() ## 428
(phase1aligns.groupby('sseqid').count()['length'] > 3).sum() ## 307
(phase1aligns.groupby('sseqid').count()['length'] > 4).sum() ## 236
(phase1aligns.groupby('sseqid').count()['length'] > 5).sum() ## 193
(phase1aligns.groupby('sseqid').count()['length'] > 6).sum() ## 163
(phase1aligns.groupby('sseqid').count()['length'] > 7).sum() ## 163
(phase1aligns.groupby('sseqid').count()['length'] > 10).sum() ## 105
(phase1aligns.groupby('sseqid').count()['length'] > 20).sum() ## 53
(phase1aligns.groupby('sseqid').count()['length'] > 30).sum() ## 41
(phase1aligns.groupby('sseqid').count()['length'] > 100).sum() ## 18
(phase1aligns.groupby('sseqid').count()['length'] > 200).sum() ## 11
(phase1aligns.groupby('sseqid').count()['length'] > 250).sum() ## 9
(phase1aligns.groupby('sseqid').count()['length'] > 300).sum() ## 7

## how about we take just those contigs that appear at least 200 times?

## which are these?

phase1aligns.head()

aa = pd.Series(phase1aligns['sseqid'].unique())
filter = (phase1aligns.groupby('sseqid').count()['length'] > 200).to_list()

phase1controlVeryCommonContigs = aa[filter]
phase1controlVeryCommonContigs.reset_index(inplace=True, drop=True)

phase1controlVeryCommonContigs_short = phase1controlVeryCommonContigs.str[0:11]

## can we find these genomes?

bb = phase1controlVeryCommonContigs_short.apply(lambda x: bactRefGenomes['accession'].str.contains(x).any()) 
bb.all() ## they all seem to be discoverable this way

phase1controlVeryCommonContigs_short

bactRefGenomes.head()

bactRefGenomes['access_short'] = bactRefGenomes['accession'].str[0:11] 

genomeFilter = bactRefGenomes['access_short'].apply(lambda x: x in phase1controlVeryCommonContigs_short.values)

bactRefGenomes[genomeFilter]

phase1controlCommonBact = bactRefGenomes[genomeFilter]

help(phase1controlCommonBact.to_csv)

phase1controlCommonBact.to_csv('phase1controlCommonBact.csv', index=False)

## these files should be:
  GCF_000153465.1_ASM15346v1_genomic.fna
     GCF_000618385.1_EcK1795_genomic.fna
 GCF_001896145.1_ASM189614v1_genomic.fna
 GCF_002109385.1_ASM210938v1_genomic.fna
 GCF_003258865.1_ASM325886v1_genomic.fna
 GCF_004342085.1_ASM434208v1_genomic.fna
 GCF_005780165.1_ASM578016v1_genomic.fna
GCF_014199735.1_ASM1419973v1_genomic.fna
GCF_014200015.1_ASM1420001v1_genomic.fna
GCF_014636175.1_ASM1463617v1_genomic.fna
       GCF_022664535.1_Sx8-8_genomic.fna

## btw, I think we should include a Corynebacterium genome, it keeps popping up.

## okay, can we repeat this with the new alignment?

####### find common bact from host depletion ########

cd /mnt/ephem/look4bact3phase

python3
import pandas as pd
from Bio import SeqIO
import os

bactRefGenomes = pd.read_csv('/mnt/ephem/look4bact3phase/bactRefGenFirstLines.tsv', names=['filename','accession', 'sp'], sep='\t')

head=[ 'qseqid','sseqid','pident','length','mismatch',
  'gapopen','qstart','qend','sstart','send','evalue','bitscore' ]
phase2aligns=pd.read_csv("/mnt/ephem/look4bact3phase/phase2hostDepletion/phase2hostDepletion_bacterial_alignments.csv", names=head)

phase2aligns.head()

phase2aligns.shape ##33238 alignments

phase2aligns['sseqid'].unique().shape ## there are 7714 different contigs aligned too


phase2aligns.groupby('sseqid').count()['length']

phase2aligns['sseqid'].str[0:11].unique().shape ## this brings us to 3045 uniq values
## to sleepy to think about what that means..I think it means that there 
## probably 3045 genomes represented, so ~3045 "otus", strain level

aa = pd.Series(phase2aligns['sseqid'].unique())

## we have a larger dataset. contigs that have been aligned to <500 times
## gets us to 9 contigs, probably nine genomes
filter = (phase2aligns.groupby('sseqid').count()['length'] > 200).to_list()
sum(filter) ## 22 contigs

filter = (phase2aligns.groupby('sseqid').count()['length'] > 400).to_list()
sum(filter) ## 13 contigs

filter = (phase2aligns.groupby('sseqid').count()['length'] > 500).to_list()
sum(filter) ## 9 contigs


phase2hostDepletionVeryCommonContigs = aa[filter]

phase2hostDepletionVeryCommonContigs.reset_index(inplace=True, drop=True)

phase2hostDepletionVeryCommonContigs_short = phase2hostDepletionVeryCommonContigs.str[0:11]

genomeFilter = bactRefGenomes['access_short'].apply(lambda x: x in phase2hostDepletionVeryCommonContigs_short.values)

bactRefGenomes[genomeFilter]

phase2hostDepletionCommonBact = bactRefGenomes[genomeFilter]
phase2hostDepletionCommonBact.to_csv('phase2hostDepletionCommonBact.csv', index=False)

### ok, so concatenate these:

phase1controlCommonBact = pd.read_csv('phase1controlCommonBact.csv')

phase1controlCommonBact 
phase2hostDepletionCommonBact

interestingGenomes = pd.concat([phase1controlCommonBact,phase2hostDepletionCommonBact]).reset_index(drop=True)

interestingGenomes

## now, how do we convert this into a useful target for readfish?

cd /mnt/ephem/look4bact3phase

interesting=(
/mnt/ephem/bactGenomes/refSeq/GCF_000153465.1_ASM15346v1_genomic.fna 
/mnt/ephem/bactGenomes/refSeq/GCF_000618385.1_EcK1795_genomic.fna
/mnt/ephem/bactGenomes/refSeq/GCF_001896145.1_ASM189614v1_genomic.fna
/mnt/ephem/bactGenomes/refSeq/GCF_002109385.1_ASM210938v1_genomic.fna
/mnt/ephem/bactGenomes/refSeq/GCF_003258865.1_ASM325886v1_genomic.fna
/mnt/ephem/bactGenomes/refSeq/GCF_004342085.1_ASM434208v1_genomic.fna
/mnt/ephem/bactGenomes/refSeq/GCF_005780165.1_ASM578016v1_genomic.fna
/mnt/ephem/bactGenomes/refSeq/GCF_014199735.1_ASM1419973v1_genomic.fna
/mnt/ephem/bactGenomes/refSeq/GCF_014200015.1_ASM1420001v1_genomic.fna
/mnt/ephem/bactGenomes/refSeq/GCF_014636175.1_ASM1463617v1_genomic.fna
/mnt/ephem/bactGenomes/refSeq/GCF_022664535.1_Sx8-8_genomic.fna
/mnt/ephem/bactGenomes/refSeq/GCF_000510665.1_ASM51066v1_genomic.fna
/mnt/ephem/bactGenomes/refSeq/GCF_002312805.1_ASM231280v1_genomic.fna
/mnt/ephem/bactGenomes/refSeq/GCF_002778835.1_ASM277883v1_genomic.fna
/mnt/ephem/bactGenomes/refSeq/GCF_004795855.1_ASM479585v1_genomic.fna
/mnt/ephem/bactGenomes/refSeq/GCF_006385685.1_ASM638568v1_genomic.fna
/mnt/ephem/bactGenomes/refSeq/GCF_013112485.1_ASM1311248v1_genomic.fna
/mnt/ephem/bactGenomes/refSeq/GCF_014202505.1_ASM1420250v1_genomic.fna
/mnt/ephem/bactGenomes/refSeq/GCF_027945725.1_ASM2794572v1_genomic.fna
/mnt/ephem/bactGenomes/refSeq/GCF_030269665.1_ASM3026966v1_genomic.fna
)


echo ${interesting[*]}

echo ${interesting[@]}

cd /mnt/ephem/look4bact3phase/interestingGenomes

for i in ${interesting[@]}; do
 cp $i .
done

du -sh ## these total to just 113M

## great. 

## we want to get rid of the headers...can we do this?:

cd /mnt/ephem/look4bact3phase/interestingGenomes


grep -v "^>" GCF_022664535.1_Sx8-8_genomic.fna | sed '1i\>GCF_022664535.1_Sx8-8_genomic.fna' > test.fasta

text="zoop!"
grep -v "^>" GCF_022664535.1_Sx8-8_genomic.fna | sed "1i\>$text" > test.fasta
## optional:
seqtk seq -l 80 test.fasta > testPretty.fasta

## loop this:

cd /mnt/ephem/look4bact3phase/interestingGenomes
#mkdir catInterestingGenomes
for i in *.fna; do
  echo $i
  j=$(basename $i _genomic.fna)
  grep -v "^>" $i | sed "1i\>$j" > "catInterestingGenomes/"$j"_modif.fna"
done

## and concat these:

cd /mnt/ephem/look4bact3phase/interestingGenomes/catInterestingGenomes

cat * > catCommonBacterialGenomes.fna

## clean up a bit

seqtk seq -l 80 catCommonBacterialGenomes.fna > commonBacterialGenomes.fna

grep -c "^>" commonBacterialGenomes.fna ## twenty genomes, each genome is a target.

## put this on the nanocomp computer, try using it for a reference genome to enrich:

/mnt/ephem/look4bact3phase/interestingGenomes/catInterestingGenomes/commonBacterialGenomes.fna 

path2get=/mnt/ephem/look4bact3phase/interestingGenomes/catInterestingGenomes/commonBacterialGenomes.fna 
path2put=/media/vol1/daniel/hostDepletion/bacteriaEnrich3phase/
scp -rP 30419 -i ~/.ssh/lab2denbi ubuntu@129.70.51.6:$path2get $path2put

## make an mmi
conda activate readfish
minimap2 -d commonBacterialGenomes.mmi commonBacterialGenomes.fna

## we need a toml file

### bacterialEnrichment.toml ###
[caller_settings]
config_name = "dna_r10.4.1_e8.2_400bps_fast.cfg"
host = "ipc:///tmp/.guppy/"
port = 5555

[conditions]
reference = "/media/vol1/daniel/hostDepletion/bacteriaEnrich3phase/commonBacterialGenomes.mmi"

[conditions.0]
name = "bacterialEnrichment3phase_phase3"
control = false
min_chunks = 0
max_chunks = 3
targets = ["GCF_000153465.1_ASM15346v1", "GCF_000510665.1_ASM51066v1", "GCF_000618385.1_EcK1795", "GCF_001896145.1_ASM189614v1", "GCF_002109385.1_ASM210938v1", "GCF_002312805.1_ASM231280v1", "GCF_002778835.1_ASM277883v1", "GCF_003258865.1_ASM325886v1", "GCF_004342085.1_ASM434208v1", "GCF_004795855.1_ASM479585v1", "GCF_005780165.1_ASM578016v1", "GCF_006385685.1_ASM638568v1", "GCF_013112485.1_ASM1311248v1", "GCF_014199735.1_ASM1419973v1", "GCF_014200015.1_ASM1420001v1", "GCF_014202505.1_ASM1420250v1", "GCF_014636175.1_ASM1463617v1", "GCF_022664535.1_Sx8-8", "GCF_027945725.1_ASM2794572v1", "GCF_030269665.1_ASM3026966v1"]
single_on = "stop_receiving"
multi_on = "stop_receiving"
single_off = "unblock"
multi_off = "unblock"
no_seq = "proceed"
no_map = "unblock"
############################


cd /media/vol1/daniel/hostDepletion/bacteriaEnrich3phase

readfish targets --device MN40608 \
              --experiment-name "hostDepletion3phase" \
              --toml bacterialEnrichment.toml \
              --log-file hostDepletion3phase_phase3.log


############## switch over computers ###########

## okay, we have to set up the new denbi comp
## we need to get the files we need off the 
## ephemeral storage

## then shut down and restart, should be the lower
## memory instance. 

## we have ~497G of mixed genomes. 
## what do we need to keep?

## the concatenated files are nice, but we 
## lost all the species information when we
## concatenated them. 

## but we did all the work of created blast indexes 
## for the concatenated files...

## I think we can repeat this with lower memory. 
## the downloads were the greater time commitment.

## so keep the separate files, ditch the concatenated
## genomes

## to keep things organized, all the download metadata
## might be useful, make the bact and fung architecture
## the same:

cd /mnt/ephem/fungalGenomes

cd /vol/piceaNanopore/dan/fungalGenomes

fungDownloadMeta=(
depletionTrial_fungal_blastn.csv
howmanyfungi.txt
nohup.out
)

for i in ${fungDownloadMeta[@]}; do
  #ls $i 
  mv $i /mnt/ephem/fungalGenomes/
done

## now we can move the entire directories over to the volume

## but we have to make room, need to delete the concatenated 
## directories:

## concatenated genomes are here:

ls /vol/piceaNanopore/dan/bactGenomes/

ls /vol/piceaNanopore/dan/fungalGenomes/

## let's clear these out. nervewracking, they were a lot of work.

cd /vol/piceaNanopore/dan/

rm -r /vol/piceaNanopore/dan/bactGenomes/

rm -r /vol/piceaNanopore/dan/fungalGenomes/
   
## should have room now, but let's see if there 
## are other things we can clean up:

## the next big memory suck is here:
/vol/piceaNanopore/chris/k2_standard_20230605

## what is this? Ask chris if we can delete
## yup

rm -r /vol/piceaNanopore/chris/k2_standard_20230605

## anyway, we have enough room to transfer the 
## genomes:

cd /vol/piceaNanopore/dan/ 

cp -r /mnt/ephem/bactGenomes /vol/piceaNanopore/dan/refGenomes/ &
cp -r /mnt/ephem/fungalGenomes /vol/piceaNanopore/dan/refGenomes/ &


## don't think we need to bother deleting these original files... 
## anything else to clean out?

cp -r /mnt/ephem/look4bact3phase /vol/piceaNanopore/dan/ 

## oh, right, a copy of our old fstab might be useful:

LABEL=cloudimg-rootfs   /        ext4   defaults        0 1
LABEL=UEFI      /boot/efi       vfat    umask=0077      0 1
/dev/vdb        /mnt    auto    defaults,nofail,x-systemd.requires=cloud-init.service,_netdev,comment=cloudconfig       0       2
/dev/vdc        none    swap    sw,comment=cloudconfig  0       0
UUID=bca62bee-3532-4905-9789-30b70c09c86b       /vol/piceaNanopore      auto    defaults        0       2

## anything else....nope...

sudo shutdown -r now

## lara pubic key =??
|1|y6vljbmOiUmIHZ98qaudURXjEaM=|yQ7qTlGbUJOkUrJVH0fbCS01wp0= ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBDkrKMDJpr+Uw7RpUKU8lEzARXqGPf5HoI///qj1ZMb+ZkIac0DxJ/GHglBcGPo8qwApYKkT8D3FU4Ro7hlrLHs= 

## update fstab

ls -l /dev/disk/by-uuid/ ## looks same as above

blkid /dev/vdd ## not on there

lsblk -f ## yup. same as above. add the above line to fstab and make the path

## looks good, just need to get lara's login going...


## try an example fastqc run

fastqc -t 3 --extract sample1.fq.gz

path2get=/home/test/sample1_fastqc.html
scp -r -i ~/.ssh/id_ed25519 test@132.180.112.115:$path2get /home/daniel/Desktop

##### lara alignments ####

## find your reads to be aligned..
## for today, we are using the raw nanopore reads from 
## phase2. 
## they will need to be concatenated, and the unblocked 
## reads will need to be removed, like yesterday
## then you should put them on your denbi computer 
## somewhere

## the lab computer can scp directly to denbi if you want to do
## this, let me know if you want to set this up and need help


## you have two large storage areas on denbi:

## ephemeral storage is here:
/mnt/ephem

## long term storage is here:
/vol/piceaNanopore

## feel free to make yourself working directories in both

## the full spruce genome is here on denbi:
ls -lh /vol/piceaNanopore/spruceGenome/Pabies-haploid_withOrganelles.fa

## make a fresh minimap2 index from this
## (do you have minimap2 installed somewhere on denbi?)
## (if not, make an environment and intall it!)
cd /vol/piceaNanopore/dan/chrisHostDepletionTrial/spruceRefGenome/

## once you have your reads in place, and a minimap index

reads=/path/to/your/reads
nohup minimap2 \
  -I100g \
  -t 20 \
  -x map-ont \
  --secondary=no \
  $sprucemmi $reads \
  1> phase2hostDepletion_Align2Spruce.paf \
  2> phase2hostDepletion_Align2Spruce.paf.log &

## let's look at this together when you have this paf file.

## looking at this in R:

cd /vol/piceaNanopore/dan/look4bact3phase/phase2hostDepletion

head phase2hostDepletion_Align2Spruce.paf

cut -f1-12 phase2hostDepletion_Align2Spruce.paf | head -n 20 > paftoy.tsv

R
#install.packages("dplyr")
library("dplyr")

## we want to get only the highest-quality alignment for each read to the 
## spruce host:

pafNames=c("QueryName","QueryLength","QueryStart","QueryEnd","RelativeStrand",
"TargetName","TargeLength","TargetStart","TargetEnd",
"ResidueMatches","AlignmentBlockLngth","MappingQuality")
aa = read.csv("paftoy.tsv", sep="\t",col.names=pafNames)

bb <- aa %>%
  group_by(QueryName) %>%
  summarize(maxScore = max(MappingQuality))

bestscores = as.data.frame(bb)


:
## can we do this with tapply? the old fashioned way? just for fun:
bestMatchScore = tapply(aa$MappingQuality, aa$QueryName, max)
## works, but then you have to parse it back to a dataframe
bestMatchScoreDF = as.data.frame(bestMatchScore)

## the two methods seem to give the same result...

cat (estMatchScoreDF$QueryName, file="test.txt")

## or maybe better:
cat (bestMatchScoreDF$QueryName, sep="\n", file="test.txt")

bestMatchScoreDF$QueryName

cat (bestMatchScoreDF$QueryName)


## how do we get the matches not included in our alignment?

## seqtk and sort/uniq might be useful

## let's pretend the names of all the reads in the alignment are here:
cut -f1-12 phase2hostDepletion_Align2Spruce.paf | head -n 20 > toy.paf

## to get just the names we use cut
cut toy.paf -f 1

## so we can sort these and get only the unique ones:
cut toy.paf -f 1 | sort | uniq > seqsThatAligned.txt

## now we need the names of the sequences in our fastq

## for this we would use sed, and tell it to print every 
## fourth line

sed -n '1~4p' toy.fastq 

## but this gives us a lot of extra information. We just want the name of the read

## cut to the rescue, again!

sed -n '1~4p' toy.fastq |
cut -f 1 -d " " 

## but notice there is still an "@" at the beginning? Use sed again to get rid of it:

sed -n '1~4p' toy.fastq |
cut -f 1 -d " " |
sed "s/^@//" > namesOfAllPhase2Reads.txt

## give it a sanity-check, make sure it looks like only read names are in there
## (with less or whatever)

## now, we want get reads that didn't align at all to the spruce genome
## this means we need to subtract the the aligned reads from the total reads

## we can do this like we did before with "sort | uniq" 
## can you build the command in BASH for this?


### new concatenated bacterial reference genome ###

## we are going to remake our bacterial reference genome file

## we need to ensure that includes the name of the organism it aligned to
## and that we can recover the exact genome of the match, if necessary

## our ref genomes are here:
cd /vol/piceaNanopore/dan/refGenomes/bactGenomes/refSeq

## is the information about organism name in all the record
## names:

exampleGens=(
GCF_948472415.1_JK4103_genomic.fna
GCF_948495795.1_MGBC105309_genomic.fna
GCF_949025885.1_MGBC114154_genomic.fna
GCF_949152215.1_Q7238_genomic.fna
GCF_949152225.1_Q7487_genomic.fna
GCF_949428155.1_extra-SRR4116659.61_1677764545_genomic.fna
GCF_949429935.1_extra-SRR5574496.25_1677764854_genomic.fna
GCF_949740115.1_iMGMC-24_1678881672_genomic.fna
GCF_949743855.1_iMGMC-20_1678881660_genomic.fna
GCF_949769135.1_Geo25_assembly_genomic.fna
GCF_949769145.1_Geo24_assembly_genomic.fna
GCF_949769155.1_Geo48_assembly_genomic.fna
GCF_949769225.1_Q7072_genomic.fna
GCF_949769255.1_Q7826_genomic.fna
GCF_949769265.1_Q7820_genomic.fna
GCF_949769285.1_Q7828_genomic.fna
GCF_949775715.1_Brytella_acorum_LMG_32879_assembly_genomic.fna
GCF_950054335.1_SRR3962771_bin.15_MetaWRAP_v1.3_MAG_genomic.fna
GCF_950065245.1_SRR3960580_bin.7_MetaWRAP_v1.3_MAG_genomic.fna
GCF_900215595.1_IMG-taxon_2728369725_annotated_assembly_genomic.fna
GCF_900215605.1_IMG-taxon_2728369261_annotated_assembly_genomic.fna
GCF_900215615.1_IMG-taxon_2738541359_annotated_assembly_genomic.fna
GCF_900215625.1_IMG-taxon_2681812962_annotated_assembly_genomic.fna
GCF_900215645.1_IMG-taxon_2716884894_annotated_assembly_genomic.fna
GCF_900215655.1_IMG-taxon_2728369262_annotated_assembly_genomic.fna
GCF_900215725.1_PRJEB22476_genomic.fna
GCF_900217235.1_DPRO_PRJEB22548_v1_genomic.fna
GCF_900217715.1_IMG-taxon_2596583541_annotated_assembly_genomic.fna
GCF_900217725.1_IMG-taxon_2740891830_annotated_assembly_genomic.fna
GCF_900217795.1_IMG-taxon_2740891832_annotated_assembly_genomic.fna
GCF_900217815.1_IMG-taxon_2739368070_annotated_assembly_genomic.fna
GCF_900217845.1_IMG-taxon_2596583606_annotated_assembly_genomic.fna
GCF_900218015.1_IMG-taxon_2740891836_annotated_assembly_genomic.fna
GCF_900218035.1_IMG-taxon_2596583532_annotated_assembly_genomic.fna
GCF_900218065.1_Chol11_genomic.fna
GCF_900220965.1_IMG-taxon_2740891860_annotated_assembly_genomic.fna
GCF_900220975.1_IMG-taxon_2740891857_annotated_assembly_genomic.fna
GCF_900220985.1_IMG-taxon_2740891889_annotated_assembly_genomic.fna
)

for i in ${exampleGens[@]}; do
  ls $i
  sed -n '1p' $i
done

## sure looks like it.
## so we need to insure that blastn retains this information 
## in the output

## get blastn install via conda
conda create -n blast -c bioconda blast

conda activate blast

## for example

cd /vol/piceaNanopore/dan/refGenomes/bactGenomes/playWithBlast
 
cp /vol/piceaNanopore/dan/refGenomes/bactGenomes/refSeq/GCF_013350045.1_ASM1335004v1_genomic.fna .

## let's make two databases, one with parsing and one without
## one may give the output we need...

## first with parse
makeblastdb -in GCF_013350045.1_ASM1335004v1_genomic.fna -parse_seqids -dbtype nucl 
## try searching for known seq, check output:

>randoseq
GTCTTTGGCATCGTGCTTCGTTGGCAAGTTGTCGTCCAGTTCTTTCAACCGTTTGACATGCATCGGATTGACCATCACCA
## in a file called randoseq.fa

blastn -db GCF_013350045.1_ASM1335004v1_genomic.fna \
  -query randoseq.fa \
  -num_alignments 1 \
  -out alignedWithParse.csv \
  -outfmt "10 sacc staxids sscinames bitscore"

## and that's not what we need, only retains the accession number, not organism name

## try making database without parse:
makeblastdb -in GCF_013350045.1_ASM1335004v1_genomic.fna -dbtype nucl

sed -n "1p" GCF_013350045.1_ASM1335004v1_genomic.fna

## in a file called randoseq.fa

blastn -db GCF_013350045.1_ASM1335004v1_genomic.fna \
  -query randoseq.fa \
  -num_alignments 1 \
  -out alignedNoParse.csv \
  -outfmt "10 sacc sscinames"

## nope, don't matter

wget ftp://ftp.ncbi.nlm.nih.gov/blast/db/taxdb.tar.gz

tar -xvf taxdb.tar.gz

## none of this is working. 
## so, maybe it is possible to edit the record 
## headers?


sed "/^>/ s/ /_/" GCF_013350045.1_ASM1335004v1_genomic.fna 

## does this work on other genomes?

cp ../refSeq/GCF_013419135.1_ASM1341913v1_genomic.fna . 

sed "/^>/ s/ /_/" GCF_013350045.1_ASM1335004v1_genomic.fna | less

## that also looks pretty good.

## does this allow us to keep genus info in our blast matches?:
cp "../refSeq/GCF_013419135.1_ASM1341913v1_genomic.fna" .
cp "../refSeq/GCF_013350045.1_ASM1335004v1_genomic.fna" .
cat GCF_013419135.1_ASM1341913v1_genomic.fna GCF_013350045.1_ASM1335004v1_genomic.fna > genome3.fna

#genome="GCF_013419135.1_ASM1341913v1_genomic.fna"
#genome="GCF_013350045.1_ASM1335004v1_genomic.fna"
genome="genome3.fna"
cp ../refSeq/$genome .
rm *fna.n* ## get rid of old indexes
sed -i  "/^>/ s/ /:/" $genome
makeblastdb -in $genome -dbtype nucl
blastn -db $genome \
  -query randoseq.fa \
  -num_alignments 1 \
  -out aligned_keepGenus.csv \
  -outfmt 10

head aligned_keepGenus.csv 

## seems to work. 
## will probably fail somehow on the 
## big concatenated genomes, but let's try.

for i in *; do
  echo $i
done

## need to grab an example of our own higher quality 
## data:

scp 

path2get=/media/vol1/daniel/hostDepletion/ourData/bigGenome_depletion_data/bigDataRedo/qcBig/comboSpruceZoop5_onlyBigReads_fastqc/fastqc_report.html 
path2put=/home/daniel/Documents/projects/fichtelgebirge_project/spruceGenome/nanoporeHostDepletionSpruce/dan
scp -r -i ~/.ssh/id_ed25519 test@132.180.112.115:$path2get $path2put

## and Lara's fastqc reports, an example?
path2get=/experiment_spruce_threephase/
path2put=/home/daniel/Documents/projects/fichtelgebirge_project/spruceGenome/depletion2big4github/
scp -r -i ~/.ssh/id_ed25519 daniel@132.180.112.24:$path2get $path2put

ssh daniel@132.180.112.24
/experiment_spruce_threephase

## How is your 

### backing up denbi ###

## okay, we are done with our denbi instance for the moment,
## so we need to back up our results - what and where?:

## on denbi,
## let's grab it all:

## chris
tar -czvf chrisNanoporeWork.gz /vol/piceaNanopore/chris

tar -czvf danNanoporeWork.gz /vol/piceaNanopore/dan

## put this onto my volume on the office computer:

## from nanopore computer:

## chris
file=/home/ubuntu/chrisNanoporeWork.gz
dest=/media/vol1/daniel/denbiBackups/
scp -P 30336 -i /home/test/.ssh/lab2denbi ubuntu@129.70.51.6:$file $dest

## my stuff:
file=/home/ubuntu/danNanoporeWork.gz
dest=/media/vol1/daniel/denbiBackups/
scp -P 30336 -i /home/test/.ssh/lab2denbi ubuntu@129.70.51.6:$file $dest

###### return to the data for grant writing #####

## we need to get back to this now, look at what we did
## where is the data now?

## depends on what kind of data we want - we want the raw reads 
## from all runs...

## phase 1 is...here?

/var/lib/minknow/data/experiment_spruce_threephase/controll_130923/20230913_1355_MN40608_FAX46654_83018e78

## this experiment started on:
2023-09-13T13:58:42.158478+02:00
## and stopped on 
2023-09-13T22:31:23.075176+02:00
## so it lasted 9 hours or so?

## phase 2:
/var/lib/minknow/data/experiment_spruce_threephase/phase2hostDepletion/20230913_2238_MN40608_FAX46654_d4c9fea9
2023-09-13T22:41:06.779062+02:00
2023-09-14T03:49:52.747187+02:00
## so 5 hours? 

## phase 3 was in two parts:
/var/lib/minknow/data/experiment_spruce_threephase/bacterialEnrichment
/var/lib/minknow/data/experiment_spruce_threephase/bacterialEnrichment_second
##  ran for several days:
1 start 2023-09-15T02:19:17.868669+02:00
1 stop  2023-09-15T17:19:19.107374+02:00
2 start 2023-09-15T17:42:53.045486+02:00
2 stop  2023-09-18T17:42:54.450859+02:00

## anyway, looks right.  
## what do we want to do with this?

## we ran alignments to estimate 
## amount of bacterial reads...

## let's see if we can repeat Lara's results...

## where do we get the reads that blasted to bacteria refGenomes?

## can't find them on nanopore computer...

## office computer:

## ah, there they are.. 
scp daniel@132.180.112.24:/home/lara/data/3phaseExperiment/phase*.csv .

phase1_bacterial_blastn.csv
phase2_bacterial_blastn.csv
phase3_first_bacterial_blastn.csv
phase3_second_bacterial_blastn.csv

## and fungal reads? hope I kept this...
## this may be it:

scp test@132.180.112.115:/media/vol1/daniel/denbiBackups/vol/piceaNanopore/dan/refGenomes/fungalGenomes/depletionTrial_fungal_blastn.csv \
  /home/daniel/Documents/projects/fichtelgebirge_project/spruceGenome/nanoporeHostDepletionSpruce/dan/reads3phaseBlast/readsBlast2Fungi/

